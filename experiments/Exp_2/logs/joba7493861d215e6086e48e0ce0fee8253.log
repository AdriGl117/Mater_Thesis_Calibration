### [bt]: This is batchtools v0.9.17
### [bt]: Starting calculation of 1 jobs
### [bt]: Setting working directory to '/content/drive/MyDrive/GitHub/Mater_Thesis_Calibration'
ece.R :
Friedmann_tasks.R :
Functions.R :
ici.R :
mse_feature_effects.R :
PipeOpCalibration_TuningExperiment.R :
PipeOpCalibration.R :
PipeOpCalibrationUnion.R :
### [bt]: Memory measurement disabled
### [bt]: Starting job [batchtools job.id=1438]
### [bt]: Generating problem instance for problem '341ee781c7d79b8f' ...
### [bt]: Applying algorithm 'run_learner' on problem '341ee781c7d79b8f' for job 1438 (seed = 1561) ...
INFO  [16:15:21.145] [mlr3] Applying learner 'xgboost TwP platt' on task 'wdbc' (iter 8/10)
INFO  [16:15:22.182] [bbotk] Starting to optimize 8 parameter(s) with '<OptimizerMbo>' and '<TerminatorEvals> [n_evals=100, k=0]'
INFO  [16:15:36.759] [bbotk] Evaluating 32 configuration(s)
INFO  [16:15:37.377] [mlr3] Running benchmark with 96 resampling iterations
INFO  [16:15:37.665] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:17:54.022] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:19:47.265] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:21:20.591] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:21:59.379] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:22:50.527] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:23:31.994] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:24:19.692] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:25:31.264] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:26:59.738] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:27:27.199] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:27:56.520] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:28:35.600] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:30:40.484] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:32:11.193] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:34:09.421] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:34:50.814] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:35:37.365] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:36:28.161] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:37:44.578] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:39:01.043] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:40:10.064] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:40:43.107] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:41:25.753] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:42:02.843] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:42:48.778] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:43:23.445] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:44:00.017] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:44:58.585] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:46:03.611] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:47:09.930] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:47:32.969] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:48:05.123] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:48:40.958] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:49:35.694] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:50:19.107] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:51:13.607] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:53:52.824] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:56:24.632] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:58:30.039] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:59:13.463] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:00:11.667] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:01:03.394] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:02:12.749] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:03:41.878] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:05:01.333] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:05:52.236] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:06:19.000] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:06:52.229] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:08:11.546] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:09:34.295] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:11:21.924] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:12:12.391] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:12:47.320] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:13:40.840] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:14:35.322] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:15:11.173] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:15:55.784] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:16:19.076] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:16:36.352] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:16:59.857] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:17:41.459] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:18:19.178] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:19:05.064] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:19:37.551] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:20:39.361] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:21:38.705] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:22:19.027] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:23:01.149] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:24:01.353] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:24:52.885] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:25:23.833] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:26:22.388] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:26:44.311] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:27:14.910] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:27:40.860] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:28:15.254] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:28:43.386] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:29:21.628] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:29:47.243] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:30:18.326] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:30:46.249] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:31:29.216] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:32:21.905] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:32:59.554] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:33:48.475] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:34:26.428] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:35:15.949] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:35:32.485] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:35:49.292] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:36:05.979] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:36:50.036] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:37:29.201] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:38:23.161] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:38:50.259] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:39:12.101] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:39:43.371] [mlr3] Finished benchmark
INFO  [17:39:44.220] [bbotk] Result of batch 1:
INFO  [17:39:44.285] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:39:44.285] [bbotk]              3.2963959                         0.7246401
INFO  [17:39:44.285] [bbotk]             -3.6113590                         0.2746401
INFO  [17:39:44.285] [bbotk]             -0.1574814                         0.9496401
INFO  [17:39:44.285] [bbotk]              6.7502735                         0.4996401
INFO  [17:39:44.285] [bbotk]             -5.3382978                         0.6121401
INFO  [17:39:44.285] [bbotk]              1.5694570                         0.1621401
INFO  [17:39:44.285] [bbotk]              5.0233347                         0.8371401
INFO  [17:39:44.285] [bbotk]             -1.8844202                         0.3871401
INFO  [17:39:44.285] [bbotk]             -1.0209508                         0.1058901
INFO  [17:39:44.285] [bbotk]              5.8868041                         0.5558901
INFO  [17:39:44.285] [bbotk]              2.4329264                         0.3308901
INFO  [17:39:44.285] [bbotk]             -4.4748284                         0.7808901
INFO  [17:39:44.285] [bbotk]             -2.7478896                         0.6683901
INFO  [17:39:44.285] [bbotk]              4.1598653                         0.2183901
INFO  [17:39:44.285] [bbotk]              0.7059876                         0.8933901
INFO  [17:39:44.285] [bbotk]             -6.2017673                         0.4433901
INFO  [17:39:44.285] [bbotk]             -4.9065631                         0.3590151
INFO  [17:39:44.285] [bbotk]              2.0011917                         0.8090151
INFO  [17:39:44.285] [bbotk]              5.4550694                         0.1340151
INFO  [17:39:44.285] [bbotk]             -1.4526855                         0.5840151
INFO  [17:39:44.285] [bbotk]              3.7281306                         0.6965151
INFO  [17:39:44.285] [bbotk]             -3.1796243                         0.2465151
INFO  [17:39:44.285] [bbotk]              0.2742529                         0.4715151
INFO  [17:39:44.285] [bbotk]             -6.6335020                         0.9215151
INFO  [17:39:44.285] [bbotk]              1.1377223                         0.6402651
INFO  [17:39:44.285] [bbotk]             -5.7700325                         0.1902651
INFO  [17:39:44.285] [bbotk]             -2.3161549                         0.8652651
INFO  [17:39:44.285] [bbotk]              4.5916000                         0.4152651
INFO  [17:39:44.285] [bbotk]              2.8646612                         0.3027651
INFO  [17:39:44.285] [bbotk]             -4.0430937                         0.7527651
INFO  [17:39:44.285] [bbotk]             -0.5892161                         0.5277651
INFO  [17:39:44.285] [bbotk]              6.3185388                         0.9777651
INFO  [17:39:44.285] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:39:44.285] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:39:44.285] [bbotk]                         0.2469759          -7.9513805             -6.7477431
INFO  [17:39:44.285] [bbotk]                         0.6969759          -3.3462102              0.1600123
INFO  [17:39:44.285] [bbotk]                         0.4719759          -5.6487956              3.6138899
INFO  [17:39:44.285] [bbotk]                         0.9219759          -1.0436251             -3.2938653
INFO  [17:39:44.285] [bbotk]                         0.8094759          -4.4975028              1.8869511
INFO  [17:39:44.285] [bbotk]                         0.3594759          -9.1026731             -5.0208044
INFO  [17:39:44.285] [bbotk]                         0.5844759          -2.1949177             -1.5669265
INFO  [17:39:44.285] [bbotk]                         0.1344759          -6.8000881              5.3408288
INFO  [17:39:44.285] [bbotk]                         0.9782259          -7.3757343             -0.7034571
INFO  [17:39:44.285] [bbotk]                         0.5282259          -2.7705639              6.2042982
INFO  [17:39:44.285] [bbotk]                         0.7532259          -5.0731493              2.7504205
INFO  [17:39:44.285] [bbotk]                         0.3032259          -0.4679789             -4.1573350
INFO  [17:39:44.285] [bbotk]                         0.6407259          -8.5270268             -2.4303959
INFO  [17:39:44.285] [bbotk]                         0.1907259          -3.9218565              4.4773594
INFO  [17:39:44.285] [bbotk]                         0.8657259          -6.2244419              1.0234817
INFO  [17:39:44.285] [bbotk]                         0.4157259          -1.6192714             -5.8842738
INFO  [17:39:44.285] [bbotk]                         0.8376009          -8.8148500              4.0456246
INFO  [17:39:44.285] [bbotk]                         0.3876009          -4.2096796             -2.8621306
INFO  [17:39:44.285] [bbotk]                         0.6126009          -6.5122650             -6.3160084
INFO  [17:39:44.285] [bbotk]                         0.1626009          -1.9070945              0.5917470
INFO  [17:39:44.285] [bbotk]                         0.9501009          -5.3609724             -4.5890697
INFO  [17:39:44.285] [bbotk]                         0.5001009          -0.7558020              2.3186858
INFO  [17:39:44.285] [bbotk]                         0.2751009          -3.0583871             -1.1351918
INFO  [17:39:44.285] [bbotk]                         0.7251009          -7.6635574              5.7725635
INFO  [17:39:44.285] [bbotk]                         0.6688509          -0.1801557              4.9090941
INFO  [17:39:44.285] [bbotk]                         0.2188509          -4.7853262             -1.9986612
INFO  [17:39:44.285] [bbotk]                         0.8938509          -2.4827408             -5.4525391
INFO  [17:39:44.285] [bbotk]                         0.4438509          -7.0879111              1.4552164
INFO  [17:39:44.285] [bbotk]                         0.7813509          -1.3314483              6.6360329
INFO  [17:39:44.285] [bbotk]                         0.3313509          -5.9366187             -0.2717224
INFO  [17:39:44.285] [bbotk]                         0.5563509          -3.6340334             -3.7256002
INFO  [17:39:44.285] [bbotk]                         0.1063509          -8.2392037              3.1821552
INFO  [17:39:44.285] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:39:44.285] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:39:44.285] [bbotk]                          7                    4373                 0.8044846
INFO  [17:39:44.285] [bbotk]                         17                    1873                 0.3544845
INFO  [17:39:44.285] [bbotk]                         12                    3123                 0.1294845
INFO  [17:39:44.285] [bbotk]                          2                     623                 0.5794846
INFO  [17:39:44.285] [bbotk]                         15                    4998                 0.9169846
INFO  [17:39:44.285] [bbotk]                          5                    2498                 0.4669845
INFO  [17:39:44.285] [bbotk]                         10                    3748                 0.2419845
INFO  [17:39:44.285] [bbotk]                         20                    1248                 0.6919846
INFO  [17:39:44.285] [bbotk]                         14                    1560                 0.4107345
INFO  [17:39:44.285] [bbotk]                          4                    4060                 0.8607346
INFO  [17:39:44.285] [bbotk]                          9                     310                 0.6357346
INFO  [17:39:44.285] [bbotk]                         19                    2810                 0.1857345
INFO  [17:39:44.285] [bbotk]                         16                    4685                 0.9732346
INFO  [17:39:44.285] [bbotk]                          6                    2185                 0.5232345
INFO  [17:39:44.285] [bbotk]                          1                    3435                 0.2982345
INFO  [17:39:44.285] [bbotk]                         11                     935                 0.7482346
INFO  [17:39:44.285] [bbotk]                          3                    4217                 0.6076096
INFO  [17:39:44.285] [bbotk]                         13                    1717                 0.1576095
INFO  [17:39:44.285] [bbotk]                         18                    2967                 0.3826095
INFO  [17:39:44.285] [bbotk]                          8                     467                 0.8326096
INFO  [17:39:44.285] [bbotk]                         11                    1092                 0.9451096
INFO  [17:39:44.285] [bbotk]                          1                    3592                 0.4951095
INFO  [17:39:44.285] [bbotk]                         16                    4842                 0.7201096
INFO  [17:39:44.285] [bbotk]                          6                    2342                 0.2701095
INFO  [17:39:44.285] [bbotk]                         19                     779                 0.8888596
INFO  [17:39:44.285] [bbotk]                          9                    3279                 0.4388595
INFO  [17:39:44.285] [bbotk]                          4                    2029                 0.2138595
INFO  [17:39:44.285] [bbotk]                         14                    4529                 0.6638596
INFO  [17:39:44.285] [bbotk]                         12                    2654                 0.3263595
INFO  [17:39:44.285] [bbotk]                          2                     154                 0.7763596
INFO  [17:39:44.285] [bbotk]                          7                    3904                 0.5513596
INFO  [17:39:44.285] [bbotk]                         17                    1404                 0.1013595
INFO  [17:39:44.285] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:39:44.285] [bbotk]  classif.bbrier warnings errors runtime_learners
INFO  [17:39:44.285] [bbotk]      0.04195622        0      0          342.214
INFO  [17:39:44.285] [bbotk]      0.02317549        0      0          131.108
INFO  [17:39:44.285] [bbotk]      0.04544020        0      0          206.956
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0           95.275
INFO  [17:39:44.285] [bbotk]      0.03187547        0      0          333.156
INFO  [17:39:44.285] [bbotk]      0.10545543        0      0          138.296
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0          221.717
INFO  [17:39:44.285] [bbotk]      0.16040913        0      0          112.384
INFO  [17:39:44.285] [bbotk]      0.03741805        0      0          116.459
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0          189.190
INFO  [17:39:44.285] [bbotk]      0.04073063        0      0           90.332
INFO  [17:39:44.285] [bbotk]      0.03107310        0      0          152.132
INFO  [17:39:44.285] [bbotk]      0.04160373        0      0          436.008
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0          152.670
INFO  [17:39:44.285] [bbotk]      0.04080666        0      0          236.899
INFO  [17:39:44.285] [bbotk]      0.02887732        0      0          110.247
INFO  [17:39:44.285] [bbotk]      0.08752550        0      0          268.623
INFO  [17:39:44.285] [bbotk]      0.04577345        0      0          138.113
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0          134.415
INFO  [17:39:44.285] [bbotk]      0.02871780        0      0           63.686
INFO  [17:39:44.285] [bbotk]      0.04855490        0      0          124.679
INFO  [17:39:44.285] [bbotk]      0.02703159        0      0          153.107
INFO  [17:39:44.285] [bbotk]      0.02994518        0      0          142.206
INFO  [17:39:44.285] [bbotk]      0.38225513        0      0          140.650
INFO  [17:39:44.285] [bbotk]      0.03388073        0      0           78.175
INFO  [17:39:44.285] [bbotk]      0.02751097        0      0          100.382
INFO  [17:39:44.285] [bbotk]      0.02240768        0      0           84.145
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0          132.808
INFO  [17:39:44.285] [bbotk]      0.05427182        0      0          135.987
INFO  [17:39:44.285] [bbotk]      0.04332877        0      0           49.686
INFO  [17:39:44.285] [bbotk]      0.02591005        0      0          136.851
INFO  [17:39:44.285] [bbotk]      0.43376843        0      0           75.914
INFO  [17:39:44.285] [bbotk]  classif.bbrier warnings errors runtime_learners
INFO  [17:39:44.285] [bbotk]                                 uhash
INFO  [17:39:44.285] [bbotk]  40c4d58f-e313-4f3a-b5dd-ab6d10e6bd4c
INFO  [17:39:44.285] [bbotk]  1ce913f9-b45b-483c-af38-cdd6fd2f99ee
INFO  [17:39:44.285] [bbotk]  d13ebff1-6d42-4a03-a073-55440b625ab4
INFO  [17:39:44.285] [bbotk]  980a3522-02e2-4073-8f4b-db95da248366
INFO  [17:39:44.285] [bbotk]  f12002ee-d057-4a39-adad-f7c12d096f83
INFO  [17:39:44.285] [bbotk]  8726794b-670f-4bb2-ab92-07db5fd55825
INFO  [17:39:44.285] [bbotk]  7fb7a853-dd4d-471e-b353-65232da81516
INFO  [17:39:44.285] [bbotk]  04d62b35-ccda-44d8-94d1-41cc0e6fdab6
INFO  [17:39:44.285] [bbotk]  b23147d3-c135-4c34-bb51-ac0517a58201
INFO  [17:39:44.285] [bbotk]  57c0e0a1-682b-4527-8685-592b84e650e5
INFO  [17:39:44.285] [bbotk]  91fced94-0521-4b12-8b95-920124ce7e3c
INFO  [17:39:44.285] [bbotk]  cc308bd4-b4f6-44d1-ae9b-14b87ca3d555
INFO  [17:39:44.285] [bbotk]  3d993fd7-1167-4f9e-b19c-87504958a927
INFO  [17:39:44.285] [bbotk]  ca356fb8-1d1e-4e36-ae51-06ceaa8c1e89
INFO  [17:39:44.285] [bbotk]  178f27bd-083a-4d69-aab2-0e3a33ad965f
INFO  [17:39:44.285] [bbotk]  0f0648ee-4991-4ba8-ae44-1bec088c03ff
INFO  [17:39:44.285] [bbotk]  133cb25e-eae6-4db0-9056-0653d2ded11f
INFO  [17:39:44.285] [bbotk]  96b0e607-2ac6-414c-afd6-7eab08315322
INFO  [17:39:44.285] [bbotk]  b503a3fb-47cb-49de-9c8a-edb86a926e94
INFO  [17:39:44.285] [bbotk]  1ae8fe88-b083-48d5-ac89-cd424d58c8af
INFO  [17:39:44.285] [bbotk]  f16b2428-16e9-47ba-aa11-9b57febf2699
INFO  [17:39:44.285] [bbotk]  7b45051e-c889-4dd6-b4f1-1ddc76a34a45
INFO  [17:39:44.285] [bbotk]  d33e6070-bf03-4bae-970b-3c60feb291c6
INFO  [17:39:44.285] [bbotk]  9234e019-1742-4fbc-b4ee-27f503ae2f99
INFO  [17:39:44.285] [bbotk]  d04c0ea5-f18e-4fa1-a259-492b147d8ba5
INFO  [17:39:44.285] [bbotk]  d874b685-976a-4ffe-bbb9-dce601a9b2da
INFO  [17:39:44.285] [bbotk]  03f722e9-e383-4e77-99c7-0df81e398070
INFO  [17:39:44.285] [bbotk]  c8d5413b-ad38-4a8a-a278-dfca8dcdea3a
INFO  [17:39:44.285] [bbotk]  a975aa17-fc4a-4ce2-bc0f-b0697efbab67
INFO  [17:39:44.285] [bbotk]  33164e6c-f8ac-4189-9adf-dd165feb763e
INFO  [17:39:44.285] [bbotk]  e5f24dd7-a5f4-403a-a22c-6be0d4cbbdf8
INFO  [17:39:44.285] [bbotk]  316b7eb1-6eff-4874-bdae-8dd2ec5b3843
INFO  [17:39:44.285] [bbotk]                                 uhash
INFO  [17:39:50.888] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 32 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:39:57.305] [bbotk] Evaluating 1 configuration(s)
INFO  [17:39:57.519] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:39:57.860] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:40:24.320] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:41:24.310] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:42:14.638] [mlr3] Finished benchmark
INFO  [17:42:14.729] [bbotk] Result of batch 2:
INFO  [17:42:14.791] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:42:14.791] [bbotk]              -4.817209                         0.1128231
INFO  [17:42:14.791] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:42:14.791] [bbotk]                          0.705687           -2.787919              -4.557501
INFO  [17:42:14.791] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:42:14.791] [bbotk]                          9                    4751                 0.9989969
INFO  [17:42:14.791] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:42:14.791] [bbotk]  0.07314033 <list[8]>              FALSE      0.0292588        0      0
INFO  [17:42:14.791] [bbotk]  runtime_learners                                uhash
INFO  [17:42:14.791] [bbotk]           136.183 61596c08-e270-41cf-b8a2-dd466e46e647
INFO  [17:42:16.588] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 33 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:42:21.842] [bbotk] Evaluating 1 configuration(s)
INFO  [17:42:21.927] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:42:22.153] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:42:57.092] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:43:48.470] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:44:25.992] [mlr3] Finished benchmark
INFO  [17:44:26.115] [bbotk] Result of batch 3:
INFO  [17:44:26.218] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:44:26.218] [bbotk]               3.023537                         0.8713984
INFO  [17:44:26.218] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:44:26.218] [bbotk]                         0.9520071           -1.240296              -3.990847
INFO  [17:44:26.218] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:44:26.218] [bbotk]                         16                    1761                 0.8290612
INFO  [17:44:26.218] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:44:26.218] [bbotk]  0.06083733 <list[8]>              FALSE     0.03920464        0      0
INFO  [17:44:26.218] [bbotk]  runtime_learners                                uhash
INFO  [17:44:26.218] [bbotk]           123.392 038d8b5b-3efc-461c-b492-a1a884afc1f3
INFO  [17:44:29.808] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 34 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:44:37.972] [bbotk] Evaluating 1 configuration(s)
INFO  [17:44:38.322] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:44:38.554] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:45:05.769] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:45:28.848] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:46:01.432] [mlr3] Finished benchmark
INFO  [17:46:01.743] [bbotk] Result of batch 4:
INFO  [17:46:01.848] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:46:01.848] [bbotk]              -5.010432                         0.3338894
INFO  [17:46:01.848] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:46:01.848] [bbotk]                         0.2148862          -0.6579888             -0.6608731
INFO  [17:46:01.848] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:46:01.848] [bbotk]                         14                    1526                 0.8283439
INFO  [17:46:01.848] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:46:01.848] [bbotk]  0.03429998 <list[8]>              FALSE     0.02703863        0      0
INFO  [17:46:01.848] [bbotk]  runtime_learners                                uhash
INFO  [17:46:01.848] [bbotk]            82.265 2ad27a1f-ca36-4502-873d-ac86c8231a11
INFO  [17:46:04.145] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 35 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:46:10.723] [bbotk] Evaluating 1 configuration(s)
INFO  [17:46:11.019] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:46:11.201] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:47:18.247] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:48:13.971] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:49:04.181] [mlr3] Finished benchmark
INFO  [17:49:04.335] [bbotk] Result of batch 5:
INFO  [17:49:04.445] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:49:04.445] [bbotk]               1.676065                         0.6318331
INFO  [17:49:04.445] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:49:04.445] [bbotk]                         0.9484946           -1.917104               6.350885
INFO  [17:49:04.445] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:49:04.445] [bbotk]                         13                    4502                 0.2385643
INFO  [17:49:04.445] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:49:04.445] [bbotk]  0.03688511 <list[8]>              FALSE     0.04177774        0      0
INFO  [17:49:04.445] [bbotk]  runtime_learners                                uhash
INFO  [17:49:04.445] [bbotk]           172.368 0fc835d6-8fa4-42d1-a57a-de2f2c5d0e91
INFO  [17:49:07.286] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 36 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:49:13.440] [bbotk] Evaluating 1 configuration(s)
INFO  [17:49:13.565] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:49:13.798] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:49:52.159] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:50:40.376] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:51:38.889] [mlr3] Finished benchmark
INFO  [17:51:39.048] [bbotk] Result of batch 6:
INFO  [17:51:39.070] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:51:39.070] [bbotk]              -5.361012                         0.4548917
INFO  [17:51:39.070] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:51:39.070] [bbotk]                         0.3416865           -5.547778              -4.462049
INFO  [17:51:39.070] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:51:39.070] [bbotk]                         18                    2838                 0.9226022
INFO  [17:51:39.070] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:51:39.070] [bbotk]  0.03111856 <list[8]>              FALSE     0.03009092        0      0
INFO  [17:51:39.070] [bbotk]  runtime_learners                                uhash
INFO  [17:51:39.070] [bbotk]           143.534 62026411-142a-46d7-a415-d867faa81882
INFO  [17:51:41.709] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 37 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:51:50.236] [bbotk] Evaluating 1 configuration(s)
INFO  [17:51:50.320] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:51:50.404] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:52:13.741] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:52:37.944] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:53:15.236] [mlr3] Finished benchmark
INFO  [17:53:15.329] [bbotk] Result of batch 7:
INFO  [17:53:15.358] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:53:15.358] [bbotk]               2.939712                         0.9941931
INFO  [17:53:15.358] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:53:15.358] [bbotk]                         0.7807745           -9.095132              -6.723523
INFO  [17:53:15.358] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:53:15.358] [bbotk]                          1                    1119                 0.6894174
INFO  [17:53:15.358] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:53:15.358] [bbotk]  0.03292793 <list[8]>              FALSE      0.3299951        0      0
INFO  [17:53:15.358] [bbotk]  runtime_learners                                uhash
INFO  [17:53:15.358] [bbotk]             84.42 13f19a72-1849-4e8d-8f06-aadcfe3b157c
WARN  [17:53:17.626] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [17:53:17.717] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 38 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:53:24.872] [bbotk] Evaluating 1 configuration(s)
INFO  [17:53:25.006] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:53:25.188] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:54:12.386] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:55:02.534] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:55:57.605] [mlr3] Finished benchmark
INFO  [17:55:57.788] [bbotk] Result of batch 8:
INFO  [17:55:57.858] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:55:57.858] [bbotk]               2.860307                         0.4128232
INFO  [17:55:57.858] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:55:57.858] [bbotk]                         0.5190403           -1.698918              -3.430141
INFO  [17:55:57.858] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:55:57.858] [bbotk]                         11                    3483                 0.9268647
INFO  [17:55:57.858] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:55:57.858] [bbotk]  0.05025925 <list[8]>              FALSE       0.042786        0      0
INFO  [17:55:57.858] [bbotk]  runtime_learners                                uhash
INFO  [17:55:57.858] [bbotk]            151.83 20a14d7e-f7bf-463e-b047-04742ae39540
INFO  [17:55:58.467] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 39 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:56:04.493] [bbotk] Evaluating 1 configuration(s)
INFO  [17:56:04.683] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:56:04.920] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:56:26.927] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:56:51.685] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:57:16.382] [mlr3] Finished benchmark
INFO  [17:57:16.522] [bbotk] Result of batch 9:
INFO  [17:57:16.593] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:57:16.593] [bbotk]              0.8219784                         0.4150692
INFO  [17:57:16.593] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:57:16.593] [bbotk]                         0.8078673           -4.874998               1.012411
INFO  [17:57:16.593] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:57:16.593] [bbotk]                         10                     377                 0.7902158
INFO  [17:57:16.593] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:57:16.593] [bbotk]  0.04595932 <list[8]>              FALSE     0.04114121        0      0
INFO  [17:57:16.593] [bbotk]  runtime_learners                                uhash
INFO  [17:57:16.593] [bbotk]            71.017 88344702-83bb-4480-968f-ab379a25a889
WARN  [17:57:18.676] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [17:57:18.704] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 40 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:57:24.355] [bbotk] Evaluating 1 configuration(s)
INFO  [17:57:24.545] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:57:24.884] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:57:51.415] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:58:20.817] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:59:00.837] [mlr3] Finished benchmark
INFO  [17:59:01.390] [bbotk] Result of batch 10:
INFO  [17:59:01.589] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:59:01.589] [bbotk]              -3.049018                         0.5000807
INFO  [17:59:01.589] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:59:01.589] [bbotk]                         0.7538186           -3.706731             -0.1749498
INFO  [17:59:01.589] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:59:01.589] [bbotk]                          9                    3095                 0.4789012
INFO  [17:59:01.589] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:59:01.589] [bbotk]  0.04324575 <list[8]>              FALSE     0.02908505        0      0
INFO  [17:59:01.589] [bbotk]  runtime_learners                                uhash
INFO  [17:59:01.589] [bbotk]            94.769 41f6c0c3-e196-40fa-991f-80de5ba3672e
INFO  [17:59:02.433] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 41 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:59:06.575] [bbotk] Evaluating 1 configuration(s)
INFO  [17:59:06.728] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:59:07.240] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:00:02.633] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:00:45.243] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:01:41.490] [mlr3] Finished benchmark
INFO  [18:01:41.600] [bbotk] Result of batch 11:
INFO  [18:01:41.636] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:01:41.636] [bbotk]               3.561328                         0.3065557
INFO  [18:01:41.636] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:01:41.636] [bbotk]                         0.5094248           -8.669982              -6.519436
INFO  [18:01:41.636] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:01:41.636] [bbotk]                         16                    4545                 0.9304917
INFO  [18:01:41.636] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:01:41.636] [bbotk]  0.0460569 <list[8]>              FALSE      0.0532106        0      0
INFO  [18:01:41.636] [bbotk]  runtime_learners                                uhash
INFO  [18:01:41.636] [bbotk]           153.776 6c0b689e-7393-4fb3-a0b0-145e11e4a2b3
INFO  [18:01:42.547] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 42 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:01:50.307] [bbotk] Evaluating 1 configuration(s)
INFO  [18:01:50.445] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:01:50.616] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:02:18.654] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:02:49.255] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:03:09.369] [mlr3] Finished benchmark
INFO  [18:03:09.503] [bbotk] Result of batch 12:
INFO  [18:03:09.537] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:03:09.537] [bbotk]              -1.587604                           0.36422
INFO  [18:03:09.537] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:03:09.537] [bbotk]                         0.8247625           -1.014466             -0.7222856
INFO  [18:03:09.537] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:03:09.537] [bbotk]                          1                    2460                 0.2243628
INFO  [18:03:09.537] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:03:09.537] [bbotk]  0.0319561 <list[8]>              FALSE     0.02167378        0      0
INFO  [18:03:09.537] [bbotk]  runtime_learners                                uhash
INFO  [18:03:09.537] [bbotk]            77.892 5169de13-e3c1-447c-88a6-a7c30a081570
INFO  [18:03:10.548] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 43 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:03:16.310] [bbotk] Evaluating 1 configuration(s)
INFO  [18:03:16.470] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:03:16.647] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:03:52.705] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:04:21.187] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:04:49.182] [mlr3] Finished benchmark
INFO  [18:04:49.439] [bbotk] Result of batch 13:
INFO  [18:04:49.583] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:04:49.583] [bbotk]              -5.291507                          0.194242
INFO  [18:04:49.583] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:04:49.583] [bbotk]                         0.6531263          -0.5317948              -5.473677
INFO  [18:04:49.583] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:04:49.583] [bbotk]                         12                    2697                 0.5141308
INFO  [18:04:49.583] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:04:49.583] [bbotk]  0.03018836 <list[8]>              FALSE     0.02639075        0      0
INFO  [18:04:49.583] [bbotk]  runtime_learners                                uhash
INFO  [18:04:49.583] [bbotk]            91.793 43c72764-4eeb-4f55-b169-748558b18eb5
INFO  [18:04:50.910] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 44 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:04:54.657] [bbotk] Evaluating 1 configuration(s)
INFO  [18:04:54.779] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:04:55.021] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:05:26.043] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:05:50.965] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:06:28.288] [mlr3] Finished benchmark
INFO  [18:06:28.542] [bbotk] Result of batch 14:
INFO  [18:06:28.599] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:06:28.599] [bbotk]               3.204571                         0.3852405
INFO  [18:06:28.599] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:06:28.599] [bbotk]                         0.9734003           -4.550535              0.4948748
INFO  [18:06:28.599] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:06:28.599] [bbotk]                         20                    2250                 0.8497595
INFO  [18:06:28.599] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:06:28.599] [bbotk]  0.03775413 <list[8]>              FALSE     0.04104783        0      0
INFO  [18:06:28.599] [bbotk]  runtime_learners                                uhash
INFO  [18:06:28.599] [bbotk]            92.867 4b1e3987-fc24-4e45-8cc6-d77773e724d7
WARN  [18:06:30.498] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:06:30.558] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 45 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:06:36.878] [bbotk] Evaluating 1 configuration(s)
INFO  [18:06:37.389] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:06:37.898] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:06:58.190] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:07:18.889] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:07:38.777] [mlr3] Finished benchmark
INFO  [18:07:38.877] [bbotk] Result of batch 15:
INFO  [18:07:38.956] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:07:38.956] [bbotk]               2.132279                         0.2637176
INFO  [18:07:38.956] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:07:38.956] [bbotk]                         0.7240255           -4.739664                3.83808
INFO  [18:07:38.956] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:07:38.956] [bbotk]                         19                     348                 0.7945137
INFO  [18:07:38.956] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:07:38.956] [bbotk]  0.03499194 <list[8]>              FALSE     0.03993575        0      0
INFO  [18:07:38.956] [bbotk]  runtime_learners                                uhash
INFO  [18:07:38.956] [bbotk]             60.22 8834c6a9-d53e-45f9-a7e1-49da165095d9
INFO  [18:07:40.198] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 46 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:07:44.948] [bbotk] Evaluating 1 configuration(s)
INFO  [18:07:45.138] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:07:45.391] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:08:13.188] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:08:40.189] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:09:02.229] [mlr3] Finished benchmark
INFO  [18:09:02.428] [bbotk] Result of batch 16:
INFO  [18:09:02.538] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:09:02.538] [bbotk]              -2.203119                         0.8853383
INFO  [18:09:02.538] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:09:02.538] [bbotk]                         0.3153541          -0.6230995              -3.626234
INFO  [18:09:02.538] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:09:02.538] [bbotk]                          5                    1221                 0.4910928
INFO  [18:09:02.538] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:09:02.538] [bbotk]  0.03396923 <list[8]>              FALSE     0.02351024        0      0
INFO  [18:09:02.538] [bbotk]  runtime_learners                                uhash
INFO  [18:09:02.538] [bbotk]            76.581 ba10d9d4-85a5-4dd0-8a73-419e74feaf73
WARN  [18:09:05.825] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:09:05.986] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 47 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:09:13.311] [bbotk] Evaluating 1 configuration(s)
INFO  [18:09:13.496] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:09:13.671] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:10:14.272] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:10:59.720] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:11:46.840] [mlr3] Finished benchmark
INFO  [18:11:46.959] [bbotk] Result of batch 17:
INFO  [18:11:47.084] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:11:47.084] [bbotk]               3.405397                         0.9863753
INFO  [18:11:47.084] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:11:47.084] [bbotk]                         0.3701588           -1.223979              -5.034605
INFO  [18:11:47.084] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:11:47.084] [bbotk]                         20                    4365                 0.7721929
INFO  [18:11:47.084] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:11:47.084] [bbotk]  0.0400978 <list[8]>              FALSE     0.04110742        0      0
INFO  [18:11:47.084] [bbotk]  runtime_learners                                uhash
INFO  [18:11:47.084] [bbotk]           152.877 352f17cf-cca3-436d-a5ca-60a923654416
WARN  [18:11:48.506] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:11:48.566] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 48 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:11:54.136] [bbotk] Evaluating 1 configuration(s)
INFO  [18:11:54.166] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:11:54.302] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:12:29.227] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:12:58.681] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:13:39.141] [mlr3] Finished benchmark
INFO  [18:13:39.282] [bbotk] Result of batch 18:
INFO  [18:13:39.320] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:13:39.320] [bbotk]               1.414962                         0.8976173
INFO  [18:13:39.320] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:13:39.320] [bbotk]                         0.2385661           -1.397315               1.863354
INFO  [18:13:39.320] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:13:39.320] [bbotk]                         14                    2464                 0.2325378
INFO  [18:13:39.320] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:13:39.320] [bbotk]  0.03566471 <list[8]>              FALSE     0.03608099        0      0
INFO  [18:13:39.320] [bbotk]  runtime_learners                                uhash
INFO  [18:13:39.320] [bbotk]           104.319 0eb4f1bb-3200-4ec7-a829-26e424306183
INFO  [18:13:40.521] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 49 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:13:44.832] [bbotk] Evaluating 1 configuration(s)
INFO  [18:13:45.132] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:13:45.342] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:14:21.097] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:15:01.766] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:15:58.057] [mlr3] Finished benchmark
INFO  [18:15:58.503] [bbotk] Result of batch 19:
INFO  [18:15:58.575] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:15:58.575] [bbotk]               3.164098                         0.6919863
INFO  [18:15:58.575] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:15:58.575] [bbotk]                         0.1280076           -4.205414              -5.753884
INFO  [18:15:58.575] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:15:58.575] [bbotk]                         19                    4718                 0.8851838
INFO  [18:15:58.575] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:15:58.575] [bbotk]  0.0440744 <list[8]>              FALSE     0.04057739        0      0
INFO  [18:15:58.575] [bbotk]  runtime_learners                                uhash
INFO  [18:15:58.575] [bbotk]           131.469 796e8992-c947-4f58-b76a-67426daeefb4
INFO  [18:16:00.576] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 50 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:16:07.282] [bbotk] Evaluating 1 configuration(s)
INFO  [18:16:07.412] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:16:07.495] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:16:30.443] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:17:11.739] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:17:51.488] [mlr3] Finished benchmark
INFO  [18:17:51.930] [bbotk] Result of batch 20:
INFO  [18:17:52.061] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:17:52.061] [bbotk]              -4.121684                         0.3025827
INFO  [18:17:52.061] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:17:52.061] [bbotk]                         0.3761676           -5.552466               1.359982
INFO  [18:17:52.061] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:17:52.061] [bbotk]                         11                    4464                 0.6788618
INFO  [18:17:52.061] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:17:52.061] [bbotk]  0.03066619 <list[8]>              FALSE     0.03035477        0      0
INFO  [18:17:52.061] [bbotk]  runtime_learners                                uhash
INFO  [18:17:52.061] [bbotk]           102.125 fb2b2284-9c64-4901-be1a-dd24b71a5653
INFO  [18:17:53.242] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 51 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:17:58.753] [bbotk] Evaluating 1 configuration(s)
INFO  [18:17:58.872] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:17:58.945] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:18:39.648] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:19:18.525] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:20:06.450] [mlr3] Finished benchmark
INFO  [18:20:06.693] [bbotk] Result of batch 21:
INFO  [18:20:06.849] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:20:06.849] [bbotk]              -5.670218                         0.2356591
INFO  [18:20:06.849] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:20:06.849] [bbotk]                         0.1299586          -0.7038943              -3.360685
INFO  [18:20:06.849] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:20:06.849] [bbotk]                          9                    4856                 0.9333138
INFO  [18:20:06.849] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:20:06.849] [bbotk]  0.02760242 <list[8]>              FALSE     0.02745292        0      0
INFO  [18:20:06.849] [bbotk]  runtime_learners                                uhash
INFO  [18:20:06.849] [bbotk]           127.069 da535c1d-aafd-4e91-a135-41bf1a324d76
INFO  [18:20:08.203] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 52 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:20:15.197] [bbotk] Evaluating 1 configuration(s)
INFO  [18:20:15.389] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:20:15.551] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:20:50.628] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:21:29.697] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:22:01.974] [mlr3] Finished benchmark
INFO  [18:22:02.137] [bbotk] Result of batch 22:
INFO  [18:22:02.188] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:22:02.188] [bbotk]              -1.527723                         0.6428126
INFO  [18:22:02.188] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:22:02.188] [bbotk]                         0.9542129           -2.702293              -2.047468
INFO  [18:22:02.188] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:22:02.188] [bbotk]                          8                    2054                 0.8438462
INFO  [18:22:02.188] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:22:02.188] [bbotk]  0.02538605 <list[8]>              FALSE     0.03211707        0      0
INFO  [18:22:02.188] [bbotk]  runtime_learners                                uhash
INFO  [18:22:02.188] [bbotk]            105.87 c843e94b-c34a-46c9-b34e-ef64d31a848f
INFO  [18:22:03.407] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 53 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:22:10.964] [bbotk] Evaluating 1 configuration(s)
INFO  [18:22:11.103] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:22:11.403] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:22:39.346] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:22:58.195] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:23:41.874] [mlr3] Finished benchmark
INFO  [18:23:42.012] [bbotk] Result of batch 23:
INFO  [18:23:42.052] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:23:42.052] [bbotk]               -3.54971                         0.1014332
INFO  [18:23:42.052] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:23:42.052] [bbotk]                         0.6178478          -0.7863958               -4.46013
INFO  [18:23:42.052] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:23:42.052] [bbotk]                          4                    2703                 0.5075892
INFO  [18:23:42.052] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:23:42.052] [bbotk]  0.02479715 <list[8]>              FALSE     0.02203057        0      0
INFO  [18:23:42.052] [bbotk]  runtime_learners                                uhash
INFO  [18:23:42.052] [bbotk]            89.453 2049c4d0-7cf9-462f-809b-57ae71dc1344
INFO  [18:23:43.284] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 54 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:23:49.014] [bbotk] Evaluating 1 configuration(s)
INFO  [18:23:49.143] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:23:49.747] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:24:51.698] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:25:33.953] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:26:16.419] [mlr3] Finished benchmark
INFO  [18:26:16.604] [bbotk] Result of batch 24:
INFO  [18:26:16.651] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:26:16.651] [bbotk]              -4.244568                         0.6470289
INFO  [18:26:16.651] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:26:16.651] [bbotk]                         0.7657256           -4.474843              -2.219127
INFO  [18:26:16.651] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:26:16.651] [bbotk]                         17                    3451                 0.5045948
INFO  [18:26:16.651] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:26:16.651] [bbotk]  0.02352411 <list[8]>              FALSE     0.03020115        0      0
INFO  [18:26:16.651] [bbotk]  runtime_learners                                uhash
INFO  [18:26:16.651] [bbotk]            145.62 7a65080c-28e1-41d8-9340-6b6ea69be75e
INFO  [18:26:18.074] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 55 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:26:25.042] [bbotk] Evaluating 1 configuration(s)
INFO  [18:26:25.160] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:26:25.219] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:26:55.632] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:27:26.295] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:27:59.840] [mlr3] Finished benchmark
INFO  [18:28:00.181] [bbotk] Result of batch 25:
INFO  [18:28:00.240] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:28:00.240] [bbotk]               -2.55328                         0.1450392
INFO  [18:28:00.240] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:28:00.240] [bbotk]                          0.394053           -6.872705              -4.057188
INFO  [18:28:00.240] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:28:00.240] [bbotk]                          1                    2781                 0.1417797
INFO  [18:28:00.240] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:28:00.240] [bbotk]  0.02404742 <list[8]>              FALSE     0.04426922        0      0
INFO  [18:28:00.240] [bbotk]  runtime_learners                                uhash
INFO  [18:28:00.240] [bbotk]            94.081 e7d6131f-4aff-400a-a24a-1734f90fec0b
INFO  [18:28:01.374] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 56 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:28:10.064] [bbotk] Evaluating 1 configuration(s)
INFO  [18:28:10.159] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:28:10.209] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:28:44.473] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:29:12.938] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:29:46.076] [mlr3] Finished benchmark
INFO  [18:29:46.226] [bbotk] Result of batch 26:
INFO  [18:29:46.272] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:29:46.272] [bbotk]             -0.1966196                         0.4074462
INFO  [18:29:46.272] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:29:46.272] [bbotk]                         0.5201781           -1.492723               1.824798
INFO  [18:29:46.272] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:29:46.272] [bbotk]                          2                    2093                 0.2232243
INFO  [18:29:46.272] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:29:46.272] [bbotk]  0.02253153 <list[8]>              FALSE     0.02397135        0      0
INFO  [18:29:46.272] [bbotk]  runtime_learners                                uhash
INFO  [18:29:46.272] [bbotk]             95.62 d2173415-ea40-4dba-b8b7-46446fcc5a01
WARN  [18:29:48.882] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:29:48.961] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 57 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:30:10.719] [bbotk] Evaluating 1 configuration(s)
INFO  [18:30:10.847] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:30:10.922] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:30:29.224] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:30:57.609] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:31:20.524] [mlr3] Finished benchmark
INFO  [18:31:20.621] [bbotk] Result of batch 27:
INFO  [18:31:20.650] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:31:20.650] [bbotk]                2.00736                         0.2439128
INFO  [18:31:20.650] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:31:20.650] [bbotk]                         0.8963525           -3.588635               5.230568
INFO  [18:31:20.650] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:31:20.650] [bbotk]                         14                     984                 0.1549111
INFO  [18:31:20.650] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:31:20.650] [bbotk]  0.02343357 <list[8]>              FALSE     0.05827806        0      0
INFO  [18:31:20.650] [bbotk]  runtime_learners                                uhash
INFO  [18:31:20.650] [bbotk]            69.352 3f06471d-dbe1-475b-a02c-4c9ccaaf7a18
INFO  [18:31:21.960] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 58 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:31:26.273] [bbotk] Evaluating 1 configuration(s)
INFO  [18:31:26.309] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:31:26.348] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:32:23.018] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:33:34.247] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:34:49.387] [mlr3] Finished benchmark
INFO  [18:34:49.801] [bbotk] Result of batch 28:
INFO  [18:34:49.848] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:34:49.848] [bbotk]               2.027802                          0.957114
INFO  [18:34:49.848] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:34:49.848] [bbotk]                         0.7635482           -2.158579               2.554535
INFO  [18:34:49.848] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:34:49.848] [bbotk]                         14                    4850                 0.9909449
INFO  [18:34:49.848] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:34:49.848] [bbotk]  0.02301907 <list[8]>              FALSE     0.04136877        0      0
INFO  [18:34:49.848] [bbotk]  runtime_learners                                uhash
INFO  [18:34:49.848] [bbotk]           202.569 055fb317-9e58-48af-a027-edaf6e9dbeda
INFO  [18:35:05.820] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 59 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:35:19.072] [bbotk] Evaluating 1 configuration(s)
INFO  [18:35:19.159] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:35:19.351] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:36:05.316] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:36:49.404] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:37:36.108] [mlr3] Finished benchmark
INFO  [18:37:36.940] [bbotk] Result of batch 29:
INFO  [18:37:37.226] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:37:37.226] [bbotk]               0.563138                         0.6298176
INFO  [18:37:37.226] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:37:37.226] [bbotk]                         0.3013251           -3.806983               3.327704
INFO  [18:37:37.226] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:37:37.226] [bbotk]                          6                    4103                 0.3390829
INFO  [18:37:37.226] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:37:37.226] [bbotk]  0.0208609 <list[8]>              FALSE     0.03396263        0      0
INFO  [18:37:37.226] [bbotk]  runtime_learners                                uhash
INFO  [18:37:37.226] [bbotk]           135.547 f03d15f1-13f0-4ddc-840b-b139cf63f433
INFO  [18:37:39.658] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 60 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:37:53.116] [bbotk] Evaluating 1 configuration(s)
INFO  [18:37:53.147] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:37:53.268] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:38:22.797] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:38:42.720] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:39:21.691] [mlr3] Finished benchmark
INFO  [18:39:21.895] [bbotk] Result of batch 30:
INFO  [18:39:21.975] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:39:21.975] [bbotk]              -1.028798                         0.5680988
INFO  [18:39:21.975] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:39:21.975] [bbotk]                         0.6415727          -0.3659126               -2.92585
INFO  [18:39:21.975] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:39:21.975] [bbotk]                          9                    1842                 0.1479538
INFO  [18:39:21.975] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:39:21.975] [bbotk]  0.02267294 <list[8]>              FALSE     0.02518822        0      0
INFO  [18:39:21.975] [bbotk]  runtime_learners                                uhash
INFO  [18:39:21.975] [bbotk]            88.178 803a060c-bb5f-432f-b2a8-ddded3727d13
INFO  [18:39:22.975] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 61 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:39:28.784] [bbotk] Evaluating 1 configuration(s)
INFO  [18:39:28.846] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:39:28.876] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:39:56.524] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:40:19.972] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:40:43.605] [mlr3] Finished benchmark
INFO  [18:40:43.807] [bbotk] Result of batch 31:
INFO  [18:40:43.858] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:40:43.858] [bbotk]             -0.9307191                         0.7139091
INFO  [18:40:43.858] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:40:43.858] [bbotk]                          0.653045           -8.818837              -1.264826
INFO  [18:40:43.858] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:40:43.858] [bbotk]                          2                     617                 0.6405504
INFO  [18:40:43.858] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:40:43.858] [bbotk]  0.02016355 <list[8]>              FALSE      0.1998075        0      0
INFO  [18:40:43.858] [bbotk]  runtime_learners                                uhash
INFO  [18:40:43.858] [bbotk]            74.539 78f0752f-8da7-4d68-9f0e-73395066724f
INFO  [18:40:45.679] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 62 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:40:52.395] [bbotk] Evaluating 1 configuration(s)
INFO  [18:40:52.612] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:40:52.765] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:41:37.902] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:42:24.042] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:43:13.727] [mlr3] Finished benchmark
INFO  [18:43:14.070] [bbotk] Result of batch 32:
INFO  [18:43:14.222] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:43:14.222] [bbotk]              -1.081216                         0.1839606
INFO  [18:43:14.222] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:43:14.222] [bbotk]                         0.1124239           -1.025776               -1.19248
INFO  [18:43:14.222] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:43:14.222] [bbotk]                         12                    4608                 0.7788569
INFO  [18:43:14.222] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:43:14.222] [bbotk]  0.03893585 <list[8]>              FALSE     0.02656792        0      0
INFO  [18:43:14.222] [bbotk]  runtime_learners                                uhash
INFO  [18:43:14.222] [bbotk]           139.851 bfa743b6-03da-4bd3-b9fb-ee5ac8efdede
INFO  [18:43:16.181] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 63 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:43:21.716] [bbotk] Evaluating 1 configuration(s)
INFO  [18:43:21.807] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:43:21.924] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:44:00.217] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:44:47.516] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:45:32.181] [mlr3] Finished benchmark
INFO  [18:45:32.785] [bbotk] Result of batch 33:
INFO  [18:45:32.845] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:45:32.845] [bbotk]                1.41194                         0.1732646
INFO  [18:45:32.845] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:45:32.845] [bbotk]                         0.9470508           -1.924956               2.512297
INFO  [18:45:32.845] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:45:32.845] [bbotk]                         12                    2736                 0.4841756
INFO  [18:45:32.845] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:45:32.845] [bbotk]  0.04793924 <list[8]>              FALSE      0.0339938        0      0
INFO  [18:45:32.845] [bbotk]  runtime_learners                                uhash
INFO  [18:45:32.845] [bbotk]             129.3 0a48db62-4312-4225-b5c9-ab2669a9f334
INFO  [18:45:35.104] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 64 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:45:41.651] [bbotk] Evaluating 1 configuration(s)
INFO  [18:45:41.831] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:45:42.134] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:46:36.783] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:47:25.778] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:48:31.119] [mlr3] Finished benchmark
INFO  [18:48:31.662] [bbotk] Result of batch 34:
INFO  [18:48:31.856] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:48:31.856] [bbotk]             0.08886944                         0.6575609
INFO  [18:48:31.856] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:48:31.856] [bbotk]                         0.8872011           -1.779444              -1.536083
INFO  [18:48:31.856] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:48:31.856] [bbotk]                         11                    4864                 0.2953214
INFO  [18:48:31.856] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:48:31.856] [bbotk]  0.03628007 <list[8]>              FALSE     0.02477302        0      0
INFO  [18:48:31.856] [bbotk]  runtime_learners                                uhash
INFO  [18:48:31.856] [bbotk]           168.607 832c2db0-f0dc-4231-bb42-67a3b51e9ca2
INFO  [18:48:37.209] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 65 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:48:44.919] [bbotk] Evaluating 1 configuration(s)
INFO  [18:48:45.060] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:48:45.139] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:49:24.381] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:50:06.572] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:50:46.619] [mlr3] Finished benchmark
INFO  [18:50:46.917] [bbotk] Result of batch 35:
INFO  [18:50:46.937] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:50:46.937] [bbotk]              -1.794051                         0.2884681
INFO  [18:50:46.937] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:50:46.937] [bbotk]                         0.9042173          -0.7399795              -1.585043
INFO  [18:50:46.937] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:50:46.937] [bbotk]                          5                    3942                 0.2834923
INFO  [18:50:46.937] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:50:46.937] [bbotk]  0.03027188 <list[8]>              FALSE     0.02570736        0      0
INFO  [18:50:46.937] [bbotk]  runtime_learners                                uhash
INFO  [18:50:46.937] [bbotk]           121.043 581a9a14-9ee9-4c6f-8434-6daa86bbaf69
INFO  [18:50:49.129] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 66 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:50:53.125] [bbotk] Evaluating 1 configuration(s)
INFO  [18:50:53.159] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:50:53.213] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:51:20.361] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:51:56.474] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:52:35.008] [mlr3] Finished benchmark
INFO  [18:52:36.218] [bbotk] Result of batch 36:
INFO  [18:52:36.905] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:52:36.905] [bbotk]             -0.4814615                         0.2230326
INFO  [18:52:36.905] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:52:36.905] [bbotk]                         0.5284621         -0.08079353             -0.7273686
INFO  [18:52:36.905] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:52:36.905] [bbotk]                          8                    2788                 0.6221252
INFO  [18:52:36.905] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:52:36.905] [bbotk]  0.0287777 <list[8]>              FALSE     0.02755978        0      0
INFO  [18:52:36.905] [bbotk]  runtime_learners                                uhash
INFO  [18:52:36.905] [bbotk]           101.496 e4237aeb-d08c-44ac-bd84-8cbad5b99592
WARN  [18:52:47.260] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:52:47.285] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 67 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:52:59.136] [bbotk] Evaluating 1 configuration(s)
INFO  [18:53:00.814] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:53:01.188] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:54:02.201] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:54:40.199] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:55:25.647] [mlr3] Finished benchmark
INFO  [18:55:25.961] [bbotk] Result of batch 37:
INFO  [18:55:25.986] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:55:25.986] [bbotk]              -1.718901                         0.6266224
INFO  [18:55:25.986] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:55:25.986] [bbotk]                         0.6616298           -2.368438              -4.803696
INFO  [18:55:25.986] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:55:25.986] [bbotk]                         16                    2882                 0.5317848
INFO  [18:55:25.986] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:55:25.986] [bbotk]  0.03060999 <list[8]>              FALSE     0.02670807        0      0
INFO  [18:55:25.986] [bbotk]  runtime_learners                                uhash
INFO  [18:55:25.986] [bbotk]            143.92 2bd3445e-3d74-403c-98dd-c89022d1c7a2
INFO  [18:55:33.837] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 68 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:55:44.699] [bbotk] Evaluating 1 configuration(s)
INFO  [18:55:44.833] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:55:44.867] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:57:00.213] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:57:54.574] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:59:33.098] [mlr3] Finished benchmark
INFO  [18:59:33.759] [bbotk] Result of batch 38:
INFO  [18:59:33.800] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:59:33.800] [bbotk]                2.08442                         0.3826474
INFO  [18:59:33.800] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:59:33.800] [bbotk]                         0.6722353           -6.635505                -6.6998
INFO  [18:59:33.800] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:59:33.800] [bbotk]                          7                    4910                 0.8751019
INFO  [18:59:33.800] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:59:33.800] [bbotk]  0.08197343 <list[8]>              FALSE     0.04026077        0      0
INFO  [18:59:33.800] [bbotk]  runtime_learners                                uhash
INFO  [18:59:33.800] [bbotk]           227.928 5f1140d6-42fc-4bb8-973c-6f31bed77db1
INFO  [18:59:36.161] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 69 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:59:47.090] [bbotk] Evaluating 1 configuration(s)
INFO  [18:59:47.128] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:59:47.159] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:00:51.889] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:01:39.310] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:02:42.439] [mlr3] Finished benchmark
INFO  [19:02:42.994] [bbotk] Result of batch 39:
INFO  [19:02:43.002] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:02:43.002] [bbotk]             -0.9504383                         0.1722966
INFO  [19:02:43.002] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:02:43.002] [bbotk]                         0.7169514           -3.904143             -0.5372572
INFO  [19:02:43.002] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:02:43.002] [bbotk]                         19                    4229                 0.2156692
INFO  [19:02:43.002] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:02:43.002] [bbotk]  0.02805863 <list[8]>              FALSE     0.02412087        0      0
INFO  [19:02:43.002] [bbotk]  runtime_learners                                uhash
INFO  [19:02:43.002] [bbotk]           174.791 2294cd4a-2891-4d57-913d-ade2bc2f41ec
WARN  [19:02:49.447] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:02:49.459] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 70 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:03:02.852] [bbotk] Evaluating 1 configuration(s)
INFO  [19:03:03.170] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:03:03.506] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:04:47.004] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:06:13.312] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [19:08:07.731] [mlr3] Finished benchmark
INFO  [19:08:08.879] [bbotk] Result of batch 40:
INFO  [19:08:08.898] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:08:08.898] [bbotk]               3.111215                         0.9527593
INFO  [19:08:08.898] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:08:08.898] [bbotk]                         0.7566822           -8.740667              -5.966105
INFO  [19:08:08.898] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:08:08.898] [bbotk]                         14                    3115                  0.806607
INFO  [19:08:08.898] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:08:08.898] [bbotk]  0.02500326 <list[8]>              FALSE     0.05026332        0      0
INFO  [19:08:08.898] [bbotk]  runtime_learners                                uhash
INFO  [19:08:08.898] [bbotk]           303.474 9ee5b62f-c170-47c0-8c40-bad52bcaad68
INFO  [19:08:11.510] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 71 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:08:22.456] [bbotk] Evaluating 1 configuration(s)
INFO  [19:08:22.790] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:08:22.934] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:08:47.726] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:09:08.742] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:09:33.088] [mlr3] Finished benchmark
INFO  [19:09:34.832] [bbotk] Result of batch 41:
INFO  [19:09:34.937] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:09:34.937] [bbotk]              -3.621095                         0.4539298
INFO  [19:09:34.937] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:09:34.937] [bbotk]                         0.3926642           -1.982651              -4.392682
INFO  [19:09:34.937] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:09:34.937] [bbotk]                         12                     260                 0.3634558
INFO  [19:09:34.937] [bbotk]   acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:09:34.937] [bbotk]  0.02941 <list[8]>              FALSE     0.02455407        0      0
INFO  [19:09:34.937] [bbotk]  runtime_learners                                uhash
INFO  [19:09:34.937] [bbotk]            69.403 bce487ea-1c8c-4df2-8f4f-a283efd39a74
WARN  [19:09:46.269] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:09:46.645] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 72 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:09:57.933] [bbotk] Evaluating 1 configuration(s)
INFO  [19:09:58.678] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:09:59.014] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:11:57.479] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:13:32.879] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [19:15:22.287] [mlr3] Finished benchmark
INFO  [19:15:22.394] [bbotk] Result of batch 42:
INFO  [19:15:22.487] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:15:22.487] [bbotk]               3.333645                          0.920673
INFO  [19:15:22.487] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:15:22.487] [bbotk]                         0.7434341           -2.912506              -6.498875
INFO  [19:15:22.487] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:15:22.487] [bbotk]                         12                    4701                  0.882851
INFO  [19:15:22.487] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:15:22.487] [bbotk]  0.02323736 <list[8]>              FALSE     0.04208943        0      0
INFO  [19:15:22.487] [bbotk]  runtime_learners                                uhash
INFO  [19:15:22.487] [bbotk]           322.891 cdb835c6-f723-4fb3-aac7-72e89372c9c5
INFO  [19:15:24.977] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 73 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:15:33.687] [bbotk] Evaluating 1 configuration(s)
INFO  [19:15:34.201] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:15:35.755] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:16:26.829] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:17:16.961] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:18:17.627] [mlr3] Finished benchmark
INFO  [19:18:18.005] [bbotk] Result of batch 43:
INFO  [19:18:18.014] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:18:18.014] [bbotk]              -4.853069                         0.3313447
INFO  [19:18:18.014] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:18:18.014] [bbotk]                         0.1012534           -4.517884              -4.299614
INFO  [19:18:18.014] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:18:18.014] [bbotk]                          5                    2612                 0.2555164
INFO  [19:18:18.014] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:18:18.014] [bbotk]  0.02268093 <list[8]>              FALSE     0.02574714        0      0
INFO  [19:18:18.014] [bbotk]  runtime_learners                                uhash
INFO  [19:18:18.014] [bbotk]           161.444 3765a9dc-9604-4254-9468-19895a8843c0
INFO  [19:18:26.294] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 74 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:18:44.108] [bbotk] Evaluating 1 configuration(s)
INFO  [19:18:44.394] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:18:44.484] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:20:15.007] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:21:21.489] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:22:30.009] [mlr3] Finished benchmark
INFO  [19:22:30.119] [bbotk] Result of batch 44:
INFO  [19:22:30.145] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:22:30.145] [bbotk]                1.10293                         0.5478302
INFO  [19:22:30.145] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:22:30.145] [bbotk]                         0.4633474            -7.78181              -3.201127
INFO  [19:22:30.145] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:22:30.145] [bbotk]                         11                    4629                 0.2451285
INFO  [19:22:30.145] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:22:30.145] [bbotk]  0.02472298 <list[8]>              FALSE     0.03986824        0      0
INFO  [19:22:30.145] [bbotk]  runtime_learners                                uhash
INFO  [19:22:30.145] [bbotk]            225.29 007f6b94-0689-41d1-b17b-2e67b31fa6e3
INFO  [19:22:35.068] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 75 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:22:44.300] [bbotk] Evaluating 1 configuration(s)
INFO  [19:22:44.576] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:22:44.751] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:23:18.315] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:23:47.496] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:24:26.211] [mlr3] Finished benchmark
INFO  [19:24:26.350] [bbotk] Result of batch 45:
INFO  [19:24:26.447] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:24:26.447] [bbotk]              -2.589153                         0.1870939
INFO  [19:24:26.447] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:24:26.447] [bbotk]                         0.5845779        -0.009114526               1.243458
INFO  [19:24:26.447] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:24:26.447] [bbotk]                          8                    1095                 0.1046146
INFO  [19:24:26.447] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:24:26.447] [bbotk]  0.02706231 <list[8]>              FALSE     0.02956918        0      0
INFO  [19:24:26.447] [bbotk]  runtime_learners                                uhash
INFO  [19:24:26.447] [bbotk]           100.912 e1e79d1c-c82d-4aa8-bb1c-b51a2e674f1c
WARN  [19:24:29.891] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:24:29.899] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 76 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:24:41.463] [bbotk] Evaluating 1 configuration(s)
INFO  [19:24:41.618] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:24:41.674] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:25:26.884] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:26:39.449] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [19:27:39.878] [mlr3] Finished benchmark
INFO  [19:27:39.979] [bbotk] Result of batch 46:
INFO  [19:27:40.265] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:27:40.265] [bbotk]               2.253756                         0.6968538
INFO  [19:27:40.265] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:27:40.265] [bbotk]                         0.8143822           -2.744545               5.882193
INFO  [19:27:40.265] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:27:40.265] [bbotk]                         11                    2346                 0.6226905
INFO  [19:27:40.265] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:27:40.265] [bbotk]  0.01828719 <list[8]>              FALSE     0.03990171        0      0
INFO  [19:27:40.265] [bbotk]  runtime_learners                                uhash
INFO  [19:27:40.265] [bbotk]           177.607 4f252d31-1e0b-49c9-ad94-c235a5ef2b56
INFO  [19:27:47.600] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 77 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:28:10.889] [bbotk] Evaluating 1 configuration(s)
INFO  [19:28:10.980] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:28:11.035] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:28:42.330] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:29:25.138] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:29:56.536] [mlr3] Finished benchmark
INFO  [19:29:57.933] [bbotk] Result of batch 47:
INFO  [19:29:58.036] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:29:58.036] [bbotk]              -2.314058                         0.1044227
INFO  [19:29:58.036] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:29:58.036] [bbotk]                         0.7004747            -2.88294              -6.650312
INFO  [19:29:58.036] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:29:58.036] [bbotk]                         10                    1263                 0.5902741
INFO  [19:29:58.036] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:29:58.036] [bbotk]  0.0175671 <list[8]>              FALSE      0.0261939        0      0
INFO  [19:29:58.036] [bbotk]  runtime_learners                                uhash
INFO  [19:29:58.036] [bbotk]           104.448 e43d72c3-bb41-4a95-bcef-0496be4106a0
WARN  [19:30:03.348] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:30:03.374] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 78 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:30:16.319] [bbotk] Evaluating 1 configuration(s)
INFO  [19:30:16.501] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:30:16.604] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:31:25.991] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:32:04.360] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:32:53.256] [mlr3] Finished benchmark
INFO  [19:32:54.160] [bbotk] Result of batch 48:
INFO  [19:32:54.183] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:32:54.183] [bbotk]              -5.380375                         0.4058445
INFO  [19:32:54.183] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:32:54.183] [bbotk]                         0.4989616           -2.303922               -1.32883
INFO  [19:32:54.183] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:32:54.183] [bbotk]                         18                    4403                 0.3511646
INFO  [19:32:54.183] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:32:54.183] [bbotk]  0.01730381 <list[8]>              FALSE     0.02475744        0      0
INFO  [19:32:54.183] [bbotk]  runtime_learners                                uhash
INFO  [19:32:54.183] [bbotk]           156.358 8f5d5d69-2cfa-46f8-9e85-69145361f3b9
INFO  [19:33:09.367] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 79 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:33:36.345] [bbotk] Evaluating 1 configuration(s)
INFO  [19:33:36.404] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:33:36.623] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:34:15.928] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:34:53.404] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:35:36.232] [mlr3] Finished benchmark
INFO  [19:35:36.896] [bbotk] Result of batch 49:
INFO  [19:35:36.949] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:35:36.949] [bbotk]              -3.305711                         0.6569176
INFO  [19:35:36.949] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:35:36.949] [bbotk]                         0.3951208           -0.407117              -1.345709
INFO  [19:35:36.949] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:35:36.949] [bbotk]                          2                    1544                 0.3959889
INFO  [19:35:36.949] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:35:36.949] [bbotk]  0.01691217 <list[8]>              FALSE      0.0209308        0      0
INFO  [19:35:36.949] [bbotk]  runtime_learners                                uhash
INFO  [19:35:36.949] [bbotk]            118.74 540ed167-a0c7-43c4-bbe6-1f3c0f2bcf69
INFO  [19:35:54.637] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 80 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:36:10.502] [bbotk] Evaluating 1 configuration(s)
INFO  [19:36:10.570] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:36:10.582] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:36:35.349] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:36:59.873] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [19:37:19.670] [mlr3] Finished benchmark
INFO  [19:37:20.052] [bbotk] Result of batch 50:
INFO  [19:37:20.120] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:37:20.120] [bbotk]               2.343276                         0.3486093
INFO  [19:37:20.120] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:37:20.120] [bbotk]                         0.3963766           -2.182246             -0.6345335
INFO  [19:37:20.120] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:37:20.120] [bbotk]                         16                     527                 0.9536944
INFO  [19:37:20.120] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:37:20.120] [bbotk]  0.01697808 <list[8]>              FALSE      0.0390308        0      0
INFO  [19:37:20.120] [bbotk]  runtime_learners                                uhash
INFO  [19:37:20.120] [bbotk]            68.589 c34dfc41-6965-4cd7-9cb6-665a878d74b8
INFO  [19:37:27.307] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 81 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:37:41.954] [bbotk] Evaluating 1 configuration(s)
INFO  [19:37:42.489] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:37:42.746] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:38:36.596] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:39:34.567] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:40:41.581] [mlr3] Finished benchmark
INFO  [19:40:42.163] [bbotk] Result of batch 51:
INFO  [19:40:42.184] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:40:42.184] [bbotk]              -1.105052                         0.1510023
INFO  [19:40:42.184] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:40:42.184] [bbotk]                         0.7570546           -3.507268              -2.407706
INFO  [19:40:42.184] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:40:42.184] [bbotk]                         12                    2966                 0.4528935
INFO  [19:40:42.184] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:40:42.184] [bbotk]  0.01639553 <list[8]>              FALSE     0.02437008        0      0
INFO  [19:40:42.184] [bbotk]  runtime_learners                                uhash
INFO  [19:40:42.184] [bbotk]           178.033 e161acaa-8552-4e11-924d-b82cb411b71d
INFO  [19:40:48.027] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 82 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:41:03.767] [bbotk] Evaluating 1 configuration(s)
INFO  [19:41:04.314] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:41:04.614] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:42:25.076] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:43:19.434] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:44:28.818] [mlr3] Finished benchmark
INFO  [19:44:28.970] [bbotk] Result of batch 52:
INFO  [19:44:28.977] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:44:28.977] [bbotk]             -0.7257299                         0.7208308
INFO  [19:44:28.977] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:44:28.977] [bbotk]                         0.9655962           -3.340366            -0.06683789
INFO  [19:44:28.977] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:44:28.977] [bbotk]                         17                    2756                 0.8889262
INFO  [19:44:28.977] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:44:28.977] [bbotk]  0.01859302 <list[8]>              FALSE      0.0327787        0      0
INFO  [19:44:28.977] [bbotk]  runtime_learners                                uhash
INFO  [19:44:28.977] [bbotk]           203.893 dea07e05-1d46-4b5e-8e1d-c766c386cb20
INFO  [19:44:37.882] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 83 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:44:46.711] [bbotk] Evaluating 1 configuration(s)
INFO  [19:44:46.748] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:44:46.763] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:46:15.640] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:48:13.457] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:50:12.959] [mlr3] Finished benchmark
INFO  [19:50:13.399] [bbotk] Result of batch 53:
INFO  [19:50:13.739] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:50:13.739] [bbotk]              0.3195925                         0.9244277
INFO  [19:50:13.739] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:50:13.739] [bbotk]                         0.9529484           -1.654305               5.579093
INFO  [19:50:13.739] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:50:13.739] [bbotk]                          2                    3746                 0.4183808
INFO  [19:50:13.739] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:50:13.739] [bbotk]  0.01754524 <list[8]>              FALSE     0.03265248        0      0
INFO  [19:50:13.739] [bbotk]  runtime_learners                                uhash
INFO  [19:50:13.739] [bbotk]           325.966 5fcd7db0-4320-4176-ac43-b27e02802bc5
INFO  [19:50:15.849] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 84 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:50:25.999] [bbotk] Evaluating 1 configuration(s)
INFO  [19:50:26.300] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:50:26.366] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:51:37.016] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:52:59.782] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:54:24.989] [mlr3] Finished benchmark
INFO  [19:54:27.389] [bbotk] Result of batch 54:
INFO  [19:54:27.572] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:54:27.572] [bbotk]              -2.377242                         0.3546036
INFO  [19:54:27.572] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:54:27.572] [bbotk]                         0.4082086             -1.5136               -3.49357
INFO  [19:54:27.572] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:54:27.572] [bbotk]                          1                    4855                 0.7129117
INFO  [19:54:27.572] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:54:27.572] [bbotk]  0.01583155 <list[8]>              FALSE     0.02547837        0      0
INFO  [19:54:27.572] [bbotk]  runtime_learners                                uhash
INFO  [19:54:27.572] [bbotk]           237.797 301f0cbd-76ff-48aa-836d-816c270f182e
WARN  [19:54:54.863] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:54:54.868] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 85 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:55:16.064] [bbotk] Evaluating 1 configuration(s)
INFO  [19:55:16.682] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:55:16.863] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:55:44.385] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:56:16.577] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:56:54.149] [mlr3] Finished benchmark
INFO  [19:56:54.319] [bbotk] Result of batch 55:
INFO  [19:56:54.342] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:56:54.342] [bbotk]               3.484033                         0.6513947
INFO  [19:56:54.342] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:56:54.342] [bbotk]                         0.9375017           -2.625603              -4.317646
INFO  [19:56:54.342] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:56:54.342] [bbotk]                         18                     338                  0.826441
INFO  [19:56:54.342] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:56:54.342] [bbotk]  0.02258666 <list[8]>              FALSE     0.04157681        0      0
INFO  [19:56:54.342] [bbotk]  runtime_learners                                uhash
INFO  [19:56:54.342] [bbotk]            97.075 61401927-2674-4863-bd3e-ef363a5cd34b
INFO  [19:56:58.544] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 86 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:57:25.970] [bbotk] Evaluating 1 configuration(s)
INFO  [19:57:26.193] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:57:26.415] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:58:18.163] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:59:18.335] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:00:06.598] [mlr3] Finished benchmark
INFO  [20:00:09.446] [bbotk] Result of batch 56:
INFO  [20:00:09.754] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:00:09.754] [bbotk]               2.522606                         0.2868849
INFO  [20:00:09.754] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:00:09.754] [bbotk]                         0.9943777           -4.945216              0.3020926
INFO  [20:00:09.754] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:00:09.754] [bbotk]                         13                    1026                   0.96311
INFO  [20:00:09.754] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:00:09.754] [bbotk]  0.01748759 <list[8]>              FALSE     0.04156644        0      0
INFO  [20:00:09.754] [bbotk]  runtime_learners                                uhash
INFO  [20:00:09.754] [bbotk]           159.509 c8275f38-4e33-445c-b0d6-16baefbcccc3
INFO  [20:00:42.958] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 87 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:01:19.004] [bbotk] Evaluating 1 configuration(s)
INFO  [20:01:19.208] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:01:19.479] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:02:00.671] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:03:05.067] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:04:43.342] [mlr3] Finished benchmark
INFO  [20:04:43.657] [bbotk] Result of batch 57:
INFO  [20:04:43.690] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:04:43.690] [bbotk]               2.332168                         0.8729996
INFO  [20:04:43.690] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:04:43.690] [bbotk]                         0.6847975          -0.5467204               5.145669
INFO  [20:04:43.690] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:04:43.690] [bbotk]                         20                    2423                 0.6156252
INFO  [20:04:43.690] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:04:43.690] [bbotk]  0.01882059 <list[8]>              FALSE     0.04004132        0      0
INFO  [20:04:43.690] [bbotk]  runtime_learners                                uhash
INFO  [20:04:43.690] [bbotk]           203.602 c1667f04-ff2c-46cc-8780-a9a1e576abe6
INFO  [20:04:46.332] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 88 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:04:59.236] [bbotk] Evaluating 1 configuration(s)
INFO  [20:04:59.359] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:04:59.462] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:07:02.127] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:08:43.020] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:11:48.743] [mlr3] Finished benchmark
INFO  [20:11:48.869] [bbotk] Result of batch 58:
INFO  [20:11:48.877] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:11:48.877] [bbotk]               2.386494                         0.6629504
INFO  [20:11:48.877] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:11:48.877] [bbotk]                         0.4673383           -6.935249              -2.391159
INFO  [20:11:48.877] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:11:48.877] [bbotk]                         20                    4011                 0.9608287
INFO  [20:11:48.877] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:11:48.877] [bbotk]  0.02568079 <list[8]>              FALSE     0.04184162        0      0
INFO  [20:11:48.877] [bbotk]  runtime_learners                                uhash
INFO  [20:11:48.877] [bbotk]           408.944 3cb8a865-1103-4f15-a4ed-4a517790459d
WARN  [20:12:24.074] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:12:24.183] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 89 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:12:35.110] [bbotk] Evaluating 1 configuration(s)
INFO  [20:12:35.487] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:12:35.675] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:13:20.293] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:14:47.670] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:16:32.875] [mlr3] Finished benchmark
INFO  [20:16:34.467] [bbotk] Result of batch 59:
INFO  [20:16:34.489] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:16:34.489] [bbotk]              0.6028223                         0.3596669
INFO  [20:16:34.489] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:16:34.489] [bbotk]                          0.807246           -1.247963               1.459703
INFO  [20:16:34.489] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:16:34.489] [bbotk]                          1                    4204                 0.2820438
INFO  [20:16:34.489] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:16:34.489] [bbotk]  0.01640589 <list[8]>              FALSE     0.02522528        0      0
INFO  [20:16:34.489] [bbotk]  runtime_learners                                uhash
INFO  [20:16:34.489] [bbotk]           236.355 e0b4c640-5e10-40f7-87e6-31154bab587c
INFO  [20:16:37.195] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 90 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:16:46.333] [bbotk] Evaluating 1 configuration(s)
INFO  [20:16:46.434] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:16:46.514] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:17:45.396] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:18:31.230] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:19:03.392] [mlr3] Finished benchmark
INFO  [20:19:03.541] [bbotk] Result of batch 60:
INFO  [20:19:03.554] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:19:03.554] [bbotk]            -0.06312971                         0.4290862
INFO  [20:19:03.554] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:19:03.554] [bbotk]                         0.8477658           -1.206241              0.9825162
INFO  [20:19:03.554] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:19:03.554] [bbotk]                         19                    1102                 0.3797112
INFO  [20:19:03.554] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:19:03.554] [bbotk]  0.01861171 <list[8]>              FALSE     0.02376688        0      0
INFO  [20:19:03.554] [bbotk]  runtime_learners                                uhash
INFO  [20:19:03.554] [bbotk]           136.804 1424465f-b21d-4807-b49e-b2c25001ef8f
WARN  [20:19:07.544] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:19:07.551] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 91 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:19:17.623] [bbotk] Evaluating 1 configuration(s)
INFO  [20:19:18.075] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:19:18.102] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:20:17.605] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:21:32.503] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:22:19.189] [mlr3] Finished benchmark
INFO  [20:22:19.928] [bbotk] Result of batch 61:
INFO  [20:22:19.942] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:22:19.942] [bbotk]              -5.802385                          0.130462
INFO  [20:22:19.942] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:22:19.942] [bbotk]                         0.2181777           -4.010775              -6.061365
INFO  [20:22:19.942] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:22:19.942] [bbotk]                         10                    3079                 0.8148029
INFO  [20:22:19.942] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:22:19.942] [bbotk]  0.01495774 <list[8]>              FALSE     0.02554787        0      0
INFO  [20:22:19.942] [bbotk]  runtime_learners                                uhash
INFO  [20:22:19.942] [bbotk]           180.392 3bbb1c54-8181-4484-b4a6-85e5731fae50
INFO  [20:22:23.721] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 92 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:22:34.366] [bbotk] Evaluating 1 configuration(s)
INFO  [20:22:34.454] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:22:35.064] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:23:02.328] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:23:32.502] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:24:17.413] [mlr3] Finished benchmark
INFO  [20:24:17.525] [bbotk] Result of batch 62:
INFO  [20:24:17.534] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:24:17.534] [bbotk]               -2.23366                         0.1244375
INFO  [20:24:17.534] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:24:17.534] [bbotk]                         0.3178316          -0.4660039              0.4584067
INFO  [20:24:17.534] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:24:17.534] [bbotk]                          5                    1573                 0.9331039
INFO  [20:24:17.534] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:24:17.534] [bbotk]  0.01684931 <list[8]>              FALSE     0.02568486        0      0
INFO  [20:24:17.534] [bbotk]  runtime_learners                                uhash
INFO  [20:24:17.534] [bbotk]           101.763 01378cce-b545-4593-81ae-36bbff45980b
WARN  [20:24:22.628] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:24:22.633] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 93 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:24:31.675] [bbotk] Evaluating 1 configuration(s)
INFO  [20:24:31.777] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:24:31.887] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:25:10.411] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:25:57.949] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:26:37.447] [mlr3] Finished benchmark
INFO  [20:26:37.549] [bbotk] Result of batch 63:
INFO  [20:26:37.558] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:26:37.558] [bbotk]             -0.9991669                         0.3106962
INFO  [20:26:37.558] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:26:37.558] [bbotk]                         0.9556775          -0.8472387                1.37688
INFO  [20:26:37.558] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:26:37.558] [bbotk]                         12                    1887                 0.3727559
INFO  [20:26:37.558] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:26:37.558] [bbotk]  0.01451527 <list[8]>              FALSE     0.02737114        0      0
INFO  [20:26:37.558] [bbotk]  runtime_learners                                uhash
INFO  [20:26:37.558] [bbotk]           125.333 cbc04c7a-6d97-4939-b358-0f7ce4d752a5
INFO  [20:26:42.537] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 94 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:26:52.037] [bbotk] Evaluating 1 configuration(s)
INFO  [20:26:52.144] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:26:52.199] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:27:19.924] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:27:53.982] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:28:15.672] [mlr3] Finished benchmark
INFO  [20:28:15.801] [bbotk] Result of batch 64:
INFO  [20:28:15.999] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:28:15.999] [bbotk]              -1.534776                         0.3182422
INFO  [20:28:15.999] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:28:15.999] [bbotk]                         0.1286629          -0.1305933              -2.141741
INFO  [20:28:15.999] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:28:15.999] [bbotk]                         15                     987                 0.2354226
INFO  [20:28:15.999] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:28:15.999] [bbotk]  0.01642005 <list[8]>              FALSE     0.03045635        0      0
INFO  [20:28:15.999] [bbotk]  runtime_learners                                uhash
INFO  [20:28:15.999] [bbotk]            83.201 a0d536b1-d203-4467-8309-e7cb723da006
INFO  [20:28:17.963] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 95 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:28:30.509] [bbotk] Evaluating 1 configuration(s)
INFO  [20:28:30.622] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:28:30.716] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:29:02.293] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:29:32.903] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:30:02.455] [mlr3] Finished benchmark
INFO  [20:30:02.569] [bbotk] Result of batch 65:
INFO  [20:30:02.613] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:30:02.613] [bbotk]              -3.718366                         0.1163963
INFO  [20:30:02.613] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:30:02.613] [bbotk]                         0.8885051           -0.814889              0.1774206
INFO  [20:30:02.613] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:30:02.613] [bbotk]                          5                    1333                   0.76763
INFO  [20:30:02.613] [bbotk]    acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:30:02.613] [bbotk]  0.014033 <list[8]>              FALSE     0.02420687        0      0
INFO  [20:30:02.613] [bbotk]  runtime_learners                                uhash
INFO  [20:30:02.613] [bbotk]            91.374 77be724d-4ab4-45e7-a952-5529133ff09a
INFO  [20:30:04.590] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 96 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:30:17.279] [bbotk] Evaluating 1 configuration(s)
INFO  [20:30:17.434] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:30:17.485] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:30:35.572] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:30:53.096] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:32:08.494] [mlr3] Finished benchmark
INFO  [20:32:08.669] [bbotk] Result of batch 66:
INFO  [20:32:08.679] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:32:08.679] [bbotk]              -5.066923                          0.650605
INFO  [20:32:08.679] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:32:08.679] [bbotk]                         0.4838869           -3.815685              -2.021592
INFO  [20:32:08.679] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:32:08.679] [bbotk]                          7                    2981                 0.9694958
INFO  [20:32:08.679] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:32:08.679] [bbotk]  0.01421998 <list[8]>              FALSE     0.03119058        0      0
INFO  [20:32:08.679] [bbotk]  runtime_learners                                uhash
INFO  [20:32:08.679] [bbotk]           110.827 cfcaaa66-97e1-479e-8d0e-b8105e50b448
WARN  [20:32:11.263] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:32:11.273] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 97 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:32:19.288] [bbotk] Evaluating 1 configuration(s)
INFO  [20:32:19.384] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:32:19.411] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:32:29.944] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:32:43.809] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [20:32:53.764] [mlr3] Finished benchmark
INFO  [20:32:53.943] [bbotk] Result of batch 67:
INFO  [20:32:53.967] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:32:53.967] [bbotk]              -3.993185                         0.1101147
INFO  [20:32:53.967] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:32:53.967] [bbotk]                         0.4396349           -4.741568             -0.7895678
INFO  [20:32:53.967] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:32:53.967] [bbotk]                          3                    1313                 0.1573451
INFO  [20:32:53.967] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:32:53.967] [bbotk]  0.01539916 <list[8]>              FALSE     0.03528953        0      0
INFO  [20:32:53.967] [bbotk]  runtime_learners                                uhash
INFO  [20:32:53.967] [bbotk]             34.04 09e460a5-3e12-47a6-b98f-ae6ee4de4d53
INFO  [20:32:55.362] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 98 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:33:01.240] [bbotk] Evaluating 1 configuration(s)
INFO  [20:33:01.378] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:33:01.426] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:33:23.602] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:33:48.182] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:34:11.097] [mlr3] Finished benchmark
INFO  [20:34:11.619] [bbotk] Result of batch 68:
INFO  [20:34:11.693] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:34:11.693] [bbotk]              -2.730542                          0.409253
INFO  [20:34:11.693] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:34:11.693] [bbotk]                         0.9573122           -7.777188              -5.682916
INFO  [20:34:11.693] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:34:11.693] [bbotk]                         18                    4442                 0.1747669
INFO  [20:34:11.693] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:34:11.693] [bbotk]  0.01320259 <list[8]>              FALSE     0.03632264        0      0
INFO  [20:34:11.693] [bbotk]  runtime_learners                                uhash
INFO  [20:34:11.693] [bbotk]            69.585 d49beed8-c181-416e-b9f0-dafed1922462
WARN  [20:34:16.038] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:34:16.363] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 99 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:34:21.445] [bbotk] Evaluating 1 configuration(s)
INFO  [20:34:21.469] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:34:21.544] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:34:38.329] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:34:56.111] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:35:17.037] [mlr3] Finished benchmark
INFO  [20:35:17.188] [bbotk] Result of batch 69:
INFO  [20:35:17.197] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:35:17.197] [bbotk]             -0.3346645                         0.7944935
INFO  [20:35:17.197] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:35:17.197] [bbotk]                         0.1492928           -1.259566             -0.9601462
INFO  [20:35:17.197] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:35:17.197] [bbotk]                          3                    3929                 0.1561735
INFO  [20:35:17.197] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:35:17.197] [bbotk]  0.01378201 <list[8]>              FALSE     0.02345939        0      0
INFO  [20:35:17.197] [bbotk]  runtime_learners                                uhash
INFO  [20:35:17.197] [bbotk]            55.393 7a5de004-280d-43e2-beee-06cbc11bfa1f
INFO  [20:35:21.153] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 100 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:35:21.221] [bbotk] Finished optimizing after 100 evaluation(s)
INFO  [20:35:21.235] [bbotk] Result:
INFO  [20:35:21.241] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:35:21.241] [bbotk]                  <num>                             <num>
INFO  [20:35:21.241] [bbotk]              -3.305711                         0.6569176
INFO  [20:35:21.241] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:35:21.241] [bbotk]                             <num>               <num>                  <num>
INFO  [20:35:21.241] [bbotk]                         0.3951208           -0.407117              -1.345709
INFO  [20:35:21.241] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:35:21.241] [bbotk]                      <int>                   <int>                     <num>
INFO  [20:35:21.241] [bbotk]                          2                    1544                 0.3959889
INFO  [20:35:21.241] [bbotk]  learner_param_vals  x_domain classif.bbrier
INFO  [20:35:21.241] [bbotk]              <list>    <list>          <num>
INFO  [20:35:21.241] [bbotk]          <list[10]> <list[8]>      0.0209308

### [bt]: Job terminated successfully [batchtools job.id=1438]
### [bt]: Calculation finished!
