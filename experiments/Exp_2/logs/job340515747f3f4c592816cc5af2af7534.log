### [bt]: This is batchtools v0.9.17
### [bt]: Starting calculation of 1 jobs
### [bt]: Setting working directory to '/content/drive/MyDrive/GitHub/Mater_Thesis_Calibration'
ece.R :
Friedmann_tasks.R :
Functions.R :
ici.R :
mse_feature_effects.R :
PipeOpCalibration_TuningExperiment.R :
PipeOpCalibration.R :
PipeOpCalibrationUnion.R :
### [bt]: Memory measurement disabled
### [bt]: Starting job [batchtools job.id=1433]
### [bt]: Generating problem instance for problem '341ee781c7d79b8f' ...
### [bt]: Applying algorithm 'run_learner' on problem '341ee781c7d79b8f' for job 1433 (seed = 1556) ...
INFO  [16:13:04.136] [mlr3] Applying learner 'xgboost TwP platt' on task 'wdbc' (iter 3/10)
INFO  [16:13:06.272] [bbotk] Starting to optimize 8 parameter(s) with '<OptimizerMbo>' and '<TerminatorEvals> [n_evals=100, k=0]'
INFO  [16:13:21.238] [bbotk] Evaluating 32 configuration(s)
INFO  [16:13:21.779] [mlr3] Running benchmark with 96 resampling iterations
INFO  [16:13:21.807] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:14:22.252] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:15:21.280] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:16:44.915] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:18:14.935] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:19:46.976] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:20:53.295] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:21:29.742] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:22:05.404] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:22:39.958] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:23:33.124] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:24:11.266] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:24:59.438] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:25:31.740] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:26:23.911] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:27:11.380] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:28:03.311] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:29:33.716] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:31:14.977] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:32:46.802] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:34:19.391] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:37:07.853] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:38:04.080] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:38:36.662] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:39:31.716] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:40:18.540] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:40:52.280] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:42:07.254] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:43:27.129] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:44:31.840] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:45:48.365] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:46:26.553] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:47:07.298] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:47:43.755] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:48:41.747] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:49:41.325] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:50:45.439] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:51:26.227] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:52:19.461] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:52:49.699] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:54:01.088] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:55:09.009] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:56:12.286] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:56:41.465] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:57:09.764] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:57:38.781] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:58:39.307] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:59:48.558] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:00:38.250] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:01:30.551] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:02:21.889] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:03:14.320] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:03:58.319] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:04:34.079] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:05:13.816] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:06:00.939] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:06:43.049] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:07:28.320] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:10:18.645] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:12:44.834] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:14:50.336] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:15:43.020] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:16:30.547] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:17:10.455] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:17:57.933] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:18:52.751] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:19:37.278] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:20:09.614] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:20:37.759] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:21:07.028] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:21:54.061] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:22:29.454] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:23:09.803] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:23:57.411] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:24:39.287] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:25:36.631] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:26:28.238] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:27:14.060] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:27:56.886] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:28:40.411] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:29:13.067] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:29:51.111] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:30:06.765] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:30:29.747] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:30:53.819] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:32:21.004] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:33:48.551] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:34:38.881] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:35:02.876] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:35:22.605] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:35:49.093] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:36:32.863] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:37:13.888] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:37:41.697] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:37:56.653] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:38:12.369] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:38:38.913] [mlr3] Finished benchmark
INFO  [17:38:39.989] [bbotk] Result of batch 1:
INFO  [17:38:40.034] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:38:40.034] [bbotk]            -1.38264825                         0.9412021
INFO  [17:38:40.034] [bbotk]             5.52510703                         0.4912021
INFO  [17:38:40.034] [bbotk]             2.07122939                         0.7162021
INFO  [17:38:40.034] [bbotk]            -4.83652609                         0.2662020
INFO  [17:38:40.034] [bbotk]            -6.56346488                         0.6037021
INFO  [17:38:40.034] [bbotk]             0.34429057                         0.1537021
INFO  [17:38:40.034] [bbotk]            -3.10958707                         0.3787021
INFO  [17:38:40.034] [bbotk]             3.79816821                         0.8287021
INFO  [17:38:40.034] [bbotk]             1.20775998                         0.9974521
INFO  [17:38:40.034] [bbotk]            -5.69999550                         0.5474521
INFO  [17:38:40.034] [bbotk]            -2.24611766                         0.7724521
INFO  [17:38:40.034] [bbotk]             4.66163762                         0.3224520
INFO  [17:38:40.034] [bbotk]            -3.97305668                         0.8849521
INFO  [17:38:40.034] [bbotk]             2.93469880                         0.4349521
INFO  [17:38:40.034] [bbotk]             6.38857644                         0.6599521
INFO  [17:38:40.034] [bbotk]            -0.51917884                         0.2099521
INFO  [17:38:40.034] [bbotk]             5.95684174                         0.9130771
INFO  [17:38:40.034] [bbotk]            -0.95091354                         0.4630771
INFO  [17:38:40.034] [bbotk]             2.50296410                         0.2380770
INFO  [17:38:40.034] [bbotk]            -4.40479139                         0.6880771
INFO  [17:38:40.034] [bbotk]            -6.13173021                         0.1255771
INFO  [17:38:40.034] [bbotk]             0.77602528                         0.5755771
INFO  [17:38:40.034] [bbotk]             4.22990292                         0.3505771
INFO  [17:38:40.034] [bbotk]            -2.67785236                         0.8005771
INFO  [17:38:40.034] [bbotk]             5.09337233                         0.7443271
INFO  [17:38:40.034] [bbotk]            -1.81438295                         0.2943270
INFO  [17:38:40.034] [bbotk]            -5.26826080                         0.9693271
INFO  [17:38:40.034] [bbotk]             1.63949469                         0.5193271
INFO  [17:38:40.034] [bbotk]            -0.08744413                         0.6318271
INFO  [17:38:40.034] [bbotk]             6.82031115                         0.1818271
INFO  [17:38:40.034] [bbotk]             3.36643351                         0.8568271
INFO  [17:38:40.034] [bbotk]            -3.54132198                         0.4068271
INFO  [17:38:40.034] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:38:40.034] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:38:40.034] [bbotk]                         0.8639982         -7.50047864              0.1363359
INFO  [17:38:40.034] [bbotk]                         0.4139982         -2.89530846             -6.7714194
INFO  [17:38:40.034] [bbotk]                         0.6389982         -5.19789355             -3.3175417
INFO  [17:38:40.034] [bbotk]                         0.1889982         -0.59272337              3.5902136
INFO  [17:38:40.034] [bbotk]                         0.5264982         -1.74401591              5.3171524
INFO  [17:38:40.034] [bbotk]                         0.9764982         -6.34918610             -1.5906029
INFO  [17:38:40.034] [bbotk]                         0.7514982         -8.65177119              1.8632748
INFO  [17:38:40.034] [bbotk]                         0.3014982         -4.04660100             -5.0444805
INFO  [17:38:40.034] [bbotk]                         0.4702482         -8.07612492              4.4536830
INFO  [17:38:40.034] [bbotk]                         0.9202482         -3.47095473             -2.4540723
INFO  [17:38:40.034] [bbotk]                         0.2452482         -5.77353982             -5.9079499
INFO  [17:38:40.034] [bbotk]                         0.6952482         -1.16836964              0.9998053
INFO  [17:38:40.034] [bbotk]                         0.5827482         -2.31966218             -0.7271335
INFO  [17:38:40.034] [bbotk]                         0.1327482         -6.92483237              6.1806218
INFO  [17:38:40.034] [bbotk]                         0.8077482         -0.01707709              2.7267442
INFO  [17:38:40.034] [bbotk]                         0.3577482         -4.62224728             -4.1810111
INFO  [17:38:40.034] [bbotk]                         0.7233732         -6.06136296              6.6123565
INFO  [17:38:40.034] [bbotk]                         0.2733732         -1.45619277             -0.2953988
INFO  [17:38:40.034] [bbotk]                         0.4983732         -3.75877787              3.1584789
INFO  [17:38:40.034] [bbotk]                         0.9483732         -8.36394805             -3.7492764
INFO  [17:38:40.034] [bbotk]                         0.6108732         -7.21265551             -5.4762152
INFO  [17:38:40.034] [bbotk]                         0.1608732         -2.60748532              1.4315400
INFO  [17:38:40.034] [bbotk]                         0.8358732         -4.91007041              4.8854177
INFO  [17:38:40.034] [bbotk]                         0.3858732         -0.30490023             -2.0223376
INFO  [17:38:40.034] [bbotk]                         0.3296232         -7.78830178             -1.1588682
INFO  [17:38:40.034] [bbotk]                         0.7796232         -3.18313159              5.7488871
INFO  [17:38:40.034] [bbotk]                         0.1046232         -5.48571669              2.2950095
INFO  [17:38:40.034] [bbotk]                         0.5546232         -0.88054650             -4.6127458
INFO  [17:38:40.034] [bbotk]                         0.6671232         -4.33442414              4.0219483
INFO  [17:38:40.034] [bbotk]                         0.2171232         -8.93959431             -2.8858070
INFO  [17:38:40.034] [bbotk]                         0.8921232         -2.03183905             -6.3396846
INFO  [17:38:40.034] [bbotk]                         0.4421232         -6.63700923              0.5680706
INFO  [17:38:40.034] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:38:40.034] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:38:40.034] [bbotk]                         15                    1554                 0.1788579
INFO  [17:38:40.034] [bbotk]                          5                    4054                 0.6288579
INFO  [17:38:40.034] [bbotk]                         10                     304                 0.8538579
INFO  [17:38:40.034] [bbotk]                         20                    2804                 0.4038579
INFO  [17:38:40.034] [bbotk]                         13                     929                 0.9663579
INFO  [17:38:40.034] [bbotk]                          3                    3429                 0.5163579
INFO  [17:38:40.034] [bbotk]                         18                    4679                 0.7413579
INFO  [17:38:40.034] [bbotk]                          8                    2179                 0.2913579
INFO  [17:38:40.034] [bbotk]                          4                    2492                 0.2351079
INFO  [17:38:40.034] [bbotk]                         14                    4992                 0.6851079
INFO  [17:38:40.034] [bbotk]                         19                    1242                 0.9101079
INFO  [17:38:40.034] [bbotk]                          9                    3742                 0.4601079
INFO  [17:38:40.034] [bbotk]                         16                    1867                 0.1226079
INFO  [17:38:40.034] [bbotk]                          6                    4367                 0.5726079
INFO  [17:38:40.034] [bbotk]                          1                     617                 0.7976079
INFO  [17:38:40.034] [bbotk]                         11                    3117                 0.3476079
INFO  [17:38:40.034] [bbotk]                         19                    2961                 0.2069829
INFO  [17:38:40.034] [bbotk]                          9                     461                 0.6569829
INFO  [17:38:40.034] [bbotk]                         14                    1711                 0.4319829
INFO  [17:38:40.034] [bbotk]                          4                    4211                 0.8819829
INFO  [17:38:40.034] [bbotk]                          7                    2336                 0.5444829
INFO  [17:38:40.034] [bbotk]                         17                    4836                 0.9944829
INFO  [17:38:40.034] [bbotk]                         12                    1086                 0.7694829
INFO  [17:38:40.034] [bbotk]                          2                    3586                 0.3194829
INFO  [17:38:40.034] [bbotk]                         13                    4523                 0.9382329
INFO  [17:38:40.034] [bbotk]                          3                    2023                 0.4882329
INFO  [17:38:40.034] [bbotk]                          8                    3273                 0.2632329
INFO  [17:38:40.034] [bbotk]                         18                     773                 0.7132329
INFO  [17:38:40.034] [bbotk]                          6                    3898                 0.8257329
INFO  [17:38:40.034] [bbotk]                         16                    1398                 0.3757329
INFO  [17:38:40.034] [bbotk]                         11                    2648                 0.1507329
INFO  [17:38:40.034] [bbotk]                          1                     148                 0.6007329
INFO  [17:38:40.034] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:38:40.034] [bbotk]  classif.bbrier warnings errors runtime_learners
INFO  [17:38:40.034] [bbotk]      0.03640003        0      0          202.784
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0          247.856
INFO  [17:38:40.034] [bbotk]      0.03782642        0      0          106.180
INFO  [17:38:40.034] [bbotk]      0.02309691        0      0          139.225
INFO  [17:38:40.034] [bbotk]      0.02408343        0      0          131.449
INFO  [17:38:40.034] [bbotk]      0.02836169        0      0          242.675
INFO  [17:38:40.034] [bbotk]      0.03585750        0      0          351.979
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0          143.548
INFO  [17:38:40.034] [bbotk]      0.19079262        0      0          155.185
INFO  [17:38:40.034] [bbotk]      0.02006943        0      0          220.706
INFO  [17:38:40.034] [bbotk]      0.02766722        0      0          114.636
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0          181.448
INFO  [17:38:40.034] [bbotk]      0.02072036        0      0          123.157
INFO  [17:38:40.034] [bbotk]      0.20341597        0      0          202.340
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0           85.561
INFO  [17:38:40.034] [bbotk]      0.02245431        0      0          178.271
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0          154.681
INFO  [17:38:40.034] [bbotk]      0.02109744        0      0          118.241
INFO  [17:38:40.034] [bbotk]      0.04028877        0      0          133.638
INFO  [17:38:40.034] [bbotk]      0.03241502        0      0          441.294
INFO  [17:38:40.034] [bbotk]      0.02986499        0      0          139.806
INFO  [17:38:40.034] [bbotk]      0.02430762        0      0          146.427
INFO  [17:38:40.034] [bbotk]      0.27177293        0      0           89.204
INFO  [17:38:40.034] [bbotk]      0.02179124        0      0          122.050
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0          146.459
INFO  [17:38:40.034] [bbotk]      0.03447091        0      0          139.899
INFO  [17:38:40.034] [bbotk]      0.03239609        0      0          113.998
INFO  [17:38:40.034] [bbotk]      0.02988774        0      0           62.359
INFO  [17:38:40.034] [bbotk]      0.02507626        0      0          224.230
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0           69.605
INFO  [17:38:40.034] [bbotk]      0.27181575        0      0          111.966
INFO  [17:38:40.034] [bbotk]      0.10188497        0      0           51.718
INFO  [17:38:40.034] [bbotk]  classif.bbrier warnings errors runtime_learners
INFO  [17:38:40.034] [bbotk]                                 uhash
INFO  [17:38:40.034] [bbotk]  5efde7d8-26b3-408f-9e27-53d3195ebcff
INFO  [17:38:40.034] [bbotk]  a8376d88-a2b0-4832-9914-119c0e5e375f
INFO  [17:38:40.034] [bbotk]  b88c71ce-b671-4994-925a-f239c088fd16
INFO  [17:38:40.034] [bbotk]  4285bf0c-2a84-4429-b35e-42a997e452ee
INFO  [17:38:40.034] [bbotk]  483e40ed-3e9e-41ca-aab4-64b605161565
INFO  [17:38:40.034] [bbotk]  f624d6f1-7b6d-404d-9440-2f709cb9f3a3
INFO  [17:38:40.034] [bbotk]  608fbe33-c8b4-4d9c-8698-d3e3613a94ec
INFO  [17:38:40.034] [bbotk]  a631bcdc-18e6-495c-977a-9ac8e5172839
INFO  [17:38:40.034] [bbotk]  a7f4e542-3330-4181-a0f7-109dd4beff4b
INFO  [17:38:40.034] [bbotk]  8731d193-39b4-4171-970a-1d5004dc9525
INFO  [17:38:40.034] [bbotk]  badad259-d547-4dc4-9fd7-4624bc2487f6
INFO  [17:38:40.034] [bbotk]  1534a36a-f671-43c6-a718-01a0bfc86f6e
INFO  [17:38:40.034] [bbotk]  dd9706cf-9c70-40d6-b1bd-dc12f6827eff
INFO  [17:38:40.034] [bbotk]  31c9043d-bee1-4800-8513-4c214c888cea
INFO  [17:38:40.034] [bbotk]  eb407cc0-e476-480f-9c88-11cb4703ee3c
INFO  [17:38:40.034] [bbotk]  4056c5e1-0832-477c-8345-3465caa4796a
INFO  [17:38:40.034] [bbotk]  c081990c-c831-4365-a74f-dcfaf58a2edd
INFO  [17:38:40.034] [bbotk]  0ed1b67f-09f7-47e8-9a68-83f3e3667340
INFO  [17:38:40.034] [bbotk]  2850c830-3aca-4c3a-b8b9-06f3ecd8c5d0
INFO  [17:38:40.034] [bbotk]  88d8364f-e3ad-4f05-a4e1-33f8b6783a1c
INFO  [17:38:40.034] [bbotk]  1980e0f6-b981-4395-9bb9-fa1ec9ce3bb4
INFO  [17:38:40.034] [bbotk]  6519d594-fbda-40c1-9205-deff266dbfa7
INFO  [17:38:40.034] [bbotk]  4fbe4dc8-e33b-423a-9256-1ecc162f7d34
INFO  [17:38:40.034] [bbotk]  13360e2e-da79-4837-b838-6614abd17da3
INFO  [17:38:40.034] [bbotk]  17c321be-ab7b-43d2-b312-512564bba59a
INFO  [17:38:40.034] [bbotk]  9679436c-7693-437c-a182-277f4fb3ce58
INFO  [17:38:40.034] [bbotk]  d22b7814-e0e6-43e5-8dce-0dd8cea1fe08
INFO  [17:38:40.034] [bbotk]  17bf2017-9cec-4a5d-a37c-87f615a2ac6b
INFO  [17:38:40.034] [bbotk]  12949460-ccf8-4fb4-bdc1-da7a0b05934e
INFO  [17:38:40.034] [bbotk]  d87c524b-8084-4ac8-8742-712824b9ed64
INFO  [17:38:40.034] [bbotk]  c6016698-437c-421c-a16f-ab5710dbea52
INFO  [17:38:40.034] [bbotk]  074fd993-9565-4821-9b7f-d9c3cb5fb8fb
INFO  [17:38:40.034] [bbotk]                                 uhash
WARN  [17:38:47.656] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [17:38:47.679] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 32 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:38:51.701] [bbotk] Evaluating 1 configuration(s)
INFO  [17:38:51.859] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:38:51.922] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:39:13.328] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:39:34.194] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:39:56.895] [mlr3] Finished benchmark
INFO  [17:39:57.422] [bbotk] Result of batch 2:
INFO  [17:39:57.446] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:39:57.446] [bbotk]               1.140538                         0.1028488
INFO  [17:39:57.446] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:39:57.446] [bbotk]                         0.7440055           -2.975157             -0.8583342
INFO  [17:39:57.446] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:39:57.446] [bbotk]                         14                     748                 0.9450015
INFO  [17:39:57.446] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:39:57.446] [bbotk]  0.03557909 <list[8]>              FALSE     0.02731473        0      0
INFO  [17:39:57.446] [bbotk]  runtime_learners                                uhash
INFO  [17:39:57.446] [bbotk]            64.257 ef31faf8-c9c4-498d-a5b4-1471e0c14a1f
WARN  [17:39:58.913] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [17:39:58.967] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 33 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:40:04.071] [bbotk] Evaluating 1 configuration(s)
INFO  [17:40:04.261] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:40:04.560] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:40:43.079] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:41:42.998] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:42:13.234] [mlr3] Finished benchmark
INFO  [17:42:13.307] [bbotk] Result of batch 3:
INFO  [17:42:13.326] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:42:13.326] [bbotk]              -1.738722                         0.5951287
INFO  [17:42:13.326] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:42:13.326] [bbotk]                         0.7921855          -0.8596729               1.222061
INFO  [17:42:13.326] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:42:13.326] [bbotk]                         20                    4121                 0.7314417
INFO  [17:42:13.326] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:42:13.326] [bbotk]  0.02853611 <list[8]>              FALSE     0.01844611        0      0
INFO  [17:42:13.326] [bbotk]  runtime_learners                                uhash
INFO  [17:42:13.326] [bbotk]           128.204 2bd6171a-5af8-42e1-9052-4fba60048320
INFO  [17:42:14.014] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 34 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:42:20.071] [bbotk] Evaluating 1 configuration(s)
INFO  [17:42:20.229] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:42:20.380] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:42:47.468] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:43:25.104] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:44:00.924] [mlr3] Finished benchmark
INFO  [17:44:01.025] [bbotk] Result of batch 4:
INFO  [17:44:01.063] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:44:01.063] [bbotk]             -0.1325284                         0.3264049
INFO  [17:44:01.063] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:44:01.063] [bbotk]                         0.5017894            -2.65303               1.449039
INFO  [17:44:01.063] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:44:01.063] [bbotk]                         19                    2872                 0.8685329
INFO  [17:44:01.063] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:44:01.063] [bbotk]  0.02072472 <list[8]>              FALSE     0.02263036        0      0
INFO  [17:44:01.063] [bbotk]  runtime_learners                                uhash
INFO  [17:44:01.063] [bbotk]           100.047 cc07110f-6bbb-409f-ba7f-9b851f5301f4
WARN  [17:44:01.784] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [17:44:01.838] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 35 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:44:06.107] [bbotk] Evaluating 1 configuration(s)
INFO  [17:44:06.139] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:44:06.230] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:44:26.443] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:45:05.678] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:45:49.074] [mlr3] Finished benchmark
INFO  [17:45:49.191] [bbotk] Result of batch 5:
INFO  [17:45:49.224] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:45:49.224] [bbotk]              -1.753539                         0.5938366
INFO  [17:45:49.224] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:45:49.224] [bbotk]                         0.7889641           -2.809518              -4.114094
INFO  [17:45:49.224] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:45:49.224] [bbotk]                          9                    3930                  0.702219
INFO  [17:45:49.224] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:45:49.224] [bbotk]  0.01835197 <list[8]>              FALSE      0.0197005        0      0
INFO  [17:45:49.224] [bbotk]  runtime_learners                                uhash
INFO  [17:45:49.224] [bbotk]           102.262 7e530f3b-1871-45a6-ab11-0ad16edcd764
INFO  [17:45:49.875] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 36 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:45:55.764] [bbotk] Evaluating 1 configuration(s)
INFO  [17:45:55.890] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:45:56.056] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:46:14.090] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:46:31.017] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:46:50.264] [mlr3] Finished benchmark
INFO  [17:46:50.382] [bbotk] Result of batch 6:
INFO  [17:46:50.412] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:46:50.412] [bbotk]               2.013463                         0.2331886
INFO  [17:46:50.412] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:46:50.412] [bbotk]                         0.3496911           -7.551077              -4.688509
INFO  [17:46:50.412] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:46:50.412] [bbotk]                         13                     239                 0.9960108
INFO  [17:46:50.412] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:46:50.412] [bbotk]  0.01992478 <list[8]>              FALSE      0.1440738        0      0
INFO  [17:46:50.412] [bbotk]  runtime_learners                                uhash
INFO  [17:46:50.412] [bbotk]            53.546 f4a5e0ad-e7ed-4810-bd30-ecd71d70c242
INFO  [17:46:51.493] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 37 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:46:57.772] [bbotk] Evaluating 1 configuration(s)
INFO  [17:46:58.001] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:46:58.165] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:47:36.934] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:48:12.311] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:48:43.552] [mlr3] Finished benchmark
INFO  [17:48:43.771] [bbotk] Result of batch 7:
INFO  [17:48:43.814] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:48:43.814] [bbotk]             -0.0719115                         0.2176431
INFO  [17:48:43.814] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:48:43.814] [bbotk]                         0.5878223          -0.1077851              0.3990504
INFO  [17:48:43.814] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:48:43.814] [bbotk]                         18                    2431                  0.843135
INFO  [17:48:43.814] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:48:43.814] [bbotk]  0.02643188 <list[8]>              FALSE     0.02469149        0      0
INFO  [17:48:43.814] [bbotk]  runtime_learners                                uhash
INFO  [17:48:43.814] [bbotk]           105.077 c620a3bc-0406-4520-a1e2-7ce27f6c1a23
INFO  [17:48:44.547] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 38 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:48:53.484] [bbotk] Evaluating 1 configuration(s)
INFO  [17:48:53.617] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:48:53.745] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:49:23.297] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:49:51.972] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:50:27.092] [mlr3] Finished benchmark
INFO  [17:50:27.265] [bbotk] Result of batch 8:
INFO  [17:50:27.298] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:50:27.298] [bbotk]             -0.3328487                         0.5254706
INFO  [17:50:27.298] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:50:27.298] [bbotk]                         0.9206544           -2.167444              -2.221822
INFO  [17:50:27.298] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:50:27.298] [bbotk]                         13                    1799                 0.9091101
INFO  [17:50:27.298] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:50:27.298] [bbotk]  0.01959985 <list[8]>              FALSE     0.02165699        0      0
INFO  [17:50:27.298] [bbotk]  runtime_learners                                uhash
INFO  [17:50:27.298] [bbotk]            92.761 3ed400c9-c3f2-4685-9960-8cd658bbeba8
WARN  [17:50:28.069] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [17:50:28.097] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 39 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:50:36.028] [bbotk] Evaluating 1 configuration(s)
INFO  [17:50:36.367] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:50:36.550] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:51:01.935] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:51:28.784] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:51:55.764] [mlr3] Finished benchmark
INFO  [17:51:55.902] [bbotk] Result of batch 9:
INFO  [17:51:56.225] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:51:56.225] [bbotk]               1.294887                          0.150557
INFO  [17:51:56.225] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:51:56.225] [bbotk]                          0.724304           -2.502048             -0.1233525
INFO  [17:51:56.225] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:51:56.225] [bbotk]                          8                    1495                 0.4545611
INFO  [17:51:56.225] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:51:56.225] [bbotk]  0.04293091 <list[8]>              FALSE     0.02922698        0      0
INFO  [17:51:56.225] [bbotk]  runtime_learners                                uhash
INFO  [17:51:56.225] [bbotk]            78.608 9d063566-5afe-4f1a-9586-594261b4c42c
INFO  [17:51:59.802] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 40 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:52:08.983] [bbotk] Evaluating 1 configuration(s)
INFO  [17:52:09.170] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:52:09.388] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:52:36.461] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:53:10.295] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:53:41.189] [mlr3] Finished benchmark
INFO  [17:53:42.059] [bbotk] Result of batch 10:
INFO  [17:53:42.424] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:53:42.424] [bbotk]               0.412236                          0.345213
INFO  [17:53:42.424] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:53:42.424] [bbotk]                         0.9260671           -1.414594               5.251957
INFO  [17:53:42.424] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:53:42.424] [bbotk]                         20                    1414                 0.4579706
INFO  [17:53:42.424] [bbotk]    acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:53:42.424] [bbotk]  0.023031 <list[8]>              FALSE     0.02558547        0      0
INFO  [17:53:42.424] [bbotk]  runtime_learners                                uhash
INFO  [17:53:42.424] [bbotk]            90.978 b77b7f62-2d82-4457-8c47-03d48937b431
INFO  [17:53:43.748] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 41 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:53:49.613] [bbotk] Evaluating 1 configuration(s)
INFO  [17:53:49.728] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:53:49.800] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:54:00.603] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:54:10.105] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:54:26.542] [mlr3] Finished benchmark
INFO  [17:54:26.875] [bbotk] Result of batch 11:
INFO  [17:54:27.015] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:54:27.015] [bbotk]               1.527808                         0.5948809
INFO  [17:54:27.015] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:54:27.015] [bbotk]                         0.5494594           -3.445625               6.105517
INFO  [17:54:27.015] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:54:27.015] [bbotk]                         18                       9                 0.9732478
INFO  [17:54:27.015] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:54:27.015] [bbotk]  0.01871636 <list[8]>              FALSE      0.2403065        0      0
INFO  [17:54:27.015] [bbotk]  runtime_learners                                uhash
INFO  [17:54:27.015] [bbotk]            36.186 d067d7dd-6ce8-46a3-bc9b-e7f31d4457aa
INFO  [17:54:28.856] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 42 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:54:34.577] [bbotk] Evaluating 1 configuration(s)
INFO  [17:54:34.797] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:54:34.931] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:55:09.250] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:55:46.686] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:56:16.911] [mlr3] Finished benchmark
INFO  [17:56:17.028] [bbotk] Result of batch 12:
INFO  [17:56:17.060] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:56:17.060] [bbotk]             -0.1861885                         0.4816832
INFO  [17:56:17.060] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:56:17.060] [bbotk]                         0.5771478           -2.385345             -0.8178216
INFO  [17:56:17.060] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:56:17.060] [bbotk]                         20                    2123                 0.4893903
INFO  [17:56:17.060] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:56:17.060] [bbotk]  0.0330511 <list[8]>              FALSE     0.01916833        0      0
INFO  [17:56:17.060] [bbotk]  runtime_learners                                uhash
INFO  [17:56:17.060] [bbotk]           101.542 4eb4792b-4577-41d8-a0ea-18dd8f1f9bad
INFO  [17:56:17.939] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 43 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:56:24.623] [bbotk] Evaluating 1 configuration(s)
INFO  [17:56:24.684] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:56:24.767] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:57:08.851] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:57:39.455] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:58:13.109] [mlr3] Finished benchmark
INFO  [17:58:13.348] [bbotk] Result of batch 13:
INFO  [17:58:13.404] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:58:13.404] [bbotk]              -0.525238                         0.1989634
INFO  [17:58:13.404] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:58:13.404] [bbotk]                         0.4302067            -2.92647             -0.4778653
INFO  [17:58:13.404] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:58:13.404] [bbotk]                         13                    3844                 0.7199747
INFO  [17:58:13.404] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:58:13.404] [bbotk]  0.0286951 <list[8]>              FALSE     0.02111676        0      0
INFO  [17:58:13.404] [bbotk]  runtime_learners                                uhash
INFO  [17:58:13.404] [bbotk]           107.267 1dbd3613-6bee-48a4-ab1d-10d76f10721f
INFO  [17:58:15.348] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 44 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:58:32.458] [bbotk] Evaluating 1 configuration(s)
INFO  [17:58:32.553] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:58:32.810] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:59:11.861] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:00:00.331] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:00:39.537] [mlr3] Finished benchmark
INFO  [18:00:40.091] [bbotk] Result of batch 14:
INFO  [18:00:40.241] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:00:40.241] [bbotk]                -4.4666                          0.666994
INFO  [18:00:40.241] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:00:40.241] [bbotk]                         0.6263524           -2.148165              -1.290306
INFO  [18:00:40.241] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:00:40.241] [bbotk]                         15                    2959                 0.5047618
INFO  [18:00:40.241] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:00:40.241] [bbotk]  0.02764589 <list[8]>              FALSE     0.01757312        0      0
INFO  [18:00:40.241] [bbotk]  runtime_learners                                uhash
INFO  [18:00:40.241] [bbotk]           125.673 625fcfcb-8824-41ca-b0ac-5a5a6069f557
INFO  [18:00:43.046] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 45 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:00:50.356] [bbotk] Evaluating 1 configuration(s)
INFO  [18:00:50.694] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:00:50.986] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:01:25.897] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:02:11.787] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:02:57.623] [mlr3] Finished benchmark
INFO  [18:02:57.732] [bbotk] Result of batch 15:
INFO  [18:02:57.786] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:02:57.786] [bbotk]             -0.2099451                         0.2374192
INFO  [18:02:57.786] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:02:57.786] [bbotk]                         0.6461941         -0.04797298               3.447684
INFO  [18:02:57.786] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:02:57.786] [bbotk]                         13                    4185                 0.4039931
INFO  [18:02:57.786] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:02:57.786] [bbotk]  0.04388612 <list[8]>              FALSE     0.01760873        0      0
INFO  [18:02:57.786] [bbotk]  runtime_learners                                uhash
INFO  [18:02:57.786] [bbotk]           126.264 a988a8fb-6779-4d2b-82e5-b833acad7cea
WARN  [18:02:59.111] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:02:59.293] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 46 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:03:11.329] [bbotk] Evaluating 1 configuration(s)
INFO  [18:03:11.499] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:03:11.633] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:03:33.382] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:03:59.038] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:04:21.066] [mlr3] Finished benchmark
INFO  [18:04:21.176] [bbotk] Result of batch 16:
INFO  [18:04:21.294] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:04:21.294] [bbotk]              0.5073953                         0.9883315
INFO  [18:04:21.294] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:04:21.294] [bbotk]                         0.2019635           -2.292456              -2.577961
INFO  [18:04:21.294] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:04:21.294] [bbotk]                         12                    1216                 0.8242675
INFO  [18:04:21.294] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:04:21.294] [bbotk]  0.02749141 <list[8]>              FALSE     0.02517504        0      0
INFO  [18:04:21.294] [bbotk]  runtime_learners                                uhash
INFO  [18:04:21.294] [bbotk]            68.657 b36f5e81-1d17-4781-a4eb-868163728232
INFO  [18:04:22.942] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 47 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:04:26.364] [bbotk] Evaluating 1 configuration(s)
INFO  [18:04:26.404] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:04:26.471] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:04:54.690] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:05:35.402] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:06:27.082] [mlr3] Finished benchmark
INFO  [18:06:27.259] [bbotk] Result of batch 17:
INFO  [18:06:27.282] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:06:27.282] [bbotk]              0.8015314                         0.1556619
INFO  [18:06:27.282] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:06:27.282] [bbotk]                         0.8412192           -3.887038               3.619844
INFO  [18:06:27.282] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:06:27.282] [bbotk]                         19                    2417                 0.4413442
INFO  [18:06:27.282] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:06:27.282] [bbotk]  0.02255175 <list[8]>              FALSE     0.03085235        0      0
INFO  [18:06:27.282] [bbotk]  runtime_learners                                uhash
INFO  [18:06:27.282] [bbotk]           120.136 4f16ae4d-4b1d-4ca7-a52c-0e221b910259
WARN  [18:06:29.862] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:06:29.930] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 48 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:06:34.689] [bbotk] Evaluating 1 configuration(s)
INFO  [18:06:34.852] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:06:35.117] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:07:12.831] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:07:50.224] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:08:26.287] [mlr3] Finished benchmark
INFO  [18:08:26.512] [bbotk] Result of batch 18:
INFO  [18:08:26.622] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:08:26.622] [bbotk]            -0.04952629                          0.640026
INFO  [18:08:26.622] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:08:26.622] [bbotk]                         0.4003246             -1.4566              -2.268948
INFO  [18:08:26.622] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:08:26.622] [bbotk]                          8                    2901                 0.8222114
INFO  [18:08:26.622] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:08:26.622] [bbotk]  0.02129767 <list[8]>              FALSE     0.02220636        0      0
INFO  [18:08:26.622] [bbotk]  runtime_learners                                uhash
INFO  [18:08:26.622] [bbotk]           110.242 aa89d581-68c5-446b-9e7f-562421da22dc
WARN  [18:08:28.218] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:08:28.304] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 49 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:08:36.263] [bbotk] Evaluating 1 configuration(s)
INFO  [18:08:36.528] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:08:36.603] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:09:22.758] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:10:10.630] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:10:55.854] [mlr3] Finished benchmark
INFO  [18:10:56.305] [bbotk] Result of batch 19:
INFO  [18:10:56.578] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:10:56.578] [bbotk]              -3.280432                         0.6806613
INFO  [18:10:56.578] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:10:56.578] [bbotk]                         0.4688399           -5.897643               -4.95357
INFO  [18:10:56.578] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:10:56.578] [bbotk]                         19                    3640                 0.3451338
INFO  [18:10:56.578] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:10:56.578] [bbotk]  0.02647741 <list[8]>              FALSE     0.02165132        0      0
INFO  [18:10:56.578] [bbotk]  runtime_learners                                uhash
INFO  [18:10:56.578] [bbotk]           137.941 878ec1ef-5228-4c59-81a7-d2c69a019a11
INFO  [18:10:58.033] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 50 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:11:03.712] [bbotk] Evaluating 1 configuration(s)
INFO  [18:11:04.029] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:11:04.134] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:11:34.355] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:11:57.670] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:12:29.010] [mlr3] Finished benchmark
INFO  [18:12:29.107] [bbotk] Result of batch 20:
INFO  [18:12:29.140] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:12:29.140] [bbotk]            -0.06476729                          0.105527
INFO  [18:12:29.140] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:12:29.140] [bbotk]                         0.3771743           -2.029865               0.376629
INFO  [18:12:29.140] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:12:29.140] [bbotk]                         13                    1564                 0.2124189
INFO  [18:12:29.140] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:12:29.140] [bbotk]  0.02800769 <list[8]>              FALSE     0.02278803        0      0
INFO  [18:12:29.140] [bbotk]  runtime_learners                                uhash
INFO  [18:12:29.140] [bbotk]            82.787 1ab2a624-7d1b-413e-8c84-db86a4cd0fdf
WARN  [18:12:31.026] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:12:31.041] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 51 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:12:36.342] [bbotk] Evaluating 1 configuration(s)
INFO  [18:12:36.528] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:12:36.615] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:12:56.550] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:13:36.020] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:14:26.368] [mlr3] Finished benchmark
INFO  [18:14:26.521] [bbotk] Result of batch 21:
INFO  [18:14:26.552] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:14:26.552] [bbotk]             -0.9578127                         0.2890341
INFO  [18:14:26.552] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:14:26.552] [bbotk]                         0.8737826          -0.2715809             -0.9127316
INFO  [18:14:26.552] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:14:26.552] [bbotk]                         10                    1776                 0.4169384
INFO  [18:14:26.552] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:14:26.552] [bbotk]  0.02014079 <list[8]>              FALSE     0.02321083        0      0
INFO  [18:14:26.552] [bbotk]  runtime_learners                                uhash
INFO  [18:14:26.552] [bbotk]           109.152 9cce5485-4eb9-4369-afd1-da50b5478545
INFO  [18:14:28.159] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 52 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:14:37.790] [bbotk] Evaluating 1 configuration(s)
INFO  [18:14:37.976] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:14:38.089] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:15:11.295] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:15:51.907] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:16:37.332] [mlr3] Finished benchmark
INFO  [18:16:37.442] [bbotk] Result of batch 22:
INFO  [18:16:37.529] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:16:37.529] [bbotk]              -6.413234                         0.8430666
INFO  [18:16:37.529] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:16:37.529] [bbotk]                         0.3270844          -0.3904791               1.811446
INFO  [18:16:37.529] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:16:37.529] [bbotk]                         15                    3742                  0.753386
INFO  [18:16:37.529] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:16:37.529] [bbotk]  0.01818667 <list[8]>              FALSE     0.01821018        0      0
INFO  [18:16:37.529] [bbotk]  runtime_learners                                uhash
INFO  [18:16:37.529] [bbotk]           118.332 b1b23ba2-cd2b-4ebd-9f75-b062d1de3dd3
INFO  [18:16:39.405] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 53 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:16:46.275] [bbotk] Evaluating 1 configuration(s)
INFO  [18:16:46.404] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:16:46.621] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:16:59.226] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:17:10.272] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:17:26.086] [mlr3] Finished benchmark
INFO  [18:17:26.289] [bbotk] Result of batch 23:
INFO  [18:17:26.345] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:17:26.345] [bbotk]              0.5286725                         0.5106726
INFO  [18:17:26.345] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:17:26.345] [bbotk]                         0.8640169           -1.039259              -3.699172
INFO  [18:17:26.345] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:17:26.345] [bbotk]                         11                      46                 0.6421404
INFO  [18:17:26.345] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:17:26.345] [bbotk]  0.01716263 <list[8]>              FALSE     0.02244841        0      0
INFO  [18:17:26.345] [bbotk]  runtime_learners                                uhash
INFO  [18:17:26.345] [bbotk]            38.664 a7d90904-eda7-479c-99bf-7345c19307f9
INFO  [18:17:27.867] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 54 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:17:36.562] [bbotk] Evaluating 1 configuration(s)
INFO  [18:17:36.965] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:17:37.467] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:18:18.743] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:18:57.821] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:19:39.941] [mlr3] Finished benchmark
INFO  [18:19:40.064] [bbotk] Result of batch 24:
INFO  [18:19:40.115] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:19:40.115] [bbotk]              -5.839308                         0.5558037
INFO  [18:19:40.115] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:19:40.115] [bbotk]                          0.658147           -1.668745               4.128824
INFO  [18:19:40.115] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:19:40.115] [bbotk]                          9                    2843                 0.7372429
INFO  [18:19:40.115] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:19:40.115] [bbotk]  0.01688841 <list[8]>              FALSE     0.01914628        0      0
INFO  [18:19:40.115] [bbotk]  runtime_learners                                uhash
INFO  [18:19:40.115] [bbotk]           122.063 2881b178-f78f-4292-b7d9-7c10358f4240
INFO  [18:19:42.684] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 55 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:19:49.237] [bbotk] Evaluating 1 configuration(s)
INFO  [18:19:49.430] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:19:49.682] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:20:20.154] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:20:52.954] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:21:26.674] [mlr3] Finished benchmark
INFO  [18:21:26.981] [bbotk] Result of batch 25:
INFO  [18:21:27.012] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:21:27.012] [bbotk]               1.534789                         0.5508259
INFO  [18:21:27.012] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:21:27.012] [bbotk]                         0.7498523            -2.77334              -1.933182
INFO  [18:21:27.012] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:21:27.012] [bbotk]                         13                    1439                 0.7370611
INFO  [18:21:27.012] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:21:27.012] [bbotk]  0.02315289 <list[8]>              FALSE     0.02802649        0      0
INFO  [18:21:27.012] [bbotk]  runtime_learners                                uhash
INFO  [18:21:27.012] [bbotk]            96.403 a3101c54-0fa0-4339-b25e-5b6c407e33b6
INFO  [18:21:28.329] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 56 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:21:34.763] [bbotk] Evaluating 1 configuration(s)
INFO  [18:21:35.117] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:21:35.486] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:22:24.509] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:23:10.670] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:24:15.259] [mlr3] Finished benchmark
INFO  [18:24:15.406] [bbotk] Result of batch 26:
INFO  [18:24:15.458] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:24:15.458] [bbotk]              -1.110956                         0.7006144
INFO  [18:24:15.458] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:24:15.458] [bbotk]                         0.5098023           -5.834101               1.134407
INFO  [18:24:15.458] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:24:15.458] [bbotk]                         10                    4558                 0.8447695
INFO  [18:24:15.458] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:24:15.458] [bbotk]  0.01655447 <list[8]>              FALSE     0.02310054        0      0
INFO  [18:24:15.458] [bbotk]  runtime_learners                                uhash
INFO  [18:24:15.458] [bbotk]            158.67 2c359d12-516d-4af9-a1af-2cd69366e77b
INFO  [18:24:16.680] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 57 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:24:24.422] [bbotk] Evaluating 1 configuration(s)
INFO  [18:24:24.720] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:24:24.926] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:24:53.750] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:25:30.483] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:26:03.881] [mlr3] Finished benchmark
INFO  [18:26:03.971] [bbotk] Result of batch 27:
INFO  [18:26:04.046] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:26:04.046] [bbotk]              0.7627055                         0.1214946
INFO  [18:26:04.046] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:26:04.046] [bbotk]                         0.4705967           -1.830835              0.7844592
INFO  [18:26:04.046] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:26:04.046] [bbotk]                         17                    2699                 0.5807954
INFO  [18:26:04.046] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:26:04.046] [bbotk]  0.01573861 <list[8]>              FALSE      0.0230858        0      0
INFO  [18:26:04.046] [bbotk]  runtime_learners                                uhash
INFO  [18:26:04.046] [bbotk]            97.797 fa08c3f0-5bad-412f-bb1d-5a3d0e738be7
INFO  [18:26:05.612] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 58 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:26:13.271] [bbotk] Evaluating 1 configuration(s)
INFO  [18:26:13.340] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:26:13.741] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:26:32.246] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:26:54.226] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:27:12.887] [mlr3] Finished benchmark
INFO  [18:27:13.012] [bbotk] Result of batch 28:
INFO  [18:27:13.173] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:27:13.173] [bbotk]              -2.935547                          0.695468
INFO  [18:27:13.173] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:27:13.173] [bbotk]                         0.8502488           -1.161166               4.401617
INFO  [18:27:13.173] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:27:13.173] [bbotk]                         16                    3386                 0.3596744
INFO  [18:27:13.173] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:27:13.173] [bbotk]  0.0181318 <list[8]>              FALSE     0.02030225        0      0
INFO  [18:27:13.173] [bbotk]  runtime_learners                                uhash
INFO  [18:27:13.173] [bbotk]            57.613 4e9126ca-c0a8-42fb-a9f8-eb37d90e0af0
INFO  [18:27:14.963] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 59 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:27:18.440] [bbotk] Evaluating 1 configuration(s)
INFO  [18:27:18.687] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:27:18.733] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:27:34.550] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:28:05.937] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:28:46.640] [mlr3] Finished benchmark
INFO  [18:28:46.768] [bbotk] Result of batch 29:
INFO  [18:28:46.812] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:28:46.812] [bbotk]             -0.7313361                         0.3407326
INFO  [18:28:46.812] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:28:46.812] [bbotk]                          0.994845           -4.328005                6.02388
INFO  [18:28:46.812] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:28:46.812] [bbotk]                         17                    2073                 0.5188121
INFO  [18:28:46.812] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:28:46.812] [bbotk]  0.01941959 <list[8]>              FALSE     0.03790437        0      0
INFO  [18:28:46.812] [bbotk]  runtime_learners                                uhash
INFO  [18:28:46.812] [bbotk]            87.573 a4af6274-f53f-460e-bc6b-ff44ae1e83b6
INFO  [18:28:48.760] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 60 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:28:56.472] [bbotk] Evaluating 1 configuration(s)
INFO  [18:28:57.296] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:28:58.385] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:29:20.855] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:29:51.455] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:30:23.705] [mlr3] Finished benchmark
INFO  [18:30:23.814] [bbotk] Result of batch 30:
INFO  [18:30:23.958] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:30:23.958] [bbotk]              -2.401386                         0.8674738
INFO  [18:30:23.958] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:30:23.958] [bbotk]                         0.1808633            -2.45496               -4.21021
INFO  [18:30:23.958] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:30:23.958] [bbotk]                         17                    1163                 0.3728265
INFO  [18:30:23.958] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:30:23.958] [bbotk]  0.01515867 <list[8]>              FALSE     0.02161849        0      0
INFO  [18:30:23.958] [bbotk]  runtime_learners                                uhash
INFO  [18:30:23.958] [bbotk]            84.094 2268a439-b00d-4b14-8341-99afa4247dcb
WARN  [18:30:25.347] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:30:25.367] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 61 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:30:39.040] [bbotk] Evaluating 1 configuration(s)
INFO  [18:30:39.129] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:30:39.250] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:31:19.318] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:32:00.730] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:32:47.496] [mlr3] Finished benchmark
INFO  [18:32:47.822] [bbotk] Result of batch 31:
INFO  [18:32:47.884] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:32:47.884] [bbotk]             -0.8679195                         0.4636345
INFO  [18:32:47.884] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:32:47.884] [bbotk]                         0.6266846           -2.841633               2.824492
INFO  [18:32:47.884] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:32:47.884] [bbotk]                         12                    4621                 0.8895069
INFO  [18:32:47.884] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:32:47.884] [bbotk]  0.01507507 <list[8]>              FALSE     0.02110935        0      0
INFO  [18:32:47.884] [bbotk]  runtime_learners                                uhash
INFO  [18:32:47.884] [bbotk]           127.529 735534ca-f763-42b1-8020-df3225d97d62
INFO  [18:32:49.631] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 62 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:32:55.937] [bbotk] Evaluating 1 configuration(s)
INFO  [18:32:56.184] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:32:56.288] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:33:05.348] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:33:34.251] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:33:55.530] [mlr3] Finished benchmark
INFO  [18:33:55.692] [bbotk] Result of batch 32:
INFO  [18:33:55.778] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:33:55.778] [bbotk]               1.382623                         0.4013088
INFO  [18:33:55.778] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:33:55.778] [bbotk]                          0.655627          -0.2838466             -0.1580427
INFO  [18:33:55.778] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:33:55.778] [bbotk]                         20                     700                  0.701613
INFO  [18:33:55.778] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:33:55.778] [bbotk]  0.02355487 <list[8]>              FALSE      0.0293244        0      0
INFO  [18:33:55.778] [bbotk]  runtime_learners                                uhash
INFO  [18:33:55.778] [bbotk]            58.875 816c77d7-373a-43e7-b5ce-c41ee19d29a7
INFO  [18:33:57.556] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 63 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:34:03.554] [bbotk] Evaluating 1 configuration(s)
INFO  [18:34:03.686] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:34:03.910] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:34:37.357] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:35:10.521] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:35:53.343] [mlr3] Finished benchmark
INFO  [18:35:54.455] [bbotk] Result of batch 33:
INFO  [18:35:54.612] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:35:54.612] [bbotk]              -1.842098                         0.3373939
INFO  [18:35:54.612] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:35:54.612] [bbotk]                         0.4213288           -2.382934                2.46558
INFO  [18:35:54.612] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:35:54.612] [bbotk]                         20                    2477                 0.2910178
INFO  [18:35:54.612] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:35:54.612] [bbotk]  0.01491922 <list[8]>              FALSE     0.02185788        0      0
INFO  [18:35:54.612] [bbotk]  runtime_learners                                uhash
INFO  [18:35:54.612] [bbotk]           107.319 a83ecbe4-5b2e-40e1-abab-2003c2f394a5
WARN  [18:35:57.387] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:35:57.467] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 64 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:36:16.140] [bbotk] Evaluating 1 configuration(s)
INFO  [18:36:16.434] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:36:16.503] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:36:32.415] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:36:53.267] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:37:27.747] [mlr3] Finished benchmark
INFO  [18:37:28.650] [bbotk] Result of batch 34:
INFO  [18:37:28.983] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:37:28.983] [bbotk]              -6.289741                          0.794742
INFO  [18:37:28.983] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:37:28.983] [bbotk]                         0.3699476           -3.872005              -2.918423
INFO  [18:37:28.983] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:37:28.983] [bbotk]                         17                    2594                 0.6565902
INFO  [18:37:28.983] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:37:28.983] [bbotk]  0.01454678 <list[8]>              FALSE     0.01859742        0      0
INFO  [18:37:28.983] [bbotk]  runtime_learners                                uhash
INFO  [18:37:28.983] [bbotk]            70.901 e8051cc9-897a-4ba7-b326-3a29d0efddce
INFO  [18:37:31.806] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 65 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:37:39.178] [bbotk] Evaluating 1 configuration(s)
INFO  [18:37:39.413] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:37:39.547] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:39:06.441] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:40:12.276] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:41:07.749] [mlr3] Finished benchmark
INFO  [18:41:07.854] [bbotk] Result of batch 35:
INFO  [18:41:07.923] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:41:07.923] [bbotk]              -5.672785                         0.5121356
INFO  [18:41:07.923] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:41:07.923] [bbotk]                         0.5406727           -6.244744              -0.448215
INFO  [18:41:07.923] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:41:07.923] [bbotk]                         13                    3779                 0.5729382
INFO  [18:41:07.923] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:41:07.923] [bbotk]  0.01577046 <list[8]>              FALSE     0.02346604        0      0
INFO  [18:41:07.923] [bbotk]  runtime_learners                                uhash
INFO  [18:41:07.923] [bbotk]           207.857 5d7c6341-26f2-4e08-9145-3635196e0993
INFO  [18:41:09.991] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 66 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:41:20.410] [bbotk] Evaluating 1 configuration(s)
INFO  [18:41:20.647] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:41:20.795] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:41:47.031] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:42:09.095] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:42:31.635] [mlr3] Finished benchmark
INFO  [18:42:31.920] [bbotk] Result of batch 36:
INFO  [18:42:32.291] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:42:32.291] [bbotk]              0.8293187                         0.7990141
INFO  [18:42:32.291] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:42:32.291] [bbotk]                          0.924906           -1.075558              -2.765688
INFO  [18:42:32.291] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:42:32.291] [bbotk]                          9                     513                 0.8446454
INFO  [18:42:32.291] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:42:32.291] [bbotk]  0.02931805 <list[8]>              FALSE     0.02538487        0      0
INFO  [18:42:32.291] [bbotk]  runtime_learners                                uhash
INFO  [18:42:32.291] [bbotk]            70.165 3114ab96-7dc6-45e8-9163-7b41e38efe61
WARN  [18:42:38.269] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:42:38.429] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 67 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:42:44.168] [bbotk] Evaluating 1 configuration(s)
INFO  [18:42:44.393] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:42:44.462] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:43:25.187] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:44:30.257] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:45:33.583] [mlr3] Finished benchmark
INFO  [18:45:33.729] [bbotk] Result of batch 37:
INFO  [18:45:33.793] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:45:33.793] [bbotk]              -2.981528                         0.9604349
INFO  [18:45:33.793] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:45:33.793] [bbotk]                         0.6281477           -3.925462               -2.05363
INFO  [18:45:33.793] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:45:33.793] [bbotk]                         15                    4284                 0.6889236
INFO  [18:45:33.793] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:45:33.793] [bbotk]  0.01518633 <list[8]>              FALSE     0.01938429        0      0
INFO  [18:45:33.793] [bbotk]  runtime_learners                                uhash
INFO  [18:45:33.793] [bbotk]           168.582 147232ea-555c-47d5-93f5-aad5d402adc3
INFO  [18:45:38.483] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 68 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:45:44.254] [bbotk] Evaluating 1 configuration(s)
INFO  [18:45:44.392] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:45:44.582] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:46:36.890] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:47:21.782] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:47:58.551] [mlr3] Finished benchmark
INFO  [18:47:58.777] [bbotk] Result of batch 38:
INFO  [18:47:58.825] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:47:58.825] [bbotk]              -2.270781                         0.5997164
INFO  [18:47:58.825] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:47:58.825] [bbotk]                         0.3453932           -1.836586              -1.783058
INFO  [18:47:58.825] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:47:58.825] [bbotk]                         15                    4827                  0.188312
INFO  [18:47:58.825] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:47:58.825] [bbotk]  0.01586041 <list[8]>              FALSE     0.02097717        0      0
INFO  [18:47:58.825] [bbotk]  runtime_learners                                uhash
INFO  [18:47:58.825] [bbotk]           133.432 f7354122-08ec-402a-9aac-2d99cc2dbc78
INFO  [18:48:01.273] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 69 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:48:09.155] [bbotk] Evaluating 1 configuration(s)
INFO  [18:48:09.230] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:48:09.303] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:48:31.557] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:48:49.752] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:49:14.167] [mlr3] Finished benchmark
INFO  [18:49:14.626] [bbotk] Result of batch 39:
INFO  [18:49:14.652] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:49:14.652] [bbotk]             -0.3178996                         0.6183965
INFO  [18:49:14.652] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:49:14.652] [bbotk]                         0.3529129            -2.27117              -2.810018
INFO  [18:49:14.652] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:49:14.652] [bbotk]                         18                     883                 0.8003451
INFO  [18:49:14.652] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:49:14.652] [bbotk]  0.01946797 <list[8]>              FALSE      0.0221294        0      0
INFO  [18:49:14.652] [bbotk]  runtime_learners                                uhash
INFO  [18:49:14.652] [bbotk]            64.627 aabb8f21-4d06-4265-9724-9c015371faa8
INFO  [18:49:16.273] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 70 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:49:22.088] [bbotk] Evaluating 1 configuration(s)
INFO  [18:49:22.262] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:49:22.309] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:49:51.401] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:50:20.074] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:50:53.240] [mlr3] Finished benchmark
INFO  [18:50:53.477] [bbotk] Result of batch 40:
INFO  [18:50:53.544] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:50:53.544] [bbotk]              -5.190307                         0.8666932
INFO  [18:50:53.544] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:50:53.544] [bbotk]                         0.5398781          -0.6831424              -5.092044
INFO  [18:50:53.544] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:50:53.544] [bbotk]                         11                    2789                 0.3128793
INFO  [18:50:53.544] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:50:53.544] [bbotk]  0.01434754 <list[8]>              FALSE     0.01883529        0      0
INFO  [18:50:53.544] [bbotk]  runtime_learners                                uhash
INFO  [18:50:53.544] [bbotk]            90.673 8b36e777-3577-498c-a7c2-ceeab1d479ed
WARN  [18:50:58.029] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:50:58.074] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 71 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:51:12.280] [bbotk] Evaluating 1 configuration(s)
INFO  [18:51:12.540] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:51:12.795] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:51:58.481] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:52:31.111] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:53:04.826] [mlr3] Finished benchmark
INFO  [18:53:04.902] [bbotk] Result of batch 41:
INFO  [18:53:04.910] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:53:04.910] [bbotk]                2.27648                         0.1396804
INFO  [18:53:04.910] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:53:04.910] [bbotk]                         0.6969953          -0.3567582              0.3792906
INFO  [18:53:04.910] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:53:04.910] [bbotk]                         11                    2353                  0.562271
INFO  [18:53:04.910] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:53:04.910] [bbotk]  0.01587853 <list[8]>              FALSE     0.03661471        0      0
INFO  [18:53:04.910] [bbotk]  runtime_learners                                uhash
INFO  [18:53:04.910] [bbotk]           111.659 ebc153f7-2d2b-47d3-be4d-e76743e4a6d3
INFO  [18:53:09.961] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 72 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:53:23.944] [bbotk] Evaluating 1 configuration(s)
INFO  [18:53:24.215] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:53:24.425] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:54:06.498] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:54:34.252] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:55:01.002] [mlr3] Finished benchmark
INFO  [18:55:01.779] [bbotk] Result of batch 42:
INFO  [18:55:01.789] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:55:01.789] [bbotk]             -0.4743615                         0.6778004
INFO  [18:55:01.789] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:55:01.789] [bbotk]                          0.931145           -2.384244               3.428018
INFO  [18:55:01.789] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:55:01.789] [bbotk]                          5                    1741                 0.2306722
INFO  [18:55:01.789] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:55:01.789] [bbotk]  0.01478917 <list[8]>              FALSE     0.02247061        0      0
INFO  [18:55:01.789] [bbotk]  runtime_learners                                uhash
INFO  [18:55:01.789] [bbotk]            96.088 69aa04a4-0d1e-4897-9c77-9ff7ea22dad6
INFO  [18:55:04.148] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 73 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:55:11.949] [bbotk] Evaluating 1 configuration(s)
INFO  [18:55:12.009] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:55:12.054] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:55:53.589] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:56:35.760] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:57:33.694] [mlr3] Finished benchmark
INFO  [18:57:34.044] [bbotk] Result of batch 43:
INFO  [18:57:34.163] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:57:34.163] [bbotk]               -5.62368                         0.7305984
INFO  [18:57:34.163] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:57:34.163] [bbotk]                          0.701063           -1.550908               2.984893
INFO  [18:57:34.163] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:57:34.163] [bbotk]                         19                    2835                  0.717262
INFO  [18:57:34.163] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:57:34.163] [bbotk]  0.01393319 <list[8]>              FALSE     0.01963221        0      0
INFO  [18:57:34.163] [bbotk]  runtime_learners                                uhash
INFO  [18:57:34.163] [bbotk]           141.427 d789a7f2-74f3-4ab4-8b6f-dedf58aa62c1
INFO  [18:57:42.870] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 74 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:57:53.006] [bbotk] Evaluating 1 configuration(s)
INFO  [18:57:53.340] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:57:53.569] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:58:25.119] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:59:02.126] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:59:34.890] [mlr3] Finished benchmark
INFO  [18:59:35.042] [bbotk] Result of batch 44:
INFO  [18:59:35.053] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:59:35.053] [bbotk]              -5.910487                         0.6631188
INFO  [18:59:35.053] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:59:35.053] [bbotk]                         0.5721367          -0.7538372              0.1642621
INFO  [18:59:35.053] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:59:35.053] [bbotk]                          8                    1625                 0.1617197
INFO  [18:59:35.053] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:59:35.053] [bbotk]  0.01373393 <list[8]>              FALSE     0.02074674        0      0
INFO  [18:59:35.053] [bbotk]  runtime_learners                                uhash
INFO  [18:59:35.053] [bbotk]           100.875 17b6f561-32f8-4472-86a8-a6ddc492a494
WARN  [18:59:46.798] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:59:46.803] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 75 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:00:00.170] [bbotk] Evaluating 1 configuration(s)
INFO  [19:00:00.320] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:00:00.371] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:01:05.897] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:02:21.150] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:03:25.651] [mlr3] Finished benchmark
INFO  [19:03:26.123] [bbotk] Result of batch 45:
INFO  [19:03:26.193] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:03:26.193] [bbotk]              -5.405083                         0.9159699
INFO  [19:03:26.193] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:03:26.193] [bbotk]                         0.7598084           -6.777946              -4.917531
INFO  [19:03:26.193] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:03:26.193] [bbotk]                         18                    2942                 0.1487442
INFO  [19:03:26.193] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:03:26.193] [bbotk]  0.01191767 <list[8]>              FALSE     0.02938496        0      0
INFO  [19:03:26.193] [bbotk]  runtime_learners                                uhash
INFO  [19:03:26.193] [bbotk]           204.687 a5221d62-5892-485e-8b6a-423c7163629f
WARN  [19:03:30.977] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:03:31.222] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 76 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:03:44.396] [bbotk] Evaluating 1 configuration(s)
INFO  [19:03:44.634] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:03:44.809] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:05:00.317] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:05:47.054] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:06:57.227] [mlr3] Finished benchmark
INFO  [19:06:57.929] [bbotk] Result of batch 46:
INFO  [19:06:58.005] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:06:58.005] [bbotk]              -2.221225                         0.6316914
INFO  [19:06:58.005] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:06:58.005] [bbotk]                          0.671512           -6.251146              -2.809225
INFO  [19:06:58.005] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:06:58.005] [bbotk]                         10                    3088                 0.2234215
INFO  [19:06:58.005] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:06:58.005] [bbotk]  0.01384941 <list[8]>              FALSE     0.02363284        0      0
INFO  [19:06:58.005] [bbotk]  runtime_learners                                uhash
INFO  [19:06:58.005] [bbotk]           192.072 6a41e1da-b96e-42f4-8481-ddef5c898c43
INFO  [19:06:59.207] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 77 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:07:16.362] [bbotk] Evaluating 1 configuration(s)
INFO  [19:07:16.503] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:07:16.518] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:08:28.395] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:09:30.570] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:10:24.644] [mlr3] Finished benchmark
INFO  [19:10:24.728] [bbotk] Result of batch 47:
INFO  [19:10:24.740] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:10:24.740] [bbotk]             -0.9025164                         0.5619646
INFO  [19:10:24.740] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:10:24.740] [bbotk]                         0.5105903          -0.4123552               2.969436
INFO  [19:10:24.740] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:10:24.740] [bbotk]                          7                    3601                   0.44851
INFO  [19:10:24.740] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:10:24.740] [bbotk]  0.01313323 <list[8]>              FALSE     0.02086456        0      0
INFO  [19:10:24.740] [bbotk]  runtime_learners                                uhash
INFO  [19:10:24.740] [bbotk]           187.909 89c22a3c-b5ca-4572-88c1-4690e6d1b39f
WARN  [19:10:27.626] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:10:27.631] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 78 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:10:36.207] [bbotk] Evaluating 1 configuration(s)
INFO  [19:10:36.772] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:10:36.847] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:11:02.441] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:11:53.746] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:13:07.951] [mlr3] Finished benchmark
INFO  [19:13:09.020] [bbotk] Result of batch 48:
INFO  [19:13:09.027] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:13:09.027] [bbotk]              -5.603402                         0.6079508
INFO  [19:13:09.027] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:13:09.027] [bbotk]                         0.3494655           -1.344298              -2.464667
INFO  [19:13:09.027] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:13:09.027] [bbotk]                         12                    4073                 0.2392007
INFO  [19:13:09.027] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:13:09.027] [bbotk]  0.01171319 <list[8]>              FALSE     0.02011393        0      0
INFO  [19:13:09.027] [bbotk]  runtime_learners                                uhash
INFO  [19:13:09.027] [bbotk]           150.645 41bfaf0f-2879-4f57-ad92-bb5f76d52b43
INFO  [19:13:15.607] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 79 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:13:30.963] [bbotk] Evaluating 1 configuration(s)
INFO  [19:13:31.038] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:13:31.081] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:13:59.812] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:14:35.350] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:14:53.172] [mlr3] Finished benchmark
INFO  [19:14:53.269] [bbotk] Result of batch 49:
INFO  [19:14:53.276] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:14:53.276] [bbotk]              0.3371836                         0.2897556
INFO  [19:14:53.276] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:14:53.276] [bbotk]                         0.8418393           -0.212705               2.114452
INFO  [19:14:53.276] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:14:53.276] [bbotk]                          5                     567                 0.4595767
INFO  [19:14:53.276] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:14:53.276] [bbotk]  0.01171599 <list[8]>              FALSE     0.02217352        0      0
INFO  [19:14:53.276] [bbotk]  runtime_learners                                uhash
INFO  [19:14:53.276] [bbotk]            81.788 2a1f6e18-ca5e-48f3-bd46-4163031c6a69
WARN  [19:15:13.616] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:15:13.622] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 80 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:15:34.988] [bbotk] Evaluating 1 configuration(s)
INFO  [19:15:35.932] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:15:36.172] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:16:41.132] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:17:50.631] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:18:53.031] [mlr3] Finished benchmark
INFO  [19:18:54.333] [bbotk] Result of batch 50:
INFO  [19:18:54.433] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:18:54.433] [bbotk]             -0.1114743                         0.3484782
INFO  [19:18:54.433] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:18:54.433] [bbotk]                         0.9198168          -0.3574455               4.895886
INFO  [19:18:54.433] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:18:54.433] [bbotk]                         11                    3259                 0.3685008
INFO  [19:18:54.433] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:18:54.433] [bbotk]  0.01249266 <list[8]>              FALSE     0.02100908        0      0
INFO  [19:18:54.433] [bbotk]  runtime_learners                                uhash
INFO  [19:18:54.433] [bbotk]           196.204 e02f717f-b932-4aa1-b86b-86dcd0782bd3
INFO  [19:19:01.617] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 81 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:19:13.844] [bbotk] Evaluating 1 configuration(s)
INFO  [19:19:13.976] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:19:14.162] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:20:32.710] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:22:05.590] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:23:20.928] [mlr3] Finished benchmark
INFO  [19:23:22.245] [bbotk] Result of batch 51:
INFO  [19:23:22.274] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:23:22.274] [bbotk]              0.2060698                         0.5527284
INFO  [19:23:22.274] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:23:22.274] [bbotk]                         0.8018287            -2.94983               1.258129
INFO  [19:23:22.274] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:23:22.274] [bbotk]                          3                    4798                 0.7243771
INFO  [19:23:22.274] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:23:22.274] [bbotk]  0.01314478 <list[8]>              FALSE     0.02174274        0      0
INFO  [19:23:22.274] [bbotk]  runtime_learners                                uhash
INFO  [19:23:22.274] [bbotk]           245.881 c1aa8406-57dd-41ce-9e36-5a7d1a54c51f
INFO  [19:23:27.152] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 82 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:23:39.927] [bbotk] Evaluating 1 configuration(s)
INFO  [19:23:39.990] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:23:40.004] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:24:15.454] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:24:54.974] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:25:34.029] [mlr3] Finished benchmark
INFO  [19:25:34.453] [bbotk] Result of batch 52:
INFO  [19:25:34.469] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:25:34.469] [bbotk]              -2.094517                          0.552546
INFO  [19:25:34.469] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:25:34.469] [bbotk]                          0.848148           -0.145445               3.138235
INFO  [19:25:34.469] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:25:34.469] [bbotk]                         18                    1322                 0.7701062
INFO  [19:25:34.469] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:25:34.469] [bbotk]  0.01373037 <list[8]>              FALSE     0.01941641        0      0
INFO  [19:25:34.469] [bbotk]  runtime_learners                                uhash
INFO  [19:25:34.469] [bbotk]           113.692 f98c80ac-6138-4235-ab55-68c571c52d90
INFO  [19:25:42.954] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 83 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:25:53.114] [bbotk] Evaluating 1 configuration(s)
INFO  [19:25:53.383] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:25:53.486] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:26:48.595] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:28:03.185] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:29:15.686] [mlr3] Finished benchmark
INFO  [19:29:15.804] [bbotk] Result of batch 53:
INFO  [19:29:15.860] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:29:15.860] [bbotk]              -3.990498                         0.8344682
INFO  [19:29:15.860] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:29:15.860] [bbotk]                         0.1582979           -1.103897               3.108447
INFO  [19:29:15.860] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:29:15.860] [bbotk]                         18                    4818                 0.4211092
INFO  [19:29:15.860] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:29:15.860] [bbotk]  0.01112164 <list[8]>              FALSE     0.02111068        0      0
INFO  [19:29:15.860] [bbotk]  runtime_learners                                uhash
INFO  [19:29:15.860] [bbotk]           201.758 acc81b2d-6042-45bb-8151-8c87e352518a
WARN  [19:29:24.253] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:29:24.282] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 84 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:29:36.653] [bbotk] Evaluating 1 configuration(s)
INFO  [19:29:36.915] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:29:37.033] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:30:21.851] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:31:06.411] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:31:57.153] [mlr3] Finished benchmark
INFO  [19:31:58.203] [bbotk] Result of batch 54:
INFO  [19:31:58.424] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:31:58.424] [bbotk]              -6.353699                         0.1588498
INFO  [19:31:58.424] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:31:58.424] [bbotk]                         0.5500023          -0.7272176               3.605038
INFO  [19:31:58.424] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:31:58.424] [bbotk]                         19                    2984                 0.6180337
INFO  [19:31:58.424] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:31:58.424] [bbotk]  0.01010793 <list[8]>              FALSE     0.02081944        0      0
INFO  [19:31:58.424] [bbotk]  runtime_learners                                uhash
INFO  [19:31:58.424] [bbotk]           139.498 e1d6e9fd-306d-4361-9f9e-7de52dcfc659
WARN  [19:32:03.962] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:32:03.972] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 85 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:32:15.635] [bbotk] Evaluating 1 configuration(s)
INFO  [19:32:15.927] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:32:16.170] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:33:22.806] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:34:21.127] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:35:55.706] [mlr3] Finished benchmark
INFO  [19:35:55.843] [bbotk] Result of batch 55:
INFO  [19:35:55.887] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:35:55.887] [bbotk]              -5.281882                         0.6095999
INFO  [19:35:55.887] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:35:55.887] [bbotk]                         0.3641167            -5.32974              -5.355903
INFO  [19:35:55.887] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:35:55.887] [bbotk]                         12                    4218                 0.8213986
INFO  [19:35:55.887] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:35:55.887] [bbotk]  0.009937719 <list[8]>              FALSE     0.02102651        0      0
INFO  [19:35:55.887] [bbotk]  runtime_learners                                uhash
INFO  [19:35:55.887] [bbotk]            218.88 72cd1aa8-6909-42f4-83d3-baddea418bc9
WARN  [19:36:03.092] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:36:03.101] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 86 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:36:16.223] [bbotk] Evaluating 1 configuration(s)
INFO  [19:36:16.386] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:36:16.693] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:37:34.325] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:38:58.050] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [19:39:57.897] [mlr3] Finished benchmark
INFO  [19:39:58.916] [bbotk] Result of batch 56:
INFO  [19:39:59.042] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:39:59.042] [bbotk]              -3.977858                         0.8665317
INFO  [19:39:59.042] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:39:59.042] [bbotk]                         0.3047262         -0.05994154               -3.22385
INFO  [19:39:59.042] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:39:59.042] [bbotk]                         14                    4822                 0.4666823
INFO  [19:39:59.042] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:39:59.042] [bbotk]  0.009741261 <list[8]>              FALSE     0.02595458        0      0
INFO  [19:39:59.042] [bbotk]  runtime_learners                                uhash
INFO  [19:39:59.042] [bbotk]           220.825 1e93d99c-f794-4078-bc31-67c8380e14ec
INFO  [19:40:06.519] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 87 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:40:28.007] [bbotk] Evaluating 1 configuration(s)
INFO  [19:40:28.044] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:40:28.064] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:41:48.868] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:43:21.881] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:44:23.338] [mlr3] Finished benchmark
INFO  [19:44:23.928] [bbotk] Result of batch 57:
INFO  [19:44:24.035] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:44:24.035] [bbotk]              -2.249741                         0.9234981
INFO  [19:44:24.035] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:44:24.035] [bbotk]                         0.6036227            -3.97764                 3.2951
INFO  [19:44:24.035] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:44:24.035] [bbotk]                         13                    3161                 0.4146954
INFO  [19:44:24.035] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:44:24.035] [bbotk]  0.008907861 <list[8]>              FALSE       0.022821        0      0
INFO  [19:44:24.035] [bbotk]  runtime_learners                                uhash
INFO  [19:44:24.035] [bbotk]            234.39 1aeca9ae-35df-433b-8023-fa6aac88e166
INFO  [19:44:29.413] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 88 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:44:44.770] [bbotk] Evaluating 1 configuration(s)
INFO  [19:44:44.865] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:44:45.006] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:45:51.054] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:47:14.752] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:48:59.701] [mlr3] Finished benchmark
INFO  [19:49:00.224] [bbotk] Result of batch 58:
INFO  [19:49:00.235] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:49:00.235] [bbotk]              -3.034286                         0.6841164
INFO  [19:49:00.235] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:49:00.235] [bbotk]                         0.9434311          -0.7652005              -1.226404
INFO  [19:49:00.235] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:49:00.235] [bbotk]                         18                    3633                 0.3870242
INFO  [19:49:00.235] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:49:00.235] [bbotk]  0.009695112 <list[8]>              FALSE     0.02225551        0      0
INFO  [19:49:00.235] [bbotk]  runtime_learners                                uhash
INFO  [19:49:00.235] [bbotk]           254.521 255f7619-fed4-49f5-aa11-d2106b18d5a6
INFO  [19:49:02.826] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 89 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:49:21.891] [bbotk] Evaluating 1 configuration(s)
INFO  [19:49:22.655] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:49:22.670] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:49:55.032] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:50:33.655] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:51:04.034] [mlr3] Finished benchmark
INFO  [19:51:04.198] [bbotk] Result of batch 59:
INFO  [19:51:04.216] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:51:04.216] [bbotk]             -0.6057427                         0.1045319
INFO  [19:51:04.216] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:51:04.216] [bbotk]                         0.6054077           -1.124201               4.043264
INFO  [19:51:04.216] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:51:04.216] [bbotk]                         20                     478                  0.412361
INFO  [19:51:04.216] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:51:04.216] [bbotk]  0.01321975 <list[8]>              FALSE     0.02731371        0      0
INFO  [19:51:04.216] [bbotk]  runtime_learners                                uhash
INFO  [19:51:04.216] [bbotk]           100.581 a39c9763-9aea-4fdb-9e1a-5481dfcec26a
INFO  [19:51:14.205] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 90 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:51:23.206] [bbotk] Evaluating 1 configuration(s)
INFO  [19:51:23.256] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:51:23.285] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:51:46.174] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:52:04.600] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:52:21.552] [mlr3] Finished benchmark
INFO  [19:52:21.629] [bbotk] Result of batch 60:
INFO  [19:52:21.637] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:52:21.637] [bbotk]               1.824392                         0.9759573
INFO  [19:52:21.637] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:52:21.637] [bbotk]                         0.5436198           -1.251879              -3.078122
INFO  [19:52:21.637] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:52:21.637] [bbotk]                         19                       5                 0.1357714
INFO  [19:52:21.637] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:52:21.637] [bbotk]  0.009354913 <list[8]>              FALSE     0.06835075        0      0
INFO  [19:52:21.637] [bbotk]  runtime_learners                                uhash
INFO  [19:52:21.637] [bbotk]            57.917 dabb15f1-7636-4d20-acc5-f06d5690a1ee
WARN  [19:52:28.225] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:52:28.242] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 91 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:52:36.774] [bbotk] Evaluating 1 configuration(s)
INFO  [19:52:36.835] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:52:36.850] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:53:07.692] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:53:56.517] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:55:01.418] [mlr3] Finished benchmark
INFO  [19:55:01.671] [bbotk] Result of batch 61:
INFO  [19:55:01.680] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:55:01.680] [bbotk]             -0.2912872                          0.407182
INFO  [19:55:01.680] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:55:01.680] [bbotk]                         0.7502196           -4.225767               -2.46442
INFO  [19:55:01.680] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:55:01.680] [bbotk]                          1                    1439                 0.9044003
INFO  [19:55:01.680] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:55:01.680] [bbotk]  0.01085687 <list[8]>              FALSE     0.02101935        0      0
INFO  [19:55:01.680] [bbotk]  runtime_learners                                uhash
INFO  [19:55:01.680] [bbotk]           143.446 fe10048a-abc6-477a-b701-69dc3d8e85b8
INFO  [19:55:24.589] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 92 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:55:49.789] [bbotk] Evaluating 1 configuration(s)
INFO  [19:55:50.318] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:55:50.478] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:56:52.658] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:57:35.392] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:58:15.679] [mlr3] Finished benchmark
INFO  [19:58:15.755] [bbotk] Result of batch 62:
INFO  [19:58:16.273] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:58:16.273] [bbotk]               -6.46031                         0.5238518
INFO  [19:58:16.273] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:58:16.273] [bbotk]                         0.1666631           -2.977413                4.87508
INFO  [19:58:16.273] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:58:16.273] [bbotk]                         16                    3299                  0.196875
INFO  [19:58:16.273] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:58:16.273] [bbotk]  0.009652667 <list[8]>              FALSE     0.03423353        0      0
INFO  [19:58:16.273] [bbotk]  runtime_learners                                uhash
INFO  [19:58:16.273] [bbotk]           144.821 a91eb0ed-216f-4623-ad4b-ac3de3895fad
INFO  [19:58:32.104] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 93 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:58:53.371] [bbotk] Evaluating 1 configuration(s)
INFO  [19:58:53.909] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:58:53.938] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:59:24.118] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:00:01.814] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:00:43.197] [mlr3] Finished benchmark
INFO  [20:00:44.424] [bbotk] Result of batch 63:
INFO  [20:00:44.564] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:00:44.564] [bbotk]               -0.17484                         0.2767489
INFO  [20:00:44.564] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:00:44.564] [bbotk]                         0.8224583           -2.391527                4.17324
INFO  [20:00:44.564] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:00:44.564] [bbotk]                         18                     992                 0.3168978
INFO  [20:00:44.564] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:00:44.564] [bbotk]  0.008843065 <list[8]>              FALSE     0.02886945        0      0
INFO  [20:00:44.564] [bbotk]  runtime_learners                                uhash
INFO  [20:00:44.564] [bbotk]           108.587 b388d486-3707-47b7-aed6-384cd1a3d448
INFO  [20:00:48.674] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 94 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:01:10.864] [bbotk] Evaluating 1 configuration(s)
INFO  [20:01:10.944] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:01:11.067] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:02:19.661] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:03:28.637] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:04:58.851] [mlr3] Finished benchmark
INFO  [20:05:00.171] [bbotk] Result of batch 64:
INFO  [20:05:00.411] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:05:00.411] [bbotk]              -5.810358                         0.8774723
INFO  [20:05:00.411] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:05:00.411] [bbotk]                         0.8386872           -3.474522              -4.357716
INFO  [20:05:00.411] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:05:00.411] [bbotk]                          2                    2767                  0.848371
INFO  [20:05:00.411] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:05:00.411] [bbotk]  0.00890674 <list[8]>              FALSE     0.01998441        0      0
INFO  [20:05:00.411] [bbotk]  runtime_learners                                uhash
INFO  [20:05:00.411] [bbotk]           227.572 21362566-1376-4ce2-b0cb-13fb59ed08cc
INFO  [20:05:06.887] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 95 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:05:18.685] [bbotk] Evaluating 1 configuration(s)
INFO  [20:05:19.332] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:05:19.817] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:05:37.727] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:06:08.728] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:06:58.820] [mlr3] Finished benchmark
INFO  [20:07:00.537] [bbotk] Result of batch 65:
INFO  [20:07:01.018] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:07:01.018] [bbotk]              -1.368245                         0.7841532
INFO  [20:07:01.018] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:07:01.018] [bbotk]                         0.3409505           -3.470532              -4.127678
INFO  [20:07:01.018] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:07:01.018] [bbotk]                         16                    2075                 0.5028769
INFO  [20:07:01.018] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:07:01.018] [bbotk]  0.01026252 <list[8]>              FALSE     0.02018942        0      0
INFO  [20:07:01.018] [bbotk]  runtime_learners                                uhash
INFO  [20:07:01.018] [bbotk]            98.797 f91f4190-16db-4f37-84e8-f96c6b2feed5
INFO  [20:07:07.746] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 96 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:07:18.965] [bbotk] Evaluating 1 configuration(s)
INFO  [20:07:19.025] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:07:19.065] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:08:14.178] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:09:04.935] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:10:09.806] [mlr3] Finished benchmark
INFO  [20:10:09.979] [bbotk] Result of batch 66:
INFO  [20:10:10.646] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:10:10.646] [bbotk]           -0.003686106                         0.2077626
INFO  [20:10:10.646] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:10:10.646] [bbotk]                         0.8427226         -0.08673806               3.249648
INFO  [20:10:10.646] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:10:10.646] [bbotk]                         20                    2115                 0.3400373
INFO  [20:10:10.646] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:10:10.646] [bbotk]  0.009285517 <list[8]>              FALSE     0.01932052        0      0
INFO  [20:10:10.646] [bbotk]  runtime_learners                                uhash
INFO  [20:10:10.646] [bbotk]           169.848 613fcff9-24a5-4f64-98ad-aa1359cc4573
WARN  [20:10:20.306] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:10:20.341] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 97 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:10:33.051] [bbotk] Evaluating 1 configuration(s)
INFO  [20:10:33.514] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:10:33.667] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:11:08.515] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:11:47.012] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [20:12:11.578] [mlr3] Finished benchmark
INFO  [20:12:12.194] [bbotk] Result of batch 67:
INFO  [20:12:12.266] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:12:12.266] [bbotk]               2.185466                         0.1221428
INFO  [20:12:12.266] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:12:12.266] [bbotk]                         0.5500031          -0.4440177               3.105153
INFO  [20:12:12.266] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:12:12.266] [bbotk]                         14                     823                 0.2979075
INFO  [20:12:12.266] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:12:12.266] [bbotk]  0.009513121 <list[8]>              FALSE     0.04088955        0      0
INFO  [20:12:12.266] [bbotk]  runtime_learners                                uhash
INFO  [20:12:12.266] [bbotk]            97.674 a602c7f7-b262-4f9d-84d4-384893559a0c
INFO  [20:12:14.430] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 98 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:12:23.423] [bbotk] Evaluating 1 configuration(s)
INFO  [20:12:23.655] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:12:23.793] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:13:19.818] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:15:58.154] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:18:17.410] [mlr3] Finished benchmark
INFO  [20:18:17.557] [bbotk] Result of batch 68:
INFO  [20:18:17.564] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:18:17.564] [bbotk]              0.1597403                          0.496487
INFO  [20:18:17.564] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:18:17.564] [bbotk]                         0.8114305           -3.373127               2.615795
INFO  [20:18:17.564] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:18:17.564] [bbotk]                         16                    4847                 0.6802794
INFO  [20:18:17.564] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:18:17.564] [bbotk]  0.008702751 <list[8]>              FALSE      0.0207646        0      0
INFO  [20:18:17.564] [bbotk]  runtime_learners                                uhash
INFO  [20:18:17.564] [bbotk]           353.408 293cf58b-00fc-4a81-afb7-5966b7217195
INFO  [20:18:23.787] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 99 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:19:15.084] [bbotk] Evaluating 1 configuration(s)
INFO  [20:19:15.602] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:19:15.807] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:20:42.216] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:21:45.020] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:22:56.357] [mlr3] Finished benchmark
INFO  [20:22:56.712] [bbotk] Result of batch 69:
INFO  [20:22:56.722] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:22:56.722] [bbotk]              -2.660908                         0.6717247
INFO  [20:22:56.722] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:22:56.722] [bbotk]                         0.4653376             -1.7648              0.4393474
INFO  [20:22:56.722] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:22:56.722] [bbotk]                         19                    3461                 0.5274999
INFO  [20:22:56.722] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:22:56.722] [bbotk]  0.009204187 <list[8]>              FALSE     0.01849063        0      0
INFO  [20:22:56.722] [bbotk]  runtime_learners                                uhash
INFO  [20:22:56.722] [bbotk]           220.197 b5094e92-579c-4c25-9797-073422b5c4e7
INFO  [20:23:01.343] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 100 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:23:01.436] [bbotk] Finished optimizing after 100 evaluation(s)
INFO  [20:23:01.438] [bbotk] Result:
INFO  [20:23:01.443] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:23:01.443] [bbotk]                  <num>                             <num>
INFO  [20:23:01.443] [bbotk]                -4.4666                          0.666994
INFO  [20:23:01.443] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:23:01.443] [bbotk]                             <num>               <num>                  <num>
INFO  [20:23:01.443] [bbotk]                         0.6263524           -2.148165              -1.290306
INFO  [20:23:01.443] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:23:01.443] [bbotk]                      <int>                   <int>                     <num>
INFO  [20:23:01.443] [bbotk]                         15                    2959                 0.5047618
INFO  [20:23:01.443] [bbotk]  learner_param_vals  x_domain classif.bbrier
INFO  [20:23:01.443] [bbotk]              <list>    <list>          <num>
INFO  [20:23:01.443] [bbotk]          <list[10]> <list[8]>     0.01757312
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()

### [bt]: Job terminated successfully [batchtools job.id=1433]
### [bt]: Calculation finished!
