### [bt]: This is batchtools v0.9.17
### [bt]: Starting calculation of 1 jobs
### [bt]: Setting working directory to '/content/drive/MyDrive/GitHub/Mater_Thesis_Calibration'
ece.R :
Friedmann_tasks.R :
Functions.R :
ici.R :
mse_feature_effects.R :
PipeOpCalibration_TuningExperiment.R :
PipeOpCalibration.R :
PipeOpCalibrationUnion.R :
### [bt]: Memory measurement disabled
### [bt]: Starting job [batchtools job.id=1431]
### [bt]: Generating problem instance for problem '341ee781c7d79b8f' ...
### [bt]: Applying algorithm 'run_learner' on problem '341ee781c7d79b8f' for job 1431 (seed = 1554) ...
INFO  [16:12:17.276] [mlr3] Applying learner 'xgboost TwP platt' on task 'wdbc' (iter 1/10)
INFO  [16:12:18.830] [bbotk] Starting to optimize 8 parameter(s) with '<OptimizerMbo>' and '<TerminatorEvals> [n_evals=100, k=0]'
INFO  [16:12:23.157] [bbotk] Evaluating 32 configuration(s)
INFO  [16:12:23.730] [mlr3] Running benchmark with 96 resampling iterations
INFO  [16:12:23.970] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:14:03.056] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:14:51.295] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:15:36.582] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:18:34.022] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:20:34.451] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:22:54.830] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:23:46.533] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:24:33.956] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:25:25.266] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:25:46.746] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:26:08.953] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:26:37.296] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:27:29.685] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:28:25.410] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:29:33.333] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:30:17.817] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:30:56.227] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:31:23.359] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:32:55.194] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:34:15.153] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:35:12.207] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:36:19.697] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:37:01.125] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:37:41.316] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:38:07.413] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:38:28.999] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:38:59.618] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:40:07.899] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:41:14.984] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:42:15.205] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:43:10.086] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:43:47.725] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:44:25.986] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:45:59.333] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:47:10.672] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:48:29.836] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:49:17.135] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:50:02.432] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:50:59.574] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:51:32.577] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:52:02.159] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:52:35.887] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:53:41.493] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:54:55.377] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:56:12.538] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:56:56.656] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [16:57:27.979] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [16:58:14.439] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [16:59:34.333] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:00:46.511] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:01:36.269] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:02:25.157] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:03:14.344] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:04:17.805] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:04:58.971] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:05:55.229] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:07:00.251] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:08:10.615] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:09:27.449] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:10:55.845] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:11:50.564] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:12:40.201] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:14:18.518] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:14:35.946] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:14:57.769] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:15:22.546] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:15:54.990] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:16:26.277] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:16:51.879] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:17:48.530] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:18:49.819] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:19:46.347] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:20:16.807] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:20:47.312] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:21:21.828] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:22:30.043] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:23:17.886] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:24:01.495] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:25:02.895] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:26:08.962] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:26:56.228] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:27:29.638] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:28:18.921] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:28:50.864] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:29:32.755] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:30:05.142] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:30:32.845] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:30:53.071] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:31:10.878] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:31:27.807] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:32:01.511] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:32:20.997] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:32:59.025] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:33:48.590] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:34:22.567] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:35:15.330] [mlr3] Finished benchmark
INFO  [17:35:18.594] [bbotk] Result of batch 1:
INFO  [17:35:18.844] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:35:18.844] [bbotk]              2.5704292                         0.5161703
INFO  [17:35:18.844] [bbotk]             -4.3373263                         0.9661703
INFO  [17:35:18.844] [bbotk]              6.0243069                         0.7411703
INFO  [17:35:18.844] [bbotk]             -0.8834484                         0.2911703
INFO  [17:35:18.844] [bbotk]             -2.6103872                         0.6286703
INFO  [17:35:18.844] [bbotk]              4.2973681                         0.1786703
INFO  [17:35:18.844] [bbotk]              0.8434904                         0.8536703
INFO  [17:35:18.844] [bbotk]             -6.0642651                         0.4036703
INFO  [17:35:18.844] [bbotk]              1.7069598                         0.2349203
INFO  [17:35:18.844] [bbotk]             -5.2007957                         0.6849203
INFO  [17:35:18.844] [bbotk]             -1.7469178                         0.4599203
INFO  [17:35:18.844] [bbotk]              5.1608375                         0.9099203
INFO  [17:35:18.844] [bbotk]              3.4338986                         0.5724203
INFO  [17:35:18.844] [bbotk]             -3.4738568                         0.1224203
INFO  [17:35:18.844] [bbotk]             -0.0199790                         0.7974203
INFO  [17:35:18.844] [bbotk]              6.8877763                         0.3474203
INFO  [17:35:18.844] [bbotk]              4.7291028                         0.4880453
INFO  [17:35:18.844] [bbotk]             -2.1786525                         0.9380453
INFO  [17:35:18.844] [bbotk]              1.2752251                         0.7130453
INFO  [17:35:18.844] [bbotk]             -5.6325304                         0.2630453
INFO  [17:35:18.844] [bbotk]             -0.4517137                         0.3755453
INFO  [17:35:18.844] [bbotk]              6.4560416                         0.8255453
INFO  [17:35:18.844] [bbotk]             -3.9055915                         0.6005453
INFO  [17:35:18.844] [bbotk]              3.0021639                         0.1505453
INFO  [17:35:18.844] [bbotk]             -6.4959998                         0.8817953
INFO  [17:35:18.844] [bbotk]              0.4117557                         0.4317953
INFO  [17:35:18.844] [bbotk]             -3.0421219                         0.2067953
INFO  [17:35:18.844] [bbotk]              3.8656333                         0.6567953
INFO  [17:35:18.844] [bbotk]             -4.7690610                         0.5442953
INFO  [17:35:18.844] [bbotk]              2.1386945                         0.9942953
INFO  [17:35:18.844] [bbotk]             -1.3151831                         0.7692953
INFO  [17:35:18.844] [bbotk]              5.5925722                         0.3192953
INFO  [17:35:18.844] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:35:18.844] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:35:18.844] [bbotk]                         0.9805574          -2.5254849             -2.8737140
INFO  [17:35:18.844] [bbotk]                         0.5305574          -7.1306552              4.0340409
INFO  [17:35:18.844] [bbotk]                         0.3055574          -4.8280700             -6.3275917
INFO  [17:35:18.844] [bbotk]                         0.7555574          -0.2228998              0.5801633
INFO  [17:35:18.844] [bbotk]                         0.8680574          -1.3741923              2.3071021
INFO  [17:35:18.844] [bbotk]                         0.4180574          -5.9793625             -4.6006530
INFO  [17:35:18.844] [bbotk]                         0.6430574          -3.6767774             -1.1467751
INFO  [17:35:18.844] [bbotk]                         0.1930574          -8.2819477              5.7609797
INFO  [17:35:18.844] [bbotk]                         0.1368074          -1.9498386              4.8975103
INFO  [17:35:18.844] [bbotk]                         0.5868074          -6.5550088             -2.0102445
INFO  [17:35:18.844] [bbotk]                         0.3618074          -4.2524237             -5.4641223
INFO  [17:35:18.844] [bbotk]                         0.8118074          -8.8575940              1.4436327
INFO  [17:35:18.844] [bbotk]                         0.4743074          -0.7985460              6.6244491
INFO  [17:35:18.844] [bbotk]                         0.9243074          -5.4037162             -0.2833057
INFO  [17:35:18.844] [bbotk]                         0.2493074          -3.1011311             -3.7371836
INFO  [17:35:18.844] [bbotk]                         0.6993074          -7.7063015              3.1705715
INFO  [17:35:18.844] [bbotk]                         0.2211824          -0.5107229             -1.5785098
INFO  [17:35:18.844] [bbotk]                         0.6711824          -5.1158931              5.3292450
INFO  [17:35:18.844] [bbotk]                         0.8961824          -7.4184783             -5.0323877
INFO  [17:35:18.844] [bbotk]                         0.4461824          -2.8133080              1.8753674
INFO  [17:35:18.844] [bbotk]                         0.7836824          -6.2671856              3.6023062
INFO  [17:35:18.844] [bbotk]                         0.3336824          -1.6620155             -3.3054487
INFO  [17:35:18.844] [bbotk]                         0.1086824          -3.9646006              0.1484286
INFO  [17:35:18.844] [bbotk]                         0.5586824          -8.5697708             -6.7593264
INFO  [17:35:18.844] [bbotk]                         0.9524324          -1.0863692             -5.8958570
INFO  [17:35:18.844] [bbotk]                         0.5024324          -5.6915394              1.0118980
INFO  [17:35:18.844] [bbotk]                         0.2774324          -7.9941246             -2.4419792
INFO  [17:35:18.844] [bbotk]                         0.7274324          -3.3889543              4.4657756
INFO  [17:35:18.844] [bbotk]                         0.6149324          -2.2376617             -4.1689183
INFO  [17:35:18.844] [bbotk]                         0.1649324          -6.8428319              2.7388368
INFO  [17:35:18.844] [bbotk]                         0.3899324          -9.1454171             -0.7150404
INFO  [17:35:18.844] [bbotk]                         0.8399324          -4.5402468              6.1927144
INFO  [17:35:18.844] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:35:18.844] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:35:18.844] [bbotk]                         16                    1257                 0.5179286
INFO  [17:35:18.844] [bbotk]                          6                    3757                 0.9679286
INFO  [17:35:18.844] [bbotk]                         11                    2507                 0.2929286
INFO  [17:35:18.844] [bbotk]                          1                       7                 0.7429286
INFO  [17:35:18.844] [bbotk]                          9                    3132                 0.1804286
INFO  [17:35:18.844] [bbotk]                         19                     632                 0.6304286
INFO  [17:35:18.844] [bbotk]                         14                    4382                 0.8554286
INFO  [17:35:18.844] [bbotk]                          4                    1882                 0.4054286
INFO  [17:35:18.844] [bbotk]                         12                     945                 0.5741786
INFO  [17:35:18.844] [bbotk]                          2                    3445                 0.1241786
INFO  [17:35:18.844] [bbotk]                          7                    2195                 0.3491786
INFO  [17:35:18.844] [bbotk]                         17                    4695                 0.7991786
INFO  [17:35:18.844] [bbotk]                         20                    2820                 0.2366786
INFO  [17:35:18.844] [bbotk]                         10                     320                 0.6866786
INFO  [17:35:18.844] [bbotk]                          5                    4070                 0.9116786
INFO  [17:35:18.844] [bbotk]                         15                    1570                 0.4616786
INFO  [17:35:18.844] [bbotk]                          2                    3288                 0.3773036
INFO  [17:35:18.844] [bbotk]                         12                     788                 0.8273036
INFO  [17:35:18.844] [bbotk]                          7                    2038                 0.1523036
INFO  [17:35:18.844] [bbotk]                         17                    4538                 0.6023036
INFO  [17:35:18.844] [bbotk]                         19                    2663                 0.4898036
INFO  [17:35:18.844] [bbotk]                          9                     163                 0.9398036
INFO  [17:35:18.844] [bbotk]                         14                    1413                 0.2648036
INFO  [17:35:18.844] [bbotk]                          4                    3913                 0.7148036
INFO  [17:35:18.844] [bbotk]                         18                    1101                 0.8835536
INFO  [17:35:18.844] [bbotk]                          8                    3601                 0.4335536
INFO  [17:35:18.844] [bbotk]                         13                    4851                 0.6585536
INFO  [17:35:18.844] [bbotk]                          3                    2351                 0.2085536
INFO  [17:35:18.844] [bbotk]                         11                    2976                 0.5460536
INFO  [17:35:18.844] [bbotk]                          1                     476                 0.9960536
INFO  [17:35:18.844] [bbotk]                         16                    1726                 0.3210536
INFO  [17:35:18.844] [bbotk]                          6                    4226                 0.7710536
INFO  [17:35:18.844] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:35:18.844] [bbotk]  classif.bbrier warnings errors runtime_learners
INFO  [17:35:18.844] [bbotk]      0.03610263        0      0          191.393
INFO  [17:35:18.844] [bbotk]      0.03646322        0      0          437.477
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          150.009
INFO  [17:35:18.844] [bbotk]      0.03591495        0      0           71.604
INFO  [17:35:18.844] [bbotk]      0.02657433        0      0          175.737
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          109.291
INFO  [17:35:18.844] [bbotk]      0.02698113        0      0          227.922
INFO  [17:35:18.844] [bbotk]      0.22798192        0      0          148.723
INFO  [17:35:18.844] [bbotk]      0.03747032        0      0           77.930
INFO  [17:35:18.844] [bbotk]      0.03511268        0      0          194.679
INFO  [17:35:18.844] [bbotk]      0.02488546        0      0          130.484
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          243.323
INFO  [17:35:18.844] [bbotk]      0.29962476        0      0          148.931
INFO  [17:35:18.844] [bbotk]      0.03166879        0      0           95.640
INFO  [17:35:18.844] [bbotk]      0.02602459        0      0          215.555
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          121.680
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          200.704
INFO  [17:35:18.844] [bbotk]      0.04240731        0      0          160.529
INFO  [17:35:18.844] [bbotk]      0.04692382        0      0          160.951
INFO  [17:35:18.844] [bbotk]      0.02220438        0      0          234.973
INFO  [17:35:18.844] [bbotk]      0.03680647        0      0          201.831
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0           63.651
INFO  [17:35:18.844] [bbotk]      0.03023850        0      0           88.810
INFO  [17:35:18.844] [bbotk]      0.07216495        0      0          174.234
INFO  [17:35:18.844] [bbotk]      0.02557358        0      0           95.229
INFO  [17:35:18.844] [bbotk]      0.03113349        0      0          159.361
INFO  [17:35:18.844] [bbotk]      0.03109328        0      0          174.397
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          114.183
INFO  [17:35:18.844] [bbotk]      0.02352505        0      0          101.595
INFO  [17:35:18.844] [bbotk]      0.07945421        0      0           54.225
INFO  [17:35:18.844] [bbotk]      0.08215930        0      0           90.598
INFO  [17:35:18.844] [bbotk]      0.29969878        0      0          125.337
INFO  [17:35:18.844] [bbotk]  classif.bbrier warnings errors runtime_learners
INFO  [17:35:18.844] [bbotk]                                 uhash
INFO  [17:35:18.844] [bbotk]  daa06ef5-ebcb-4ef0-a843-7e0324f1d818
INFO  [17:35:18.844] [bbotk]  5ce2951e-abfa-4640-94bf-20900bf56022
INFO  [17:35:18.844] [bbotk]  0e887464-bf75-4711-ac68-c84d3a5893bf
INFO  [17:35:18.844] [bbotk]  7effd5d1-7c96-473f-9c73-7caa5759e42d
INFO  [17:35:18.844] [bbotk]  4f5824a4-335e-4304-b9a0-10ab094b7e2d
INFO  [17:35:18.844] [bbotk]  0e83bc81-2a63-4a4d-931a-fd84489b6f80
INFO  [17:35:18.844] [bbotk]  6a4914b1-ec35-4e9e-ab96-f1a45a4d8f48
INFO  [17:35:18.844] [bbotk]  4bf3d326-8765-4d19-97aa-c588f053485f
INFO  [17:35:18.844] [bbotk]  b84afe2a-4b13-47f4-b743-36f195ab7e32
INFO  [17:35:18.844] [bbotk]  64440bb9-886a-4770-9503-1e6ba14ee568
INFO  [17:35:18.844] [bbotk]  81bf64e6-6de5-4a52-9780-ac6c0a66a27e
INFO  [17:35:18.844] [bbotk]  ef301f95-ee40-4598-834a-22302aa23e2d
INFO  [17:35:18.844] [bbotk]  ac677277-d7d0-46b2-879f-db2f76c07f97
INFO  [17:35:18.844] [bbotk]  a1f0ed23-8e9f-4bff-833b-63f228b95573
INFO  [17:35:18.844] [bbotk]  c9f6c916-3b69-4ece-bffb-fe24b88dc226
INFO  [17:35:18.844] [bbotk]  527a7d63-5689-4499-ae05-0c0976dcbf51
INFO  [17:35:18.844] [bbotk]  0ec64529-7e44-42f0-99a1-2f5c2e06f4d6
INFO  [17:35:18.844] [bbotk]  737e2f4a-de77-4aa2-90fd-5cb55844cf01
INFO  [17:35:18.844] [bbotk]  02540152-4127-4f31-ab16-4668baed3b6f
INFO  [17:35:18.844] [bbotk]  eaae06e9-34b8-4be0-814b-36027e9fbf73
INFO  [17:35:18.844] [bbotk]  b1b191bc-dd36-4ccf-912c-1824e5bce98a
INFO  [17:35:18.844] [bbotk]  497bf98f-66a5-4aa4-8aeb-9965e82d5a26
INFO  [17:35:18.844] [bbotk]  c03731a1-6a02-422b-bf66-54fde2fb30f0
INFO  [17:35:18.844] [bbotk]  9b11e581-d515-421e-ae32-5841a6f6065b
INFO  [17:35:18.844] [bbotk]  bff35022-2cba-493d-a03f-01524da9313a
INFO  [17:35:18.844] [bbotk]  53879cf9-6019-4faa-8cde-2c66ac9b6b46
INFO  [17:35:18.844] [bbotk]  c9176c77-93ee-4ed8-9d76-e3890a2d03bc
INFO  [17:35:18.844] [bbotk]  9cae88a7-d3f0-419a-9ede-d918c0df871f
INFO  [17:35:18.844] [bbotk]  d37b1329-11de-4cb9-903c-ce9053ab3587
INFO  [17:35:18.844] [bbotk]  52327f38-b3c8-40df-8c57-f207928bf6c3
INFO  [17:35:18.844] [bbotk]  94c41c47-3c02-4bda-8642-d535ab74f7c6
INFO  [17:35:18.844] [bbotk]  23a282d4-10b1-4a31-bf3f-0e10be1cb406
INFO  [17:35:18.844] [bbotk]                                 uhash
INFO  [17:35:29.207] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 32 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:35:34.319] [bbotk] Evaluating 1 configuration(s)
INFO  [17:35:34.493] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:35:34.817] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:36:05.748] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:36:42.821] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:37:10.963] [mlr3] Finished benchmark
INFO  [17:37:11.137] [bbotk] Result of batch 2:
INFO  [17:37:11.311] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:37:11.311] [bbotk]               1.676561                         0.1142579
INFO  [17:37:11.311] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:37:11.311] [bbotk]                         0.7880503           -3.561103              -2.905829
INFO  [17:37:11.311] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:37:11.311] [bbotk]                         12                    2578                 0.9054256
INFO  [17:37:11.311] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:37:11.311] [bbotk]  0.04365431 <list[8]>              FALSE     0.03214763        0      0
INFO  [17:37:11.311] [bbotk]  runtime_learners                                uhash
INFO  [17:37:11.311] [bbotk]            95.569 9c920b6f-c65b-473d-bd3e-665d16597804
INFO  [17:37:12.545] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 33 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:37:17.664] [bbotk] Evaluating 1 configuration(s)
INFO  [17:37:17.930] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:37:18.220] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:38:34.851] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:39:24.742] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:40:24.203] [mlr3] Finished benchmark
INFO  [17:40:24.312] [bbotk] Result of batch 3:
INFO  [17:40:24.367] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:40:24.367] [bbotk]              -5.463278                         0.9807203
INFO  [17:40:24.367] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:40:24.367] [bbotk]                         0.8180538           -4.733949              -2.952343
INFO  [17:40:24.367] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:40:24.367] [bbotk]                         20                    4649                 0.8403547
INFO  [17:40:24.367] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:40:24.367] [bbotk]  0.03381796 <list[8]>              FALSE     0.02591884        0      0
INFO  [17:40:24.367] [bbotk]  runtime_learners                                uhash
INFO  [17:40:24.367] [bbotk]           185.521 15c9785c-d291-4168-a967-9ac75e77dcd7
INFO  [17:40:24.971] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 34 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:40:32.547] [bbotk] Evaluating 1 configuration(s)
INFO  [17:40:32.670] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:40:32.750] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:41:24.310] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:42:17.746] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:42:58.173] [mlr3] Finished benchmark
INFO  [17:42:58.271] [bbotk] Result of batch 4:
INFO  [17:42:58.293] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:42:58.293] [bbotk]              -4.363231                         0.2514801
INFO  [17:42:58.293] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:42:58.293] [bbotk]                         0.5433953            -4.33641              0.5313282
INFO  [17:42:58.293] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:42:58.293] [bbotk]                         19                    4657                  0.202916
INFO  [17:42:58.293] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:42:58.293] [bbotk]  0.02556215 <list[8]>              FALSE     0.02710673        0      0
INFO  [17:42:58.293] [bbotk]  runtime_learners                                uhash
INFO  [17:42:58.293] [bbotk]           145.061 a3a42a17-0f64-48c4-befd-868a3a4787db
INFO  [17:42:58.911] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 35 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:43:07.141] [bbotk] Evaluating 1 configuration(s)
INFO  [17:43:07.431] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:43:07.525] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:43:59.883] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:44:37.618] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:45:39.459] [mlr3] Finished benchmark
INFO  [17:45:39.616] [bbotk] Result of batch 5:
INFO  [17:45:39.681] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:45:39.681] [bbotk]              -5.884545                         0.4967166
INFO  [17:45:39.681] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:45:39.681] [bbotk]                         0.7173373           -0.132144              -2.133655
INFO  [17:45:39.681] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:45:39.681] [bbotk]                         15                    4432                 0.8743592
INFO  [17:45:39.681] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:45:39.681] [bbotk]  0.02394154 <list[8]>              FALSE      0.0267099        0      0
INFO  [17:45:39.681] [bbotk]  runtime_learners                                uhash
INFO  [17:45:39.681] [bbotk]           150.848 271821b5-7c66-4e18-8fc4-530b8ab9ca8a
INFO  [17:45:41.255] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 36 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:45:49.460] [bbotk] Evaluating 1 configuration(s)
INFO  [17:45:49.509] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:45:49.545] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:46:35.899] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:47:12.873] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:47:55.102] [mlr3] Finished benchmark
INFO  [17:47:55.270] [bbotk] Result of batch 6:
INFO  [17:47:55.404] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:47:55.404] [bbotk]              -2.629819                         0.5335241
INFO  [17:47:55.404] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:47:55.404] [bbotk]                         0.2038711          -0.8627254             -0.5201941
INFO  [17:47:55.404] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:47:55.404] [bbotk]                          1                    3977                 0.6986477
INFO  [17:47:55.404] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:47:55.404] [bbotk]  0.02068733 <list[8]>              FALSE     0.02812868        0      0
INFO  [17:47:55.404] [bbotk]  runtime_learners                                uhash
INFO  [17:47:55.404] [bbotk]           124.541 5342c762-1e3b-4ca8-9cdc-da773c63a3a1
INFO  [17:47:56.068] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 37 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:48:01.619] [bbotk] Evaluating 1 configuration(s)
INFO  [17:48:01.799] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:48:01.928] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:48:18.891] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:48:39.796] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:48:59.334] [mlr3] Finished benchmark
INFO  [17:48:59.596] [bbotk] Result of batch 7:
INFO  [17:48:59.640] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:48:59.640] [bbotk]              0.5526983                         0.4204469
INFO  [17:48:59.640] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:48:59.640] [bbotk]                         0.2555379          -0.1492752              -2.513226
INFO  [17:48:59.640] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:48:59.640] [bbotk]                         18                     703                 0.4087695
INFO  [17:48:59.640] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:48:59.640] [bbotk]  0.01882174 <list[8]>              FALSE     0.02845619        0      0
INFO  [17:48:59.640] [bbotk]  runtime_learners                                uhash
INFO  [17:48:59.640] [bbotk]            57.017 cff4b630-36ec-439d-b4d2-8bedc56acc5d
INFO  [17:49:00.432] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 38 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:49:04.108] [bbotk] Evaluating 1 configuration(s)
INFO  [17:49:04.291] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:49:04.466] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:49:34.803] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:50:07.536] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:50:53.579] [mlr3] Finished benchmark
INFO  [17:50:53.723] [bbotk] Result of batch 8:
INFO  [17:50:53.889] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:50:53.889] [bbotk]              -5.504387                         0.2641157
INFO  [17:50:53.889] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:50:53.889] [bbotk]                         0.9755028          -0.4731725              -3.295976
INFO  [17:50:53.889] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:50:53.889] [bbotk]                         14                    4649                 0.3256394
INFO  [17:50:53.889] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:50:53.889] [bbotk]  0.01726443 <list[8]>              FALSE     0.03023828        0      0
INFO  [17:50:53.889] [bbotk]  runtime_learners                                uhash
INFO  [17:50:53.889] [bbotk]           108.742 abaee957-2e12-428e-b4f4-32eda81e401a
INFO  [17:50:54.887] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 39 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:51:02.980] [bbotk] Evaluating 1 configuration(s)
INFO  [17:51:03.180] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:51:03.387] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:51:22.345] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:51:39.923] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:51:57.763] [mlr3] Finished benchmark
INFO  [17:51:58.597] [bbotk] Result of batch 9:
INFO  [17:51:58.679] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:51:58.679] [bbotk]               1.326516                         0.6505741
INFO  [17:51:58.679] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:51:58.679] [bbotk]                         0.9799454          -0.4275843              0.5490485
INFO  [17:51:58.679] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:51:58.679] [bbotk]                         18                     212                 0.7492349
INFO  [17:51:58.679] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:51:58.679] [bbotk]  0.01861783 <list[8]>              FALSE      0.0255136        0      0
INFO  [17:51:58.679] [bbotk]  runtime_learners                                uhash
INFO  [17:51:58.679] [bbotk]            53.899 dca79cfc-b349-40f0-b47b-5d28279b41f5
INFO  [17:52:01.373] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 40 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:52:10.800] [bbotk] Evaluating 1 configuration(s)
INFO  [17:52:11.560] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:52:11.857] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:52:35.669] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:53:02.610] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [17:53:27.654] [mlr3] Finished benchmark
INFO  [17:53:27.773] [bbotk] Result of batch 10:
INFO  [17:53:27.802] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:53:27.802] [bbotk]               2.089219                         0.9902447
INFO  [17:53:27.802] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:53:27.802] [bbotk]                         0.7133415          -0.5262226              -6.000241
INFO  [17:53:27.802] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:53:27.802] [bbotk]                          8                     572                 0.7069755
INFO  [17:53:27.802] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:53:27.802] [bbotk]  0.01670117 <list[8]>              FALSE     0.03401219        0      0
INFO  [17:53:27.802] [bbotk]  runtime_learners                                uhash
INFO  [17:53:27.802] [bbotk]            75.181 ae96551e-3fdb-4bb4-ad3f-9e58ef39c48f
INFO  [17:53:28.539] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 41 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:53:36.335] [bbotk] Evaluating 1 configuration(s)
INFO  [17:53:36.532] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:53:36.744] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:54:07.473] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:54:44.786] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:55:22.290] [mlr3] Finished benchmark
INFO  [17:55:22.407] [bbotk] Result of batch 11:
INFO  [17:55:22.473] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:55:22.473] [bbotk]              -5.145579                         0.6913103
INFO  [17:55:22.473] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:55:22.473] [bbotk]                         0.4783385           -1.386718              -1.549031
INFO  [17:55:22.473] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:55:22.473] [bbotk]                         15                    4441                 0.6387505
INFO  [17:55:22.473] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:55:22.473] [bbotk]  0.01600148 <list[8]>              FALSE     0.02259786        0      0
INFO  [17:55:22.473] [bbotk]  runtime_learners                                uhash
INFO  [17:55:22.473] [bbotk]           104.897 6dfbc7b5-028e-434d-87fd-2f1862247f22
INFO  [17:55:23.602] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 42 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:55:28.073] [bbotk] Evaluating 1 configuration(s)
INFO  [17:55:28.141] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:55:28.186] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:56:07.836] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:56:55.399] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:57:53.003] [mlr3] Finished benchmark
INFO  [17:57:53.089] [bbotk] Result of batch 12:
INFO  [17:57:53.185] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:57:53.185] [bbotk]              -3.569141                         0.8581779
INFO  [17:57:53.185] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:57:53.185] [bbotk]                          0.568833            -4.63578             -0.3072388
INFO  [17:57:53.185] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:57:53.185] [bbotk]                          3                    3133                 0.8151745
INFO  [17:57:53.185] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:57:53.185] [bbotk]  0.01704721 <list[8]>              FALSE     0.02283081        0      0
INFO  [17:57:53.185] [bbotk]  runtime_learners                                uhash
INFO  [17:57:53.185] [bbotk]           143.587 dda05c9b-e2a5-42fd-914d-533863f0824e
INFO  [17:57:54.645] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 43 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:58:00.401] [bbotk] Evaluating 1 configuration(s)
INFO  [17:58:00.506] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:58:00.670] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:58:18.247] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [17:58:31.613] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [17:58:51.526] [mlr3] Finished benchmark
INFO  [17:58:51.703] [bbotk] Result of batch 13:
INFO  [17:58:51.721] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [17:58:51.721] [bbotk]              0.1852149                         0.4735673
INFO  [17:58:51.721] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [17:58:51.721] [bbotk]                         0.1425088           -2.334743               4.001426
INFO  [17:58:51.721] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [17:58:51.721] [bbotk]                         13                    1748                 0.8885711
INFO  [17:58:51.721] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [17:58:51.721] [bbotk]  0.01616057 <list[8]>              FALSE      0.0314681        0      0
INFO  [17:58:51.721] [bbotk]  runtime_learners                                uhash
INFO  [17:58:51.721] [bbotk]            49.765 6f53f5a1-f362-40a2-85ec-ff8b5e934132
INFO  [17:58:52.874] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 44 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [17:58:59.975] [bbotk] Evaluating 1 configuration(s)
INFO  [17:59:00.452] [mlr3] Running benchmark with 3 resampling iterations
INFO  [17:59:00.660] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [17:59:45.914] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:00:28.033] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:01:16.739] [mlr3] Finished benchmark
INFO  [18:01:17.131] [bbotk] Result of batch 14:
INFO  [18:01:17.283] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:01:17.283] [bbotk]             -0.2153466                          0.561375
INFO  [18:01:17.283] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:01:17.283] [bbotk]                         0.8509519          -0.1076683             -0.1683519
INFO  [18:01:17.283] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:01:17.283] [bbotk]                         16                    3876                 0.5994755
INFO  [18:01:17.283] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:01:17.283] [bbotk]  0.01517906 <list[8]>              FALSE     0.02546224        0      0
INFO  [18:01:17.283] [bbotk]  runtime_learners                                uhash
INFO  [18:01:17.283] [bbotk]           135.341 162054e8-dff1-4bbf-bbb2-59cb808af08f
INFO  [18:01:18.485] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 45 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:01:44.487] [bbotk] Evaluating 1 configuration(s)
INFO  [18:01:44.644] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:01:44.749] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:02:06.828] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:02:25.736] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:02:42.588] [mlr3] Finished benchmark
INFO  [18:02:42.730] [bbotk] Result of batch 15:
INFO  [18:02:42.746] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:02:42.746] [bbotk]              -5.351897                         0.1860564
INFO  [18:02:42.746] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:02:42.746] [bbotk]                         0.8946363            -1.53674              -1.459736
INFO  [18:02:42.746] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:02:42.746] [bbotk]                         20                     431                 0.8029948
INFO  [18:02:42.746] [bbotk]     acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:02:42.746] [bbotk]  0.0131419 <list[8]>              FALSE     0.02509371        0      0
INFO  [18:02:42.746] [bbotk]  runtime_learners                                uhash
INFO  [18:02:42.746] [bbotk]            57.507 ebf19b5d-69c3-4469-a952-fb58b716b1fe
INFO  [18:02:44.597] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 46 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:02:48.245] [bbotk] Evaluating 1 configuration(s)
INFO  [18:02:48.339] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:02:48.426] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:03:40.587] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:04:21.314] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:05:01.809] [mlr3] Finished benchmark
INFO  [18:05:01.961] [bbotk] Result of batch 16:
INFO  [18:05:02.049] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:05:02.049] [bbotk]              -5.401158                         0.2646887
INFO  [18:05:02.049] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:05:02.049] [bbotk]                         0.6881096           -4.583426              -3.258114
INFO  [18:05:02.049] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:05:02.049] [bbotk]                         15                    4891                 0.6557832
INFO  [18:05:02.049] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:05:02.049] [bbotk]  0.01313552 <list[8]>              FALSE     0.02380755        0      0
INFO  [18:05:02.049] [bbotk]  runtime_learners                                uhash
INFO  [18:05:02.049] [bbotk]           132.564 71da44d3-35cd-43a1-b63e-5d48122730ba
INFO  [18:05:02.871] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 47 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:05:09.370] [bbotk] Evaluating 1 configuration(s)
INFO  [18:05:09.621] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:05:09.927] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:05:34.540] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:05:59.439] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:06:29.197] [mlr3] Finished benchmark
INFO  [18:06:29.300] [bbotk] Result of batch 17:
INFO  [18:06:29.334] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:06:29.334] [bbotk]              -2.671849                         0.4512675
INFO  [18:06:29.334] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:06:29.334] [bbotk]                         0.6605253             -2.2912              -6.363962
INFO  [18:06:29.334] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:06:29.334] [bbotk]                          4                    1953                 0.3758579
INFO  [18:06:29.334] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:06:29.334] [bbotk]  0.01420473 <list[8]>              FALSE     0.02571004        0      0
INFO  [18:06:29.334] [bbotk]  runtime_learners                                uhash
INFO  [18:06:29.334] [bbotk]            78.127 73ff4d07-daef-488e-99a8-40c11a208646
INFO  [18:06:30.229] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 48 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:06:36.787] [bbotk] Evaluating 1 configuration(s)
INFO  [18:06:37.071] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:06:37.640] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:07:39.065] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:08:33.103] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:09:25.983] [mlr3] Finished benchmark
INFO  [18:09:26.108] [bbotk] Result of batch 18:
INFO  [18:09:26.131] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:09:26.131] [bbotk]              -5.189561                         0.4592249
INFO  [18:09:26.131] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:09:26.131] [bbotk]                         0.9760194           -3.327172              0.5216464
INFO  [18:09:26.131] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:09:26.131] [bbotk]                         20                    4063                 0.6477486
INFO  [18:09:26.131] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:09:26.131] [bbotk]  0.01353437 <list[8]>              FALSE     0.02374981        0      0
INFO  [18:09:26.131] [bbotk]  runtime_learners                                uhash
INFO  [18:09:26.131] [bbotk]           167.683 5d554c2d-9ae3-4eda-8835-cabc8f62247b
INFO  [18:09:27.216] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 49 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:09:32.000] [bbotk] Evaluating 1 configuration(s)
INFO  [18:09:32.170] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:09:32.514] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:10:15.726] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:10:55.036] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:11:49.154] [mlr3] Finished benchmark
INFO  [18:11:49.433] [bbotk] Result of batch 19:
INFO  [18:11:49.483] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:11:49.483] [bbotk]              -2.274364                          0.121884
INFO  [18:11:49.483] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:11:49.483] [bbotk]                         0.7588402           -3.978302              -0.263944
INFO  [18:11:49.483] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:11:49.483] [bbotk]                          9                    3266                    0.5968
INFO  [18:11:49.483] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:11:49.483] [bbotk]  0.01421744 <list[8]>              FALSE     0.02470267        0      0
INFO  [18:11:49.483] [bbotk]  runtime_learners                                uhash
INFO  [18:11:49.483] [bbotk]            135.47 aa2d5cc5-6a12-4053-ac6e-ddc5d242f801
INFO  [18:11:50.722] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 50 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:11:56.794] [bbotk] Evaluating 1 configuration(s)
INFO  [18:11:57.086] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:11:57.266] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:12:16.021] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:12:36.121] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:12:54.671] [mlr3] Finished benchmark
INFO  [18:12:54.946] [bbotk] Result of batch 20:
INFO  [18:12:54.984] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:12:54.984] [bbotk]              -5.331547                         0.9870437
INFO  [18:12:54.984] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:12:54.984] [bbotk]                          0.735067              -2.503               -4.52084
INFO  [18:12:54.984] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:12:54.984] [bbotk]                         15                     520                 0.1641414
INFO  [18:12:54.984] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:12:54.984] [bbotk]  0.01154854 <list[8]>              FALSE     0.02920636        0      0
INFO  [18:12:54.984] [bbotk]  runtime_learners                                uhash
INFO  [18:12:54.984] [bbotk]            56.052 b7e6703d-6997-446b-a9b0-f93909fa7c6b
INFO  [18:12:56.344] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 51 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:12:59.335] [bbotk] Evaluating 1 configuration(s)
INFO  [18:12:59.655] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:12:59.928] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:13:35.596] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:13:59.754] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:14:23.417] [mlr3] Finished benchmark
INFO  [18:14:23.672] [bbotk] Result of batch 21:
INFO  [18:14:23.877] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:14:23.877] [bbotk]              -2.937933                         0.3112609
INFO  [18:14:23.877] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:14:23.877] [bbotk]                         0.4916291           -1.935577                1.84818
INFO  [18:14:23.877] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:14:23.877] [bbotk]                         18                    1411                 0.9129858
INFO  [18:14:23.877] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:14:23.877] [bbotk]  0.01130631 <list[8]>              FALSE     0.02371515        0      0
INFO  [18:14:23.877] [bbotk]  runtime_learners                                uhash
INFO  [18:14:23.877] [bbotk]            82.602 d022e9c4-be6d-4d2a-858b-989c4f5bd67b
INFO  [18:14:25.779] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 52 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:14:32.195] [bbotk] Evaluating 1 configuration(s)
INFO  [18:14:32.250] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:14:32.335] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:15:18.807] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:16:06.362] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:16:49.425] [mlr3] Finished benchmark
INFO  [18:16:49.551] [bbotk] Result of batch 22:
INFO  [18:16:49.586] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:16:49.586] [bbotk]               2.000559                         0.7974823
INFO  [18:16:49.586] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:16:49.586] [bbotk]                         0.9491036          -0.8107216              -5.634282
INFO  [18:16:49.586] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:16:49.586] [bbotk]                         15                    4633                 0.2828529
INFO  [18:16:49.586] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:16:49.586] [bbotk]  0.01179622 <list[8]>              FALSE     0.03706328        0      0
INFO  [18:16:49.586] [bbotk]  runtime_learners                                uhash
INFO  [18:16:49.586] [bbotk]           136.252 dec0eee6-49dd-4325-8b76-a6b59b3f5f01
INFO  [18:16:50.469] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 53 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:16:58.219] [bbotk] Evaluating 1 configuration(s)
INFO  [18:16:58.328] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:16:58.510] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:17:36.031] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:18:10.622] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:18:48.690] [mlr3] Finished benchmark
INFO  [18:18:48.920] [bbotk] Result of batch 23:
INFO  [18:18:49.101] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:18:49.101] [bbotk]              -1.398583                         0.1347841
INFO  [18:18:49.101] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:18:49.101] [bbotk]                         0.1830793           -0.397103              -6.392806
INFO  [18:18:49.101] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:18:49.101] [bbotk]                          6                    3331                 0.4832383
INFO  [18:18:49.101] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:18:49.101] [bbotk]  0.01053265 <list[8]>              FALSE     0.03046857        0      0
INFO  [18:18:49.101] [bbotk]  runtime_learners                                uhash
INFO  [18:18:49.101] [bbotk]           109.602 df8125a1-c58d-467b-a3c9-a8c3bffbf737
INFO  [18:18:51.060] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 54 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:18:56.307] [bbotk] Evaluating 1 configuration(s)
INFO  [18:18:56.385] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:18:56.592] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:19:35.562] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:20:01.947] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:20:42.529] [mlr3] Finished benchmark
INFO  [18:20:42.920] [bbotk] Result of batch 24:
INFO  [18:20:43.255] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:20:43.255] [bbotk]              -4.868663                         0.1347613
INFO  [18:20:43.255] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:20:43.255] [bbotk]                          0.211978           -1.496782               2.369058
INFO  [18:20:43.255] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:20:43.255] [bbotk]                         18                    3417                 0.9217544
INFO  [18:20:43.255] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:20:43.255] [bbotk]  0.01011812 <list[8]>              FALSE     0.02542483        0      0
INFO  [18:20:43.255] [bbotk]  runtime_learners                                uhash
INFO  [18:20:43.255] [bbotk]           103.804 8bfcfcdd-0089-4a35-aca1-ab9f0be915b8
INFO  [18:20:51.184] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 55 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:20:57.770] [bbotk] Evaluating 1 configuration(s)
INFO  [18:20:57.850] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:20:57.947] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:21:19.814] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:21:39.011] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:22:01.135] [mlr3] Finished benchmark
INFO  [18:22:01.268] [bbotk] Result of batch 25:
INFO  [18:22:01.326] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:22:01.326] [bbotk]               2.284656                         0.3643906
INFO  [18:22:01.326] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:22:01.326] [bbotk]                         0.9995606           -3.823881              -6.465998
INFO  [18:22:01.326] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:22:01.326] [bbotk]                         11                    1290                 0.1861311
INFO  [18:22:01.326] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:22:01.326] [bbotk]  0.01115718 <list[8]>              FALSE     0.06160205        0      0
INFO  [18:22:01.326] [bbotk]  runtime_learners                                uhash
INFO  [18:22:01.326] [bbotk]             62.58 6049c585-7907-4728-931f-02510d30a0c3
INFO  [18:22:02.815] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 56 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:22:10.482] [bbotk] Evaluating 1 configuration(s)
INFO  [18:22:10.703] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:22:10.983] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:22:47.077] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:23:33.714] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:24:25.677] [mlr3] Finished benchmark
INFO  [18:24:25.769] [bbotk] Result of batch 26:
INFO  [18:24:25.816] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:24:25.816] [bbotk]               1.327263                         0.8272581
INFO  [18:24:25.816] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:24:25.816] [bbotk]                         0.4237252           -1.523832              -2.008206
INFO  [18:24:25.816] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:24:25.816] [bbotk]                          4                    3510                 0.6118964
INFO  [18:24:25.816] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:24:25.816] [bbotk]  0.01176017 <list[8]>              FALSE     0.02889277        0      0
INFO  [18:24:25.816] [bbotk]  runtime_learners                                uhash
INFO  [18:24:25.816] [bbotk]           134.381 a7f970ed-526e-4474-b2ca-a5de3fc47a3f
INFO  [18:24:28.334] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 57 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:24:35.900] [bbotk] Evaluating 1 configuration(s)
INFO  [18:24:36.087] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:24:36.151] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:25:00.141] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:25:25.479] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:26:19.966] [mlr3] Finished benchmark
INFO  [18:26:20.326] [bbotk] Result of batch 27:
INFO  [18:26:20.521] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:26:20.521] [bbotk]              -3.032742                         0.2421861
INFO  [18:26:20.521] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:26:20.521] [bbotk]                         0.6179087           -4.036259               4.356392
INFO  [18:26:20.521] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:26:20.521] [bbotk]                         11                    4796                 0.6167266
INFO  [18:26:20.521] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:26:20.521] [bbotk]  0.01099303 <list[8]>              FALSE     0.03021267        0      0
INFO  [18:26:20.521] [bbotk]  runtime_learners                                uhash
INFO  [18:26:20.521] [bbotk]           102.846 80a69cf5-e94b-4bbc-a3ce-d6c8538360aa
WARN  [18:26:23.696] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:26:23.802] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 58 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:26:31.174] [bbotk] Evaluating 1 configuration(s)
INFO  [18:26:31.268] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:26:31.375] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:26:40.649] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:26:57.766] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:27:17.413] [mlr3] Finished benchmark
INFO  [18:27:17.615] [bbotk] Result of batch 28:
INFO  [18:27:17.661] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:27:17.661] [bbotk]              -3.982342                         0.5639562
INFO  [18:27:17.661] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:27:17.661] [bbotk]                         0.5690885           -3.706101              -4.018274
INFO  [18:27:17.661] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:27:17.661] [bbotk]                         16                     242                 0.9454928
INFO  [18:27:17.661] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:27:17.661] [bbotk]  0.00963752 <list[8]>              FALSE     0.02550499        0      0
INFO  [18:27:17.661] [bbotk]  runtime_learners                                uhash
INFO  [18:27:17.661] [bbotk]            44.535 483bb62a-096c-4194-846b-7754ed33cf0f
INFO  [18:27:18.870] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 59 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:27:28.596] [bbotk] Evaluating 1 configuration(s)
INFO  [18:27:28.756] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:27:29.023] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:28:14.912] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:28:54.805] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:29:36.770] [mlr3] Finished benchmark
INFO  [18:29:36.935] [bbotk] Result of batch 29:
INFO  [18:29:37.012] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:29:37.012] [bbotk]              -5.571513                         0.2671462
INFO  [18:29:37.012] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:29:37.012] [bbotk]                          0.565334           -2.578139              -1.171329
INFO  [18:29:37.012] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:29:37.012] [bbotk]                         19                    4116                 0.5164556
INFO  [18:29:37.012] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:29:37.012] [bbotk]  0.01022328 <list[8]>              FALSE     0.02480541        0      0
INFO  [18:29:37.012] [bbotk]  runtime_learners                                uhash
INFO  [18:29:37.012] [bbotk]           126.562 59c9157b-0a69-4fb3-843e-90dd59459e2d
INFO  [18:29:38.702] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 60 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:29:42.419] [bbotk] Evaluating 1 configuration(s)
INFO  [18:29:42.463] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:29:42.526] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:30:32.924] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:31:19.364] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:32:14.903] [mlr3] Finished benchmark
INFO  [18:32:15.169] [bbotk] Result of batch 30:
INFO  [18:32:15.221] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:32:15.221] [bbotk]               2.074561                         0.7956811
INFO  [18:32:15.221] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:32:15.221] [bbotk]                           0.87785           -4.054657              -4.883861
INFO  [18:32:15.221] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:32:15.221] [bbotk]                         13                    4836                 0.7449434
INFO  [18:32:15.221] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:32:15.221] [bbotk]  0.008907393 <list[8]>              FALSE     0.03193831        0      0
INFO  [18:32:15.221] [bbotk]  runtime_learners                                uhash
INFO  [18:32:15.221] [bbotk]            151.75 900ae5f4-3b5c-4f8e-b6e5-c18765edd593
WARN  [18:32:17.249] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:32:17.292] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 61 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:32:21.314] [bbotk] Evaluating 1 configuration(s)
INFO  [18:32:21.382] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:32:21.485] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:32:56.050] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:33:33.990] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:33:56.300] [mlr3] Finished benchmark
INFO  [18:33:56.497] [bbotk] Result of batch 31:
INFO  [18:33:56.630] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:33:56.630] [bbotk]               1.438442                         0.7108109
INFO  [18:33:56.630] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:33:56.630] [bbotk]                         0.5915903           -0.381564              -2.846779
INFO  [18:33:56.630] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:33:56.630] [bbotk]                         17                    1635                 0.6879256
INFO  [18:33:56.630] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:33:56.630] [bbotk]  0.01069401 <list[8]>              FALSE     0.02978159        0      0
INFO  [18:33:56.630] [bbotk]  runtime_learners                                uhash
INFO  [18:33:56.630] [bbotk]            94.618 d2384264-83aa-4d88-abab-4131bb628f25
WARN  [18:33:58.095] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:33:58.156] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 62 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:34:03.373] [bbotk] Evaluating 1 configuration(s)
INFO  [18:34:03.412] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:34:03.460] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:34:27.935] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:34:56.245] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:35:23.633] [mlr3] Finished benchmark
INFO  [18:35:23.856] [bbotk] Result of batch 32:
INFO  [18:35:24.029] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:35:24.029] [bbotk]              0.3372499                         0.1980828
INFO  [18:35:24.029] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:35:24.029] [bbotk]                         0.6465728           -2.216281               6.245921
INFO  [18:35:24.029] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:35:24.029] [bbotk]                         10                    1024                 0.5278898
INFO  [18:35:24.029] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:35:24.029] [bbotk]  0.01290742 <list[8]>              FALSE     0.03571556        0      0
INFO  [18:35:24.029] [bbotk]  runtime_learners                                uhash
INFO  [18:35:24.029] [bbotk]            79.933 2ca80eb4-f76e-4a18-b07a-d5abc792415f
INFO  [18:35:26.101] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 63 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:35:31.427] [bbotk] Evaluating 1 configuration(s)
INFO  [18:35:31.802] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:35:32.031] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:36:05.147] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:36:47.478] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:37:23.473] [mlr3] Finished benchmark
INFO  [18:37:23.584] [bbotk] Result of batch 33:
INFO  [18:37:23.732] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:37:23.732] [bbotk]               -4.98072                          0.953148
INFO  [18:37:23.732] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:37:23.732] [bbotk]                         0.9661416           -5.592032              -5.993243
INFO  [18:37:23.732] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:37:23.732] [bbotk]                          6                     543                  0.680732
INFO  [18:37:23.732] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:37:23.732] [bbotk]  0.008430622 <list[8]>              FALSE     0.03257209        0      0
INFO  [18:37:23.732] [bbotk]  runtime_learners                                uhash
INFO  [18:37:23.732] [bbotk]           110.702 1fda2de0-66cd-423e-b0ae-b06bb9cdc560
INFO  [18:37:25.213] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 64 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:37:33.846] [bbotk] Evaluating 1 configuration(s)
INFO  [18:37:34.046] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:37:34.306] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:38:29.974] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:39:52.844] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:40:53.612] [mlr3] Finished benchmark
INFO  [18:40:53.714] [bbotk] Result of batch 34:
INFO  [18:40:53.750] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:40:53.750] [bbotk]              -4.132465                         0.7610398
INFO  [18:40:53.750] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:40:53.750] [bbotk]                         0.7358528           -8.031983              -3.607576
INFO  [18:40:53.750] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:40:53.750] [bbotk]                          2                    4490                 0.2127124
INFO  [18:40:53.750] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:40:53.750] [bbotk]  0.008780446 <list[8]>              FALSE     0.03537591        0      0
INFO  [18:40:53.750] [bbotk]  runtime_learners                                uhash
INFO  [18:40:53.750] [bbotk]           198.917 df352961-c64e-44f9-805c-8ad20f7307e7
INFO  [18:40:54.515] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 65 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:41:02.746] [bbotk] Evaluating 1 configuration(s)
INFO  [18:41:02.916] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:41:03.062] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:41:14.465] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:41:30.051] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [18:41:47.011] [mlr3] Finished benchmark
INFO  [18:41:47.312] [bbotk] Result of batch 35:
INFO  [18:41:47.363] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:41:47.363] [bbotk]               0.979474                         0.1013292
INFO  [18:41:47.363] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:41:47.363] [bbotk]                         0.2224048           -2.877396               1.287085
INFO  [18:41:47.363] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:41:47.363] [bbotk]                          8                     146                 0.6179426
INFO  [18:41:47.363] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:41:47.363] [bbotk]  0.01018059 <list[8]>              FALSE     0.03419395        0      0
INFO  [18:41:47.363] [bbotk]  runtime_learners                                uhash
INFO  [18:41:47.363] [bbotk]            43.745 d5bf1a5e-4475-4794-abde-367f8a96b352
INFO  [18:41:50.186] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 66 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:41:57.122] [bbotk] Evaluating 1 configuration(s)
INFO  [18:41:57.635] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:41:58.006] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:43:04.563] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:43:53.763] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:44:51.557] [mlr3] Finished benchmark
INFO  [18:44:51.673] [bbotk] Result of batch 36:
INFO  [18:44:51.696] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:44:51.696] [bbotk]               -3.72851                         0.4435565
INFO  [18:44:51.696] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:44:51.696] [bbotk]                         0.5122266           -2.303945               1.566995
INFO  [18:44:51.696] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:44:51.696] [bbotk]                         11                    4524                 0.6965498
INFO  [18:44:51.696] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:44:51.696] [bbotk]  0.00820302 <list[8]>              FALSE     0.02268937        0      0
INFO  [18:44:51.696] [bbotk]  runtime_learners                                uhash
INFO  [18:44:51.696] [bbotk]           172.904 42e34b39-635b-49c4-b872-ec0eee4d3b8c
INFO  [18:44:52.592] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 67 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:45:00.515] [bbotk] Evaluating 1 configuration(s)
INFO  [18:45:00.978] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:45:01.235] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:45:46.651] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:46:29.862] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:47:16.200] [mlr3] Finished benchmark
INFO  [18:47:16.412] [bbotk] Result of batch 37:
INFO  [18:47:16.452] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:47:16.452] [bbotk]               -4.93418                         0.1459877
INFO  [18:47:16.452] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:47:16.452] [bbotk]                          0.374291           -3.409868              0.9075634
INFO  [18:47:16.452] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:47:16.452] [bbotk]                         14                    4701                 0.7897684
INFO  [18:47:16.452] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:47:16.452] [bbotk]  0.00936341 <list[8]>              FALSE     0.02578762        0      0
INFO  [18:47:16.452] [bbotk]  runtime_learners                                uhash
INFO  [18:47:16.452] [bbotk]           134.009 bf1c21d2-b4f2-4805-9f77-d1f8d41706c0
WARN  [18:47:19.409] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:47:19.514] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 68 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:47:25.203] [bbotk] Evaluating 1 configuration(s)
INFO  [18:47:25.538] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:47:25.703] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:47:50.815] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:48:15.916] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:48:45.333] [mlr3] Finished benchmark
INFO  [18:48:45.830] [bbotk] Result of batch 38:
INFO  [18:48:45.896] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:48:45.896] [bbotk]              -4.163484                         0.9635177
INFO  [18:48:45.896] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:48:45.896] [bbotk]                         0.7038221           -1.987972              0.4050478
INFO  [18:48:45.896] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:48:45.896] [bbotk]                         10                    1075                 0.8908645
INFO  [18:48:45.896] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:48:45.896] [bbotk]  0.007394454 <list[8]>              FALSE     0.02456767        0      0
INFO  [18:48:45.896] [bbotk]  runtime_learners                                uhash
INFO  [18:48:45.896] [bbotk]            79.045 b14df61a-9f2e-4b5b-a4da-d8cc61cd130b
WARN  [18:48:49.520] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [18:48:49.525] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 69 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:48:58.656] [bbotk] Evaluating 1 configuration(s)
INFO  [18:48:58.778] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:48:58.919] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:49:45.978] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:50:20.953] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:51:04.808] [mlr3] Finished benchmark
INFO  [18:51:05.719] [bbotk] Result of batch 39:
INFO  [18:51:05.805] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:51:05.805] [bbotk]             0.09352164                         0.7478068
INFO  [18:51:05.805] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:51:05.805] [bbotk]                         0.9989341           -1.480666              -5.550563
INFO  [18:51:05.805] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:51:05.805] [bbotk]                          5                    3131                 0.5272359
INFO  [18:51:05.805] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:51:05.805] [bbotk]  0.008236916 <list[8]>              FALSE     0.02558619        0      0
INFO  [18:51:05.805] [bbotk]  runtime_learners                                uhash
INFO  [18:51:05.805] [bbotk]           125.679 7796378e-c1b8-4583-9af3-9f1765f739f6
INFO  [18:51:06.972] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 70 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:51:14.345] [bbotk] Evaluating 1 configuration(s)
INFO  [18:51:14.591] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:51:14.702] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:52:04.807] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:53:06.384] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:53:41.019] [mlr3] Finished benchmark
INFO  [18:53:41.118] [bbotk] Result of batch 40:
INFO  [18:53:41.132] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:53:41.132] [bbotk]                -1.4991                         0.8717501
INFO  [18:53:41.132] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:53:41.132] [bbotk]                         0.7878627          -0.8474218               -5.73775
INFO  [18:53:41.132] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:53:41.132] [bbotk]                          2                    3579                 0.9768316
INFO  [18:53:41.132] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:53:41.132] [bbotk]  0.007432605 <list[8]>              FALSE      0.0267882        0      0
INFO  [18:53:41.132] [bbotk]  runtime_learners                                uhash
INFO  [18:53:41.132] [bbotk]           145.861 69ad9801-211f-4553-b619-7a38f941301f
INFO  [18:53:43.755] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 71 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:53:51.507] [bbotk] Evaluating 1 configuration(s)
INFO  [18:53:51.812] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:53:51.870] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:54:46.064] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:55:39.050] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:56:34.867] [mlr3] Finished benchmark
INFO  [18:56:35.013] [bbotk] Result of batch 41:
INFO  [18:56:35.041] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:56:35.041] [bbotk]              -3.666425                         0.5555993
INFO  [18:56:35.041] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:56:35.041] [bbotk]                         0.6082924           -3.195207               2.853282
INFO  [18:56:35.041] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:56:35.041] [bbotk]                         15                    3089                  0.682549
INFO  [18:56:35.041] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:56:35.041] [bbotk]  0.007459987 <list[8]>              FALSE     0.02353685        0      0
INFO  [18:56:35.041] [bbotk]  runtime_learners                                uhash
INFO  [18:56:35.041] [bbotk]           162.414 f07b8834-e6e8-4338-b1f5-75f25aac85d6
INFO  [18:56:36.066] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 72 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:56:44.290] [bbotk] Evaluating 1 configuration(s)
INFO  [18:56:44.349] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:56:44.527] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:57:09.552] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:57:26.194] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:57:44.573] [mlr3] Finished benchmark
INFO  [18:57:44.774] [bbotk] Result of batch 42:
INFO  [18:57:44.784] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:57:44.784] [bbotk]              -5.314266                         0.4923129
INFO  [18:57:44.784] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:57:44.784] [bbotk]                         0.8047148         -0.01428144              -6.649176
INFO  [18:57:44.784] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:57:44.784] [bbotk]                         16                      43                 0.8599366
INFO  [18:57:44.784] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:57:44.784] [bbotk]  0.009195378 <list[8]>              FALSE     0.02992805        0      0
INFO  [18:57:44.784] [bbotk]  runtime_learners                                uhash
INFO  [18:57:44.784] [bbotk]             59.84 c7901ab2-9e1e-4bef-a006-980aed322afe
INFO  [18:57:49.955] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 73 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:58:10.738] [bbotk] Evaluating 1 configuration(s)
INFO  [18:58:11.620] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:58:11.706] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:58:36.084] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [18:58:57.510] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [18:59:20.468] [mlr3] Finished benchmark
INFO  [18:59:21.128] [bbotk] Result of batch 43:
INFO  [18:59:21.150] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [18:59:21.150] [bbotk]              -2.486012                         0.3397132
INFO  [18:59:21.150] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [18:59:21.150] [bbotk]                         0.2535979            -4.25838              -4.821836
INFO  [18:59:21.150] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [18:59:21.150] [bbotk]                         10                     255                 0.9343249
INFO  [18:59:21.150] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [18:59:21.150] [bbotk]  0.007643696 <list[8]>              FALSE     0.03006051        0      0
INFO  [18:59:21.150] [bbotk]  runtime_learners                                uhash
INFO  [18:59:21.150] [bbotk]            68.401 a9d3bc0f-4bd0-4b18-a9fc-d05e160ab986
INFO  [18:59:22.188] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 74 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [18:59:29.673] [bbotk] Evaluating 1 configuration(s)
INFO  [18:59:29.697] [mlr3] Running benchmark with 3 resampling iterations
INFO  [18:59:29.722] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [18:59:56.794] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:00:34.285] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:01:11.250] [mlr3] Finished benchmark
INFO  [19:01:11.577] [bbotk] Result of batch 44:
INFO  [19:01:11.603] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:01:11.603] [bbotk]              -2.093658                         0.5126455
INFO  [19:01:11.603] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:01:11.603] [bbotk]                         0.4150876          -0.6559569              -4.736661
INFO  [19:01:11.603] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:01:11.603] [bbotk]                          5                    1822                 0.8379239
INFO  [19:01:11.603] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:01:11.603] [bbotk]  0.007173684 <list[8]>              FALSE     0.02434804        0      0
INFO  [19:01:11.603] [bbotk]  runtime_learners                                uhash
INFO  [19:01:11.603] [bbotk]           101.369 91e833cf-f074-4b14-94d2-479be7efd4a3
WARN  [19:01:15.300] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:01:15.323] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 75 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:01:41.314] [bbotk] Evaluating 1 configuration(s)
INFO  [19:01:41.472] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:01:41.530] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:03:06.984] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:04:19.563] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:06:09.250] [mlr3] Finished benchmark
INFO  [19:06:09.737] [bbotk] Result of batch 45:
INFO  [19:06:09.771] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:06:09.771] [bbotk]              -1.723509                         0.6468806
INFO  [19:06:09.771] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:06:09.771] [bbotk]                         0.9992918           -1.099695               4.284502
INFO  [19:06:09.771] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:06:09.771] [bbotk]                          9                    4699                 0.8633583
INFO  [19:06:09.771] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:06:09.771] [bbotk]  0.005973755 <list[8]>              FALSE     0.02436669        0      0
INFO  [19:06:09.771] [bbotk]  runtime_learners                                uhash
INFO  [19:06:09.771] [bbotk]           267.402 752aa9fc-dbf1-48e8-8416-08abed712add
INFO  [19:06:11.866] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 76 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:06:17.310] [bbotk] Evaluating 1 configuration(s)
INFO  [19:06:17.464] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:06:17.555] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:08:01.885] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:08:53.369] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:10:01.915] [mlr3] Finished benchmark
INFO  [19:10:02.194] [bbotk] Result of batch 46:
INFO  [19:10:02.215] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:10:02.215] [bbotk]             -0.9733053                         0.5025064
INFO  [19:10:02.215] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:10:02.215] [bbotk]                         0.8798997           -2.419169              -3.109651
INFO  [19:10:02.215] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:10:02.215] [bbotk]                          4                    4950                 0.4619691
INFO  [19:10:02.215] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:10:02.215] [bbotk]  0.006961738 <list[8]>              FALSE      0.0226051        0      0
INFO  [19:10:02.215] [bbotk]  runtime_learners                                uhash
INFO  [19:10:02.215] [bbotk]           223.925 07388a39-f1ea-4c44-8d53-8d6ed2f0981c
INFO  [19:10:05.961] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 77 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:10:18.626] [bbotk] Evaluating 1 configuration(s)
INFO  [19:10:18.778] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:10:18.789] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:12:07.465] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:13:27.093] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:15:09.810] [mlr3] Finished benchmark
INFO  [19:15:09.951] [bbotk] Result of batch 47:
INFO  [19:15:09.977] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:15:09.977] [bbotk]               1.381796                         0.5106642
INFO  [19:15:09.977] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:15:09.977] [bbotk]                         0.5126811           -8.491456               -6.24746
INFO  [19:15:09.977] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:15:09.977] [bbotk]                          2                    4599                 0.6413761
INFO  [19:15:09.977] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:15:09.977] [bbotk]  0.01105279 <list[8]>              FALSE     0.03768588        0      0
INFO  [19:15:09.977] [bbotk]  runtime_learners                                uhash
INFO  [19:15:09.977] [bbotk]           290.088 0491b968-eaae-4505-a8a1-a18a51ff8802
INFO  [19:15:19.466] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 78 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:15:28.806] [bbotk] Evaluating 1 configuration(s)
INFO  [19:15:29.119] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:15:29.276] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:16:06.805] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:16:52.138] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:17:56.724] [mlr3] Finished benchmark
INFO  [19:17:56.951] [bbotk] Result of batch 48:
INFO  [19:17:56.960] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:17:56.960] [bbotk]              -2.991066                         0.3374517
INFO  [19:17:56.960] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:17:56.960] [bbotk]                         0.1989999           -2.663901              -3.086771
INFO  [19:17:56.960] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:17:56.960] [bbotk]                          7                    2784                 0.2449587
INFO  [19:17:56.960] [bbotk]      acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:17:56.960] [bbotk]  0.00565132 <list[8]>              FALSE     0.02644551        0      0
INFO  [19:17:56.960] [bbotk]  runtime_learners                                uhash
INFO  [19:17:56.960] [bbotk]           146.941 aa9e2b52-64c7-40f8-922e-4064da6fd150
WARN  [19:17:59.644] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:17:59.664] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 79 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:18:14.132] [bbotk] Evaluating 1 configuration(s)
INFO  [19:18:14.472] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:18:14.729] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:18:47.084] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:19:19.947] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:20:15.174] [mlr3] Finished benchmark
INFO  [19:20:15.263] [bbotk] Result of batch 49:
INFO  [19:20:15.291] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:20:15.291] [bbotk]               1.784557                         0.4429326
INFO  [19:20:15.291] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:20:15.291] [bbotk]                         0.9387985           -1.395107               -1.93035
INFO  [19:20:15.291] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:20:15.291] [bbotk]                         10                    1022                 0.6869127
INFO  [19:20:15.291] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:20:15.291] [bbotk]  0.009915968 <list[8]>              FALSE     0.03222192        0      0
INFO  [19:20:15.291] [bbotk]  runtime_learners                                uhash
INFO  [19:20:15.291] [bbotk]            120.09 46c60094-fbd1-4cad-8d1d-1f10d86c263b
INFO  [19:20:17.277] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 80 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:20:29.634] [bbotk] Evaluating 1 configuration(s)
INFO  [19:20:29.716] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:20:29.921] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:21:45.187] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:22:39.557] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:23:55.214] [mlr3] Finished benchmark
INFO  [19:23:55.431] [bbotk] Result of batch 50:
INFO  [19:23:55.450] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:23:55.450] [bbotk]              0.3128964                         0.8662654
INFO  [19:23:55.450] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:23:55.450] [bbotk]                         0.1820299            -2.65017              -4.568993
INFO  [19:23:55.450] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:23:55.450] [bbotk]                          2                    4348                 0.2999511
INFO  [19:23:55.450] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:23:55.450] [bbotk]  0.005899824 <list[8]>              FALSE     0.02388415        0      0
INFO  [19:23:55.450] [bbotk]  runtime_learners                                uhash
INFO  [19:23:55.450] [bbotk]           204.851 bae3c59e-291d-4b09-b022-77d389566a19
WARN  [19:24:07.177] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:24:07.212] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 81 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:24:46.053] [bbotk] Evaluating 1 configuration(s)
INFO  [19:24:46.164] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:24:46.239] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:26:01.255] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:27:31.169] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:28:19.333] [mlr3] Finished benchmark
INFO  [19:28:19.669] [bbotk] Result of batch 51:
INFO  [19:28:19.753] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:28:19.753] [bbotk]              -5.096004                         0.2851117
INFO  [19:28:19.753] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:28:19.753] [bbotk]                          0.708463          -0.5952486               1.527415
INFO  [19:28:19.753] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:28:19.753] [bbotk]                         19                    4400                 0.8833833
INFO  [19:28:19.753] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:28:19.753] [bbotk]  0.006670469 <list[8]>              FALSE     0.02196096        0      0
INFO  [19:28:19.753] [bbotk]  runtime_learners                                uhash
INFO  [19:28:19.753] [bbotk]           212.699 8c38a946-dead-458f-8d7b-1c6fc5c3f038
INFO  [19:28:26.089] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 82 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:28:46.166] [bbotk] Evaluating 1 configuration(s)
INFO  [19:28:46.501] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:28:46.578] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:29:42.053] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:30:32.531] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:31:25.413] [mlr3] Finished benchmark
INFO  [19:31:26.390] [bbotk] Result of batch 52:
INFO  [19:31:26.684] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:31:26.684] [bbotk]              -1.689574                         0.6014812
INFO  [19:31:26.684] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:31:26.684] [bbotk]                          0.592191         -0.05267119              -3.477549
INFO  [19:31:26.684] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:31:26.684] [bbotk]                          4                    3467                 0.2274877
INFO  [19:31:26.684] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:31:26.684] [bbotk]  0.005354531 <list[8]>              FALSE     0.03488074        0      0
INFO  [19:31:26.684] [bbotk]  runtime_learners                                uhash
INFO  [19:31:26.684] [bbotk]           158.645 9ffd5280-4a37-49fc-b0e8-70ee4685267d
INFO  [19:31:29.062] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 83 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:31:43.816] [bbotk] Evaluating 1 configuration(s)
INFO  [19:31:44.009] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:31:44.093] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:32:56.970] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:33:55.091] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:35:21.892] [mlr3] Finished benchmark
INFO  [19:35:22.337] [bbotk] Result of batch 53:
INFO  [19:35:22.435] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:35:22.435] [bbotk]              -2.451896                         0.3044494
INFO  [19:35:22.435] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:35:22.435] [bbotk]                         0.5409826           -5.729161              -6.389868
INFO  [19:35:22.435] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:35:22.435] [bbotk]                          5                    3356                 0.8926086
INFO  [19:35:22.435] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:35:22.435] [bbotk]  0.006082092 <list[8]>              FALSE     0.02533125        0      0
INFO  [19:35:22.435] [bbotk]  runtime_learners                                uhash
INFO  [19:35:22.435] [bbotk]           217.286 aef77d7b-f2cb-4a5d-9344-f769010dcef0
INFO  [19:35:29.236] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 84 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:35:45.910] [bbotk] Evaluating 1 configuration(s)
INFO  [19:35:45.991] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:35:46.030] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:36:54.971] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:38:19.860] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:39:17.461] [mlr3] Finished benchmark
INFO  [19:39:18.228] [bbotk] Result of batch 54:
INFO  [19:39:18.290] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:39:18.290] [bbotk]             -0.5259984                         0.3187699
INFO  [19:39:18.290] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:39:18.290] [bbotk]                         0.7308304           -4.885441              -6.312021
INFO  [19:39:18.290] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:39:18.290] [bbotk]                         13                    4416                  0.749991
INFO  [19:39:18.290] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:39:18.290] [bbotk]  0.006359641 <list[8]>              FALSE     0.02440858        0      0
INFO  [19:39:18.290] [bbotk]  runtime_learners                                uhash
INFO  [19:39:18.290] [bbotk]           211.184 69a96020-0f56-44d1-94fd-551229f70227
WARN  [19:39:25.597] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [19:39:25.787] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 85 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:39:39.569] [bbotk] Evaluating 1 configuration(s)
INFO  [19:39:39.708] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:39:40.122] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:40:49.582] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:41:34.633] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:42:20.550] [mlr3] Finished benchmark
INFO  [19:42:20.633] [bbotk] Result of batch 55:
INFO  [19:42:20.706] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:42:20.706] [bbotk]              -2.684512                         0.6223156
INFO  [19:42:20.706] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:42:20.706] [bbotk]                           0.88917           -2.694278               0.590585
INFO  [19:42:20.706] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:42:20.706] [bbotk]                          5                    2845                 0.9188446
INFO  [19:42:20.706] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:42:20.706] [bbotk]  0.006746627 <list[8]>              FALSE     0.02651258        0      0
INFO  [19:42:20.706] [bbotk]  runtime_learners                                uhash
INFO  [19:42:20.706] [bbotk]           159.877 d4c1ac85-29b0-4638-bf94-0af889256eee
INFO  [19:42:37.088] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 86 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:42:57.148] [bbotk] Evaluating 1 configuration(s)
INFO  [19:42:57.533] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:42:57.744] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:43:55.634] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:45:12.248] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:46:47.671] [mlr3] Finished benchmark
INFO  [19:46:52.145] [bbotk] Result of batch 56:
INFO  [19:46:52.176] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:46:52.176] [bbotk]              -1.390401                         0.1291169
INFO  [19:46:52.176] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:46:52.176] [bbotk]                         0.8083091           -1.525121               3.985439
INFO  [19:46:52.176] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:46:52.176] [bbotk]                         12                    3377                 0.9750709
INFO  [19:46:52.176] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:46:52.176] [bbotk]  0.005245619 <list[8]>              FALSE     0.02354004        0      0
INFO  [19:46:52.176] [bbotk]  runtime_learners                                uhash
INFO  [19:46:52.176] [bbotk]           229.537 d9a5084a-c8dd-4cb0-a454-ac8af56fb991
INFO  [19:46:55.409] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 87 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:47:01.162] [bbotk] Evaluating 1 configuration(s)
INFO  [19:47:01.294] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:47:01.382] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:48:24.449] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:50:36.486] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [19:52:18.477] [mlr3] Finished benchmark
INFO  [19:52:18.800] [bbotk] Result of batch 57:
INFO  [19:52:18.832] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:52:18.832] [bbotk]              -4.512141                         0.9853398
INFO  [19:52:18.832] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:52:18.832] [bbotk]                         0.4711207           -1.961572                 1.5564
INFO  [19:52:18.832] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:52:18.832] [bbotk]                          8                    4633                 0.2503142
INFO  [19:52:18.832] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:52:18.832] [bbotk]  0.006659489 <list[8]>              FALSE     0.02540235        0      0
INFO  [19:52:18.832] [bbotk]  runtime_learners                                uhash
INFO  [19:52:18.832] [bbotk]           316.543 104c4f03-7485-4222-8609-420f26698ea6
INFO  [19:52:25.303] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 88 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:52:37.510] [bbotk] Evaluating 1 configuration(s)
INFO  [19:52:37.849] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:52:38.016] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:53:31.313] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:54:45.375] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [19:56:10.672] [mlr3] Finished benchmark
INFO  [19:56:12.848] [bbotk] Result of batch 58:
INFO  [19:56:13.029] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [19:56:13.029] [bbotk]               1.112303                         0.1405302
INFO  [19:56:13.029] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [19:56:13.029] [bbotk]                         0.1746101           -2.614104               1.517019
INFO  [19:56:13.029] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [19:56:13.029] [bbotk]                         16                    5000                 0.7843386
INFO  [19:56:13.029] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [19:56:13.029] [bbotk]  0.005552645 <list[8]>              FALSE     0.03043175        0      0
INFO  [19:56:13.029] [bbotk]  runtime_learners                                uhash
INFO  [19:56:13.029] [bbotk]           211.807 4a8e301e-39e0-42bb-9bb2-057055c58b8d
INFO  [19:56:21.985] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 89 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [19:56:45.323] [bbotk] Evaluating 1 configuration(s)
INFO  [19:56:45.960] [mlr3] Running benchmark with 3 resampling iterations
INFO  [19:56:46.050] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [19:58:34.195] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [19:59:56.753] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:01:26.149] [mlr3] Finished benchmark
INFO  [20:01:26.566] [bbotk] Result of batch 59:
INFO  [20:01:26.580] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:01:26.580] [bbotk]              -0.668683                         0.3942733
INFO  [20:01:26.580] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:01:26.580] [bbotk]                         0.7214954            -3.05366              -5.413786
INFO  [20:01:26.580] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:01:26.580] [bbotk]                          1                    4057                 0.5559423
INFO  [20:01:26.580] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:01:26.580] [bbotk]  0.005531644 <list[8]>              FALSE     0.02525974        0      0
INFO  [20:01:26.580] [bbotk]  runtime_learners                                uhash
INFO  [20:01:26.580] [bbotk]           271.714 28d7c216-837e-4681-9ceb-b3c9af20c500
WARN  [20:01:44.793] [mlr3] train: Stopped because hard maximum generation limit was hit.
INFO  [20:01:45.081] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 90 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:01:59.582] [bbotk] Evaluating 1 configuration(s)
INFO  [20:01:59.667] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:01:59.684] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:02:28.407] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:02:57.796] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:03:45.845] [mlr3] Finished benchmark
INFO  [20:03:46.791] [bbotk] Result of batch 60:
INFO  [20:03:46.808] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:03:46.808] [bbotk]              -2.300804                         0.1450997
INFO  [20:03:46.808] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:03:46.808] [bbotk]                         0.9348181          -0.7116295                5.22859
INFO  [20:03:46.808] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:03:46.808] [bbotk]                         17                     750                 0.2604903
INFO  [20:03:46.808] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:03:46.808] [bbotk]  0.007210245 <list[8]>              FALSE     0.03148017        0      0
INFO  [20:03:46.808] [bbotk]  runtime_learners                                uhash
INFO  [20:03:46.808] [bbotk]           105.813 1d9e1e36-69be-43c6-b7ce-44f7043bdd5c
INFO  [20:03:50.759] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 91 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:04:04.060] [bbotk] Evaluating 1 configuration(s)
INFO  [20:04:04.097] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:04:04.113] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:04:34.755] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:05:07.812] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
INFO  [20:05:39.679] [mlr3] Finished benchmark
INFO  [20:05:40.134] [bbotk] Result of batch 61:
INFO  [20:05:40.220] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:05:40.220] [bbotk]              -4.547563                         0.9058704
INFO  [20:05:40.220] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:05:40.220] [bbotk]                         0.8902062          -0.2575508              -2.923736
INFO  [20:05:40.220] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:05:40.220] [bbotk]                         20                     599                 0.2831587
INFO  [20:05:40.220] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:05:40.220] [bbotk]  0.006603112 <list[8]>              FALSE     0.03345172        0      0
INFO  [20:05:40.220] [bbotk]  runtime_learners                                uhash
INFO  [20:05:40.220] [bbotk]            95.411 3eb654c1-6d4f-431c-84d2-1c525889b5e2
INFO  [20:05:46.934] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 92 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:05:55.224] [bbotk] Evaluating 1 configuration(s)
INFO  [20:05:55.665] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:05:55.740] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:06:19.598] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:06:59.569] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:08:09.117] [mlr3] Finished benchmark
INFO  [20:08:13.408] [bbotk] Result of batch 62:
INFO  [20:08:13.576] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:08:13.576] [bbotk]              -1.426836                         0.2163517
INFO  [20:08:13.576] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:08:13.576] [bbotk]                         0.8814078           -2.910211               -4.42664
INFO  [20:08:13.576] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:08:13.576] [bbotk]                         10                    1966                 0.7348565
INFO  [20:08:13.576] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:08:13.576] [bbotk]  0.005257145 <list[8]>              FALSE     0.02472643        0      0
INFO  [20:08:13.576] [bbotk]  runtime_learners                                uhash
INFO  [20:08:13.576] [bbotk]           132.754 27122684-1355-4de3-935d-4674913d53b4
INFO  [20:08:26.625] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 93 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:08:57.095] [bbotk] Evaluating 1 configuration(s)
INFO  [20:08:57.396] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:08:57.471] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:10:59.496] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:11:53.744] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:13:32.941] [mlr3] Finished benchmark
INFO  [20:13:33.454] [bbotk] Result of batch 63:
INFO  [20:13:33.487] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:13:33.487] [bbotk]              -4.101926                         0.9286016
INFO  [20:13:33.487] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:13:33.487] [bbotk]                         0.1433193            -2.46762               5.370066
INFO  [20:13:33.487] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:13:33.487] [bbotk]                         19                    4768                 0.8352795
INFO  [20:13:33.487] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:13:33.487] [bbotk]  0.005191626 <list[8]>              FALSE     0.02911488        0      0
INFO  [20:13:33.487] [bbotk]  runtime_learners                                uhash
INFO  [20:13:33.487] [bbotk]           275.227 8d0f84a7-5aa1-43d4-9c69-b176afcad8db
INFO  [20:13:38.967] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 94 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:13:54.440] [bbotk] Evaluating 1 configuration(s)
INFO  [20:13:54.985] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:13:55.085] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:14:32.557] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:15:43.470] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:17:10.998] [mlr3] Finished benchmark
INFO  [20:17:11.959] [bbotk] Result of batch 64:
INFO  [20:17:12.078] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:17:12.078] [bbotk]              -2.274175                         0.9734761
INFO  [20:17:12.078] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:17:12.078] [bbotk]                         0.2339593            -3.44266              -6.060172
INFO  [20:17:12.078] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:17:12.078] [bbotk]                          7                    4174                 0.7594102
INFO  [20:17:12.078] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:17:12.078] [bbotk]  0.004593723 <list[8]>              FALSE     0.02429019        0      0
INFO  [20:17:12.078] [bbotk]  runtime_learners                                uhash
INFO  [20:17:12.078] [bbotk]           195.148 fa42f25f-37e2-472f-aef3-d5f3cac33d9c
INFO  [20:17:13.454] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 95 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:17:31.629] [bbotk] Evaluating 1 configuration(s)
INFO  [20:17:31.648] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:17:31.659] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:18:46.034] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:20:15.895] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:22:06.639] [mlr3] Finished benchmark
INFO  [20:22:06.862] [bbotk] Result of batch 65:
INFO  [20:22:06.870] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:22:06.870] [bbotk]              -5.753585                         0.8145604
INFO  [20:22:06.870] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:22:06.870] [bbotk]                         0.6048626           -6.391055              -6.849033
INFO  [20:22:06.870] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:22:06.870] [bbotk]                         11                    4631                 0.2160979
INFO  [20:22:06.870] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:22:06.870] [bbotk]  0.006068566 <list[8]>              FALSE     0.02924415        0      0
INFO  [20:22:06.870] [bbotk]  runtime_learners                                uhash
INFO  [20:22:06.870] [bbotk]            274.72 53aab2a2-e739-4b1c-8ff0-2c150780374b
INFO  [20:22:11.111] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 96 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:22:18.967] [bbotk] Evaluating 1 configuration(s)
INFO  [20:22:19.206] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:22:19.250] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:23:35.102] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:24:56.627] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:26:25.012] [mlr3] Finished benchmark
INFO  [20:26:25.179] [bbotk] Result of batch 66:
INFO  [20:26:25.188] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:26:25.188] [bbotk]              -1.517535                         0.4764142
INFO  [20:26:25.188] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:26:25.188] [bbotk]                         0.3408657           -4.975723              -5.307748
INFO  [20:26:25.188] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:26:25.188] [bbotk]                          6                    4561                 0.9425758
INFO  [20:26:25.188] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:26:25.188] [bbotk]  0.005059246 <list[8]>              FALSE     0.02506159        0      0
INFO  [20:26:25.188] [bbotk]  runtime_learners                                uhash
INFO  [20:26:25.188] [bbotk]           245.605 39ac1d37-2eca-459b-9c16-f88db13257b7
INFO  [20:26:27.249] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 97 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:26:33.219] [bbotk] Evaluating 1 configuration(s)
INFO  [20:26:33.325] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:26:33.394] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:27:28.243] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:28:29.409] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:29:18.361] [mlr3] Finished benchmark
INFO  [20:29:18.536] [bbotk] Result of batch 67:
INFO  [20:29:18.548] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:29:18.548] [bbotk]             -0.7756933                          0.726837
INFO  [20:29:18.548] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:29:18.548] [bbotk]                         0.9290258           -3.325052              -5.293181
INFO  [20:29:18.548] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:29:18.548] [bbotk]                          9                    4808                  0.667322
INFO  [20:29:18.548] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:29:18.548] [bbotk]  0.004419416 <list[8]>              FALSE     0.02373088        0      0
INFO  [20:29:18.548] [bbotk]  runtime_learners                                uhash
INFO  [20:29:18.548] [bbotk]           164.786 4eaa46a2-a3f7-49b3-a9b4-e14686dcba2e
INFO  [20:29:20.151] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 98 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:29:30.085] [bbotk] Evaluating 1 configuration(s)
INFO  [20:29:30.285] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:29:30.347] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:29:50.021] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:30:10.624] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:30:33.737] [mlr3] Finished benchmark
INFO  [20:30:34.163] [bbotk] Result of batch 68:
INFO  [20:30:34.169] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:30:34.169] [bbotk]              0.6249315                         0.9736328
INFO  [20:30:34.169] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:30:34.169] [bbotk]                          0.510707           -1.324993              -6.836751
INFO  [20:30:34.169] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:30:34.169] [bbotk]                         11                    4218                 0.5331718
INFO  [20:30:34.169] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:30:34.169] [bbotk]  0.004651469 <list[8]>              FALSE     0.02639006        0      0
INFO  [20:30:34.169] [bbotk]  runtime_learners                                uhash
INFO  [20:30:34.169] [bbotk]             63.06 6f6e2da7-a4e5-45b0-9d24-ce8571addb1a
INFO  [20:30:35.952] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 99 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:30:44.820] [bbotk] Evaluating 1 configuration(s)
INFO  [20:30:44.875] [mlr3] Running benchmark with 3 resampling iterations
INFO  [20:30:44.890] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 1/3)
INFO  [20:31:10.065] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 2/3)
INFO  [20:31:29.109] [mlr3] Applying learner 'classif.xgboost' on task 'wdbc' (iter 3/3)
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: algorithm did not converge
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
This happened PipeOp classif.xgboost's $train()
This happened PipeOp classif.xgboost.tuned's $train()
INFO  [20:31:51.706] [mlr3] Finished benchmark
INFO  [20:31:51.859] [bbotk] Result of batch 69:
INFO  [20:31:51.871] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:31:51.871] [bbotk]              -5.529779                         0.9075646
INFO  [20:31:51.871] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:31:51.871] [bbotk]                         0.6611644            -2.11128              0.7662798
INFO  [20:31:51.871] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:31:51.871] [bbotk]                         18                    4993                 0.1647124
INFO  [20:31:51.871] [bbotk]       acq_ei  x_domain .already_evaluated classif.bbrier warnings errors
INFO  [20:31:51.871] [bbotk]  0.004401253 <list[8]>              FALSE        0.02524        0      0
INFO  [20:31:51.871] [bbotk]  runtime_learners                                uhash
INFO  [20:31:51.871] [bbotk]            66.672 73e4e773-21fa-4001-bd6c-e4d7da4163db
INFO  [20:31:54.460] [mlr3] Calling train method of fallback 'regr.ranger' on task 'surrogate_task' with 100 observations {learner: <LearnerRegrRanger/LearnerRegr/Learner/R6>}
INFO  [20:31:54.544] [bbotk] Finished optimizing after 100 evaluation(s)
INFO  [20:31:54.547] [bbotk] Result:
INFO  [20:31:54.557] [bbotk]  classif.xgboost.alpha classif.xgboost.colsample_bylevel
INFO  [20:31:54.557] [bbotk]                  <num>                             <num>
INFO  [20:31:54.557] [bbotk]              -5.096004                         0.2851117
INFO  [20:31:54.557] [bbotk]  classif.xgboost.colsample_bytree classif.xgboost.eta classif.xgboost.lambda
INFO  [20:31:54.557] [bbotk]                             <num>               <num>                  <num>
INFO  [20:31:54.557] [bbotk]                          0.708463          -0.5952486               1.527415
INFO  [20:31:54.557] [bbotk]  classif.xgboost.max_depth classif.xgboost.nrounds classif.xgboost.subsample
INFO  [20:31:54.557] [bbotk]                      <int>                   <int>                     <num>
INFO  [20:31:54.557] [bbotk]                         19                    4400                 0.8833833
INFO  [20:31:54.557] [bbotk]  learner_param_vals  x_domain classif.bbrier
INFO  [20:31:54.557] [bbotk]              <list>    <list>          <num>
INFO  [20:31:54.557] [bbotk]          <list[10]> <list[8]>     0.02196096

### [bt]: Job terminated successfully [batchtools job.id=1431]
### [bt]: Calculation finished!
